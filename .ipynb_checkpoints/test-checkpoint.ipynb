{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Pasic, Mr. Jakob</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315097</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>812</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Lester, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                         Name   Sex   Age  \\\n",
       "836          837         0       3             Pasic, Mr. Jakob  male  21.0   \n",
       "811          812         0       3            Lester, Mr. James  male  39.0   \n",
       "873          874         0       3  Vander Cruyssen, Mr. Victor  male  47.0   \n",
       "\n",
       "     SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
       "836      0      0     315097   8.6625   NaN        S  \n",
       "811      0      0  A/4 48871  24.1500   NaN        S  \n",
       "873      0      0     345765   9.0000   NaN        S  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "print(trainDF.info())\n",
    "trainDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "      <td>Cor, Mr. Ivan</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349229</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1046</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Master. Filip Oscar</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1233</td>\n",
       "      <td>3</td>\n",
       "      <td>Lundstrom, Mr. Thure Edvin</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350403</td>\n",
       "      <td>7.5792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                          Name   Sex   Age  SibSp  \\\n",
       "334         1226       3                 Cor, Mr. Ivan  male  27.0      0   \n",
       "154         1046       3  Asplund, Master. Filip Oscar  male  13.0      4   \n",
       "341         1233       3    Lundstrom, Mr. Thure Edvin  male  32.0      0   \n",
       "\n",
       "     Parch  Ticket     Fare Cabin Embarked  \n",
       "334      0  349229   7.8958   NaN        S  \n",
       "154      2  347077  31.3875   NaN        S  \n",
       "341      0  350403   7.5792   NaN        S  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "print(testDF.info())\n",
    "testDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.257307</td>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.155660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>0.096335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.075972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>0.075198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>0.073258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.257307</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>-0.172683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_female</th>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_male</th>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_C</th>\n",
       "      <td>0.168240</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>-0.775441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.489874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>-0.155660</td>\n",
       "      <td>0.096335</td>\n",
       "      <td>-0.075972</td>\n",
       "      <td>0.075198</td>\n",
       "      <td>0.073258</td>\n",
       "      <td>-0.172683</td>\n",
       "      <td>-0.119504</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>-0.775441</td>\n",
       "      <td>-0.489874</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Survived    Pclass       Age     SibSp     Parch      Fare  \\\n",
       "Survived    1.000000 -0.338481 -0.077221 -0.035322  0.081629  0.257307   \n",
       "Pclass     -0.338481  1.000000 -0.408106  0.060832  0.018322 -0.558629   \n",
       "Age        -0.077221 -0.408106  1.000000 -0.243699 -0.150917  0.178740   \n",
       "SibSp      -0.035322  0.060832 -0.243699  1.000000  0.373587  0.160238   \n",
       "Parch       0.081629  0.018322 -0.150917  0.373587  1.000000  0.221539   \n",
       "Fare        0.257307 -0.558629  0.178740  0.160238  0.221539  1.000000   \n",
       "Sex_female  0.543351 -0.124617 -0.063645  0.109609  0.213125  0.185523   \n",
       "Sex_male   -0.543351  0.124617  0.063645 -0.109609 -0.213125 -0.185523   \n",
       "Embarked_C  0.168240 -0.269658  0.085777 -0.048396 -0.008635  0.286269   \n",
       "Embarked_Q  0.003650  0.230491 -0.019458 -0.048678 -0.100943 -0.130059   \n",
       "Embarked_S -0.155660  0.096335 -0.075972  0.075198  0.073258 -0.172683   \n",
       "\n",
       "            Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
       "Survived      0.543351 -0.543351    0.168240    0.003650   -0.155660  \n",
       "Pclass       -0.124617  0.124617   -0.269658    0.230491    0.096335  \n",
       "Age          -0.063645  0.063645    0.085777   -0.019458   -0.075972  \n",
       "SibSp         0.109609 -0.109609   -0.048396   -0.048678    0.075198  \n",
       "Parch         0.213125 -0.213125   -0.008635   -0.100943    0.073258  \n",
       "Fare          0.185523 -0.185523    0.286269   -0.130059   -0.172683  \n",
       "Sex_female    1.000000 -1.000000    0.066564    0.088651   -0.119504  \n",
       "Sex_male     -1.000000  1.000000   -0.066564   -0.088651    0.119504  \n",
       "Embarked_C    0.066564 -0.066564    1.000000   -0.164166   -0.775441  \n",
       "Embarked_Q    0.088651 -0.088651   -0.164166    1.000000   -0.489874  \n",
       "Embarked_S   -0.119504  0.119504   -0.775441   -0.489874    1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF = concatDF.reset_index(drop=True)\n",
    "pd.get_dummies(concatDF,columns=['Sex','Embarked']).drop(columns=['PassengerId','Name','Ticket','Cabin']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredDF = pd.read_csv('train4.csv')\n",
    "# featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "# featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "# featuredDF['SibSp'] = featuredDF['SibSp']/featuredDF['SibSp'].max()\n",
    "# featuredDF['Parch'] = featuredDF['Parch']/featuredDF['Parch'].max()\n",
    "# featuredDF['Ticket'] = featuredDF['Ticket']/featuredDF['Ticket'].max()\n",
    "# featuredDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredDF = pd.read_csv('train5.csv')\n",
    "# featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "# featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "# featuredDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Name_ the Countess</th>\n",
       "      <th>Cabin_A</th>\n",
       "      <th>Cabin_B</th>\n",
       "      <th>Cabin_C</th>\n",
       "      <th>Cabin_D</th>\n",
       "      <th>Cabin_E</th>\n",
       "      <th>Cabin_F</th>\n",
       "      <th>Cabin_G</th>\n",
       "      <th>Cabin_M</th>\n",
       "      <th>Cabin_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27500</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.32500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>1305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>1306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.212559</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>1308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PassengerId  Survived      Age  SibSp     Parch    Ticket      Fare  \\\n",
       "0               1       0.0  0.27500  0.125  0.000000  0.090909  0.014151   \n",
       "1               2       1.0  0.47500  0.125  0.000000  0.181818  0.139136   \n",
       "2               3       1.0  0.32500  0.000  0.000000  0.090909  0.015469   \n",
       "3               4       1.0  0.43750  0.125  0.000000  0.181818  0.103644   \n",
       "4               5       0.0  0.43750  0.000  0.000000  0.090909  0.015713   \n",
       "...           ...       ...      ...    ...       ...       ...       ...   \n",
       "1304         1305       NaN  0.33750  0.000  0.000000  0.090909  0.015713   \n",
       "1305         1306       NaN  0.48750  0.000  0.000000  0.272727  0.212559   \n",
       "1306         1307       NaN  0.48125  0.000  0.000000  0.090909  0.014151   \n",
       "1307         1308       NaN  0.33750  0.000  0.000000  0.090909  0.015713   \n",
       "1308         1309       NaN  0.30000  0.125  0.111111  0.272727  0.043640   \n",
       "\n",
       "      Pclass_1  Pclass_2  Pclass_3  ...  Name_ the Countess  Cabin_A  Cabin_B  \\\n",
       "0        False     False      True  ...               False    False    False   \n",
       "1         True     False     False  ...               False    False    False   \n",
       "2        False     False      True  ...               False    False    False   \n",
       "3         True     False     False  ...               False    False    False   \n",
       "4        False     False      True  ...               False    False    False   \n",
       "...        ...       ...       ...  ...                 ...      ...      ...   \n",
       "1304     False     False      True  ...               False    False    False   \n",
       "1305      True     False     False  ...               False    False    False   \n",
       "1306     False     False      True  ...               False    False    False   \n",
       "1307     False     False      True  ...               False    False    False   \n",
       "1308     False     False      True  ...               False    False    False   \n",
       "\n",
       "      Cabin_C  Cabin_D  Cabin_E  Cabin_F  Cabin_G  Cabin_M  Cabin_T  \n",
       "0       False    False    False    False    False     True    False  \n",
       "1        True    False    False    False    False    False    False  \n",
       "2       False    False    False    False    False     True    False  \n",
       "3        True    False    False    False    False    False    False  \n",
       "4       False    False    False    False    False     True    False  \n",
       "...       ...      ...      ...      ...      ...      ...      ...  \n",
       "1304    False    False    False    False    False     True    False  \n",
       "1305     True    False    False    False    False    False    False  \n",
       "1306    False    False    False    False    False     True    False  \n",
       "1307    False    False    False    False    False     True    False  \n",
       "1308    False    False    False    False    False     True    False  \n",
       "\n",
       "[1309 rows x 42 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuredDF = pd.read_csv('train6.csv')\n",
    "featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "featuredDF['SibSp'] = featuredDF['SibSp']/featuredDF['SibSp'].max()\n",
    "featuredDF['Parch'] = featuredDF['Parch']/featuredDF['Parch'].max()\n",
    "featuredDF['Ticket'] = featuredDF['Ticket']/featuredDF['Ticket'].max()\n",
    "featuredDF= pd.get_dummies(featuredDF,columns=['Pclass','Sex','Embarked','Name','Cabin'])\n",
    "featuredDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4313, 0.0000, 0.0000, 0.0909, 0.0153, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 1.0000, 0.0000]),\n",
       " tensor([]),\n",
       " 892)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF:pd.DataFrame):\n",
    "        self.PassengerId= DF['PassengerId'].values\n",
    "        self.Servived = pd.get_dummies(DF['Survived']).values\n",
    "        DF = DF.drop(columns=['PassengerId','Survived'])\n",
    "        self.data = DF.astype(float).values\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PassengerId)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        id = self.PassengerId[idx]\n",
    "        x = torch.FloatTensor(self.data[idx])\n",
    "        y = torch.FloatTensor(self.Servived[idx])\n",
    "        return x, y , id\n",
    "\n",
    "\n",
    "dataSet = MyDataset(DF=featuredDF[:891])\n",
    "testSet = MyDataset(DF=featuredDF[891:])\n",
    "testSet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainSet, valSet = torch.utils.data.random_split(dataSet,(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.2750, 0.1250, 0.1111,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.3375, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.4875, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2875, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000]]),\n",
       " tensor([[1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         ...,\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]),\n",
       " tensor([842, 324, 682, 764, 599, 537, 803, 304, 636, 378, 261, 215,  90, 740,\n",
       "         250, 662, 135, 179, 707, 241, 619, 739, 497, 846, 781, 584,  92,  69,\n",
       "         246, 347, 856, 260, 613, 483, 571, 512, 569, 326, 162, 526, 149, 422,\n",
       "         462, 408, 146, 753, 701, 595, 702, 393, 221, 850, 139, 802,  37, 648,\n",
       "         772, 423,  65, 821, 799, 140, 327, 183, 424, 236, 346, 321,  74, 736,\n",
       "         551, 124, 227, 631, 742, 119,   5, 222, 426, 605, 762, 470, 638, 364,\n",
       "         891, 201, 559, 735, 863, 809, 880, 733, 747, 787, 514, 607, 337, 723,\n",
       "          63, 284, 291, 867, 195, 176, 441, 729, 703, 313, 293, 494, 592, 189,\n",
       "         522, 432, 171, 471, 509, 415, 540, 870, 137, 832, 679, 658, 247, 678,\n",
       "         148, 670, 355, 674, 881, 714, 239,  68, 782,   4,  36, 686, 281, 458,\n",
       "         320, 481, 318, 165, 818, 107, 791,  89, 438, 160, 158, 573, 348, 144,\n",
       "         147, 554, 625, 309, 451,  71, 170, 106, 181, 515, 622, 307, 518, 687,\n",
       "         466, 253, 484, 289,  46, 344, 445, 824, 103, 142, 372, 357,  38, 783,\n",
       "         305, 555,  99, 271, 718, 279, 350,   3, 552, 277,  76, 885, 872, 102,\n",
       "         779, 677, 831, 635,  32, 129, 593, 111,  42, 603, 560, 768, 630, 283,\n",
       "         535, 611, 887, 205, 507,  70, 836,  48,  26, 308, 382, 455, 472, 754,\n",
       "         508, 369, 653, 446, 890, 547, 748, 447, 879,  28, 848, 401, 220, 209,\n",
       "         604, 335, 676, 391, 594, 245, 750, 325,  30, 108, 634, 741, 214,  55,\n",
       "         767, 855, 697, 430, 278, 530, 449, 790, 534, 411, 529, 499, 511, 473,\n",
       "         845, 805, 136,  49, 557,  60, 572, 655, 564, 627, 835, 816, 242, 618,\n",
       "         437, 298, 808, 400, 186, 385, 228, 312, 513, 190, 272, 565, 237, 865,\n",
       "         434, 314, 646, 333, 510, 154,  91, 869, 392, 671, 838, 527,  88, 874,\n",
       "         546, 104, 448, 628, 339, 632, 488,  62, 252,  56, 528, 331, 319, 118,\n",
       "          97, 664, 498, 503,  84, 716, 452, 813, 243, 200, 769, 234, 629, 517,\n",
       "         583, 340, 827, 138, 474, 570, 112, 574, 341, 225, 302, 711, 777,  35,\n",
       "         229, 506, 224, 558, 290, 577, 582,  19, 760, 288, 644, 386, 623, 567,\n",
       "         395, 766, 817, 614,  66, 235, 500, 105,  77, 765,   8, 323, 167, 216,\n",
       "         269, 710, 793, 334, 588, 516, 491, 286, 210, 443, 487, 665, 639, 826,\n",
       "         541, 684,  79, 843, 123, 113, 645, 549, 266,  52, 311, 114,  64, 667,\n",
       "         689, 122, 468, 771, 121, 230, 699, 248, 759,  20, 543,  27, 814, 414,\n",
       "         396, 381, 387, 807, 532, 563, 778, 217, 839, 715,  59,  17, 643, 810,\n",
       "         480, 332, 223, 398, 151,  23, 784, 440, 601, 775, 187, 157, 233, 675,\n",
       "         141, 695, 203, 580, 159, 467,  45,   7, 403, 596, 109, 345, 213, 833,\n",
       "         854, 857, 668, 388, 704, 342,  10, 268, 352, 545, 521, 871,  12, 806,\n",
       "         383, 501, 837, 878, 303, 717,  40, 746, 208, 389, 211, 637, 376, 402,\n",
       "         336, 721, 811, 709, 416, 578, 883, 743, 287,  86, 368, 889, 591, 852,\n",
       "          25, 600, 586, 649, 251, 463, 800, 297, 299, 328, 708, 519, 475, 700,\n",
       "         418, 859, 734,  80, 164, 310,  51, 786, 240, 590, 184, 270, 693, 587,\n",
       "         439, 359, 413, 192, 776, 626,  93, 198, 300, 177, 134, 420, 390, 525,\n",
       "         556, 127, 380, 730,  14, 568, 705, 886, 479, 864, 706, 358, 436,  39,\n",
       "         444, 469, 548, 823,  21, 296,   6, 166, 282, 720, 353, 561, 425, 562,\n",
       "         168, 656, 465, 238, 206, 257,  94, 696, 795, 576, 145, 566,  31, 450,\n",
       "         844, 504, 275, 822, 273, 610, 862, 581, 659, 338, 620, 202, 681, 417,\n",
       "         317, 828, 370, 407,  67, 650, 598, 713, 478, 536, 640,  96, 652,  13,\n",
       "          95, 464,  53, 431, 366, 285, 178, 399])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(trainSet,batch_size=1024,sampler=torch.utils.data.RandomSampler(trainSet))\n",
    "valLoader= torch.utils.data.DataLoader(valSet,batch_size=1024,sampler=torch.utils.data.RandomSampler(valSet))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=1024)\n",
    "next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(40, 40),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(40, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: 0 - valLoss: 0.701723575592041 - trainLoss: 0.6990471482276917\n",
      "cnt: 0 - valLoss: 0.7005828022956848 - trainLoss: 0.6977992653846741\n",
      "cnt: 0 - valLoss: 0.6994634866714478 - trainLoss: 0.6965736150741577\n",
      "cnt: 0 - valLoss: 0.6983652114868164 - trainLoss: 0.695370614528656\n",
      "cnt: 0 - valLoss: 0.6972872018814087 - trainLoss: 0.694189190864563\n",
      "cnt: 0 - valLoss: 0.6962290406227112 - trainLoss: 0.6930291056632996\n",
      "cnt: 0 - valLoss: 0.6951897144317627 - trainLoss: 0.6918909549713135\n",
      "cnt: 0 - valLoss: 0.6941683888435364 - trainLoss: 0.6907727718353271\n",
      "cnt: 0 - valLoss: 0.6931634545326233 - trainLoss: 0.6896737217903137\n",
      "cnt: 0 - valLoss: 0.6921765208244324 - trainLoss: 0.688593327999115\n",
      "cnt: 0 - valLoss: 0.6912065148353577 - trainLoss: 0.6875320672988892\n",
      "cnt: 0 - valLoss: 0.6902552843093872 - trainLoss: 0.6864894032478333\n",
      "cnt: 0 - valLoss: 0.6893265843391418 - trainLoss: 0.6854671835899353\n",
      "cnt: 0 - valLoss: 0.6884129643440247 - trainLoss: 0.6844690442085266\n",
      "cnt: 0 - valLoss: 0.6875148415565491 - trainLoss: 0.6834864616394043\n",
      "cnt: 0 - valLoss: 0.6866316795349121 - trainLoss: 0.6825195550918579\n",
      "cnt: 0 - valLoss: 0.6857625842094421 - trainLoss: 0.6815679669380188\n",
      "cnt: 0 - valLoss: 0.6849075555801392 - trainLoss: 0.6806308031082153\n",
      "cnt: 0 - valLoss: 0.6840654015541077 - trainLoss: 0.6797081232070923\n",
      "cnt: 0 - valLoss: 0.6832361221313477 - trainLoss: 0.6787989735603333\n",
      "cnt: 0 - valLoss: 0.6824195384979248 - trainLoss: 0.6779037714004517\n",
      "cnt: 0 - valLoss: 0.6816147565841675 - trainLoss: 0.6770215034484863\n",
      "cnt: 0 - valLoss: 0.6808215379714966 - trainLoss: 0.6761520504951477\n",
      "cnt: 0 - valLoss: 0.6800403594970703 - trainLoss: 0.6752952933311462\n",
      "cnt: 0 - valLoss: 0.6792710423469543 - trainLoss: 0.6744503378868103\n",
      "cnt: 0 - valLoss: 0.6785122156143188 - trainLoss: 0.6736166477203369\n",
      "cnt: 0 - valLoss: 0.6777640581130981 - trainLoss: 0.6727938652038574\n",
      "cnt: 0 - valLoss: 0.6770259141921997 - trainLoss: 0.67198246717453\n",
      "cnt: 0 - valLoss: 0.676296591758728 - trainLoss: 0.6711819767951965\n",
      "cnt: 0 - valLoss: 0.6755759716033936 - trainLoss: 0.6703917980194092\n",
      "cnt: 0 - valLoss: 0.6748645901679993 - trainLoss: 0.6696118116378784\n",
      "cnt: 0 - valLoss: 0.674161434173584 - trainLoss: 0.6688411831855774\n",
      "cnt: 0 - valLoss: 0.6734665036201477 - trainLoss: 0.6680794954299927\n",
      "cnt: 0 - valLoss: 0.6727808117866516 - trainLoss: 0.6673274040222168\n",
      "cnt: 0 - valLoss: 0.6721034646034241 - trainLoss: 0.6665849089622498\n",
      "cnt: 0 - valLoss: 0.6714349389076233 - trainLoss: 0.6658517718315125\n",
      "cnt: 0 - valLoss: 0.670773983001709 - trainLoss: 0.6651277542114258\n",
      "cnt: 0 - valLoss: 0.6701199412345886 - trainLoss: 0.6644117832183838\n",
      "cnt: 0 - valLoss: 0.6694737076759338 - trainLoss: 0.6637037992477417\n",
      "cnt: 0 - valLoss: 0.6688351631164551 - trainLoss: 0.6630036234855652\n",
      "cnt: 0 - valLoss: 0.6682047843933105 - trainLoss: 0.6623114347457886\n",
      "cnt: 0 - valLoss: 0.6675825715065002 - trainLoss: 0.6616278290748596\n",
      "cnt: 0 - valLoss: 0.6669679880142212 - trainLoss: 0.6609518527984619\n",
      "cnt: 0 - valLoss: 0.666361927986145 - trainLoss: 0.6602829098701477\n",
      "cnt: 0 - valLoss: 0.665765643119812 - trainLoss: 0.6596209406852722\n",
      "cnt: 0 - valLoss: 0.6651754379272461 - trainLoss: 0.6589677929878235\n",
      "cnt: 0 - valLoss: 0.664591372013092 - trainLoss: 0.6583216190338135\n",
      "cnt: 0 - valLoss: 0.6640139222145081 - trainLoss: 0.6576827764511108\n",
      "cnt: 0 - valLoss: 0.6634423732757568 - trainLoss: 0.6570509672164917\n",
      "cnt: 0 - valLoss: 0.662876546382904 - trainLoss: 0.656424880027771\n",
      "cnt: 0 - valLoss: 0.6623163819313049 - trainLoss: 0.6558042764663696\n",
      "cnt: 0 - valLoss: 0.6617622375488281 - trainLoss: 0.6551896333694458\n",
      "cnt: 0 - valLoss: 0.6612135171890259 - trainLoss: 0.65458083152771\n",
      "cnt: 0 - valLoss: 0.6606696844100952 - trainLoss: 0.6539790034294128\n",
      "cnt: 0 - valLoss: 0.6601309776306152 - trainLoss: 0.6533830165863037\n",
      "cnt: 0 - valLoss: 0.6595967411994934 - trainLoss: 0.65279221534729\n",
      "cnt: 0 - valLoss: 0.6590669751167297 - trainLoss: 0.6522061824798584\n",
      "cnt: 0 - valLoss: 0.6585396528244019 - trainLoss: 0.6516245603561401\n",
      "cnt: 0 - valLoss: 0.6580156683921814 - trainLoss: 0.6510447859764099\n",
      "cnt: 0 - valLoss: 0.6574937701225281 - trainLoss: 0.6504681706428528\n",
      "cnt: 0 - valLoss: 0.6569716930389404 - trainLoss: 0.6498939990997314\n",
      "cnt: 0 - valLoss: 0.6564511060714722 - trainLoss: 0.649321436882019\n",
      "cnt: 0 - valLoss: 0.6559330821037292 - trainLoss: 0.6487512588500977\n",
      "cnt: 0 - valLoss: 0.6554185748100281 - trainLoss: 0.6481839418411255\n",
      "cnt: 0 - valLoss: 0.6549080014228821 - trainLoss: 0.6476207971572876\n",
      "cnt: 0 - valLoss: 0.6544007062911987 - trainLoss: 0.6470615863800049\n",
      "cnt: 0 - valLoss: 0.6538964509963989 - trainLoss: 0.6465066075325012\n",
      "cnt: 0 - valLoss: 0.6533951163291931 - trainLoss: 0.6459557414054871\n",
      "cnt: 0 - valLoss: 0.6528959274291992 - trainLoss: 0.6454084515571594\n",
      "cnt: 0 - valLoss: 0.6523993015289307 - trainLoss: 0.6448646187782288\n",
      "cnt: 0 - valLoss: 0.6519052982330322 - trainLoss: 0.6443243026733398\n",
      "cnt: 0 - valLoss: 0.6514142155647278 - trainLoss: 0.6437875032424927\n",
      "cnt: 0 - valLoss: 0.6509260535240173 - trainLoss: 0.6432541012763977\n",
      "cnt: 0 - valLoss: 0.6504401564598083 - trainLoss: 0.642723798751831\n",
      "cnt: 0 - valLoss: 0.6499563455581665 - trainLoss: 0.642196536064148\n",
      "cnt: 0 - valLoss: 0.6494747400283813 - trainLoss: 0.6416715979576111\n",
      "cnt: 0 - valLoss: 0.6489944458007812 - trainLoss: 0.6411476731300354\n",
      "cnt: 0 - valLoss: 0.6485162377357483 - trainLoss: 0.640625536441803\n",
      "cnt: 0 - valLoss: 0.6480398178100586 - trainLoss: 0.6401064395904541\n",
      "cnt: 0 - valLoss: 0.6475657224655151 - trainLoss: 0.6395885348320007\n",
      "cnt: 0 - valLoss: 0.647092878818512 - trainLoss: 0.6390725374221802\n",
      "cnt: 0 - valLoss: 0.6466217041015625 - trainLoss: 0.6385578513145447\n",
      "cnt: 0 - valLoss: 0.6461527347564697 - trainLoss: 0.6380452513694763\n",
      "cnt: 0 - valLoss: 0.6456860303878784 - trainLoss: 0.6375358700752258\n",
      "cnt: 0 - valLoss: 0.6452214121818542 - trainLoss: 0.6370286345481873\n",
      "cnt: 0 - valLoss: 0.6447584629058838 - trainLoss: 0.6365230083465576\n",
      "cnt: 0 - valLoss: 0.6442979574203491 - trainLoss: 0.6360187530517578\n",
      "cnt: 0 - valLoss: 0.643839418888092 - trainLoss: 0.6355161666870117\n",
      "cnt: 0 - valLoss: 0.6433823108673096 - trainLoss: 0.6350163817405701\n",
      "cnt: 0 - valLoss: 0.6429269313812256 - trainLoss: 0.6345183253288269\n",
      "cnt: 0 - valLoss: 0.6424740552902222 - trainLoss: 0.6340230107307434\n",
      "cnt: 0 - valLoss: 0.6420221924781799 - trainLoss: 0.6335307955741882\n",
      "cnt: 0 - valLoss: 0.6415711641311646 - trainLoss: 0.633040726184845\n",
      "cnt: 0 - valLoss: 0.641120970249176 - trainLoss: 0.6325525045394897\n",
      "cnt: 0 - valLoss: 0.6406723260879517 - trainLoss: 0.632064700126648\n",
      "cnt: 0 - valLoss: 0.6402262449264526 - trainLoss: 0.6315786838531494\n",
      "cnt: 0 - valLoss: 0.6397815346717834 - trainLoss: 0.6310948133468628\n",
      "cnt: 0 - valLoss: 0.6393389701843262 - trainLoss: 0.630612850189209\n",
      "cnt: 0 - valLoss: 0.6388983726501465 - trainLoss: 0.630133330821991\n",
      "cnt: 0 - valLoss: 0.6384592652320862 - trainLoss: 0.629655659198761\n",
      "cnt: 0 - valLoss: 0.6380218863487244 - trainLoss: 0.6291801333427429\n",
      "cnt: 0 - valLoss: 0.6375859975814819 - trainLoss: 0.6287071704864502\n",
      "cnt: 0 - valLoss: 0.637151837348938 - trainLoss: 0.6282358169555664\n",
      "cnt: 0 - valLoss: 0.6367195248603821 - trainLoss: 0.6277664303779602\n",
      "cnt: 0 - valLoss: 0.6362887620925903 - trainLoss: 0.6272979378700256\n",
      "cnt: 0 - valLoss: 0.6358596682548523 - trainLoss: 0.6268300414085388\n",
      "cnt: 0 - valLoss: 0.6354320049285889 - trainLoss: 0.626363217830658\n",
      "cnt: 0 - valLoss: 0.635004997253418 - trainLoss: 0.6258979439735413\n",
      "cnt: 0 - valLoss: 0.6345798969268799 - trainLoss: 0.625434160232544\n",
      "cnt: 0 - valLoss: 0.6341557502746582 - trainLoss: 0.6249717473983765\n",
      "cnt: 0 - valLoss: 0.6337319612503052 - trainLoss: 0.6245115399360657\n",
      "cnt: 0 - valLoss: 0.6333093643188477 - trainLoss: 0.6240522265434265\n",
      "cnt: 0 - valLoss: 0.6328873634338379 - trainLoss: 0.6235939860343933\n",
      "cnt: 0 - valLoss: 0.6324664950370789 - trainLoss: 0.623136579990387\n",
      "cnt: 0 - valLoss: 0.6320465207099915 - trainLoss: 0.6226809620857239\n",
      "cnt: 0 - valLoss: 0.6316283345222473 - trainLoss: 0.6222273111343384\n",
      "cnt: 0 - valLoss: 0.631212055683136 - trainLoss: 0.6217756271362305\n",
      "cnt: 0 - valLoss: 0.6307962536811829 - trainLoss: 0.6213253140449524\n",
      "cnt: 0 - valLoss: 0.6303813457489014 - trainLoss: 0.6208757162094116\n",
      "cnt: 0 - valLoss: 0.6299675107002258 - trainLoss: 0.620427131652832\n",
      "cnt: 0 - valLoss: 0.6295540928840637 - trainLoss: 0.6199794411659241\n",
      "cnt: 0 - valLoss: 0.6291419267654419 - trainLoss: 0.6195324659347534\n",
      "cnt: 0 - valLoss: 0.6287307739257812 - trainLoss: 0.619087815284729\n",
      "cnt: 0 - valLoss: 0.6283208727836609 - trainLoss: 0.6186444163322449\n",
      "cnt: 0 - valLoss: 0.6279119849205017 - trainLoss: 0.6182021498680115\n",
      "cnt: 0 - valLoss: 0.6275045275688171 - trainLoss: 0.6177608966827393\n",
      "cnt: 0 - valLoss: 0.6270985007286072 - trainLoss: 0.6173208355903625\n",
      "cnt: 0 - valLoss: 0.62669438123703 - trainLoss: 0.6168815493583679\n",
      "cnt: 0 - valLoss: 0.6262910962104797 - trainLoss: 0.6164435148239136\n",
      "cnt: 0 - valLoss: 0.6258902549743652 - trainLoss: 0.6160064339637756\n",
      "cnt: 0 - valLoss: 0.6254910230636597 - trainLoss: 0.6155707240104675\n",
      "cnt: 0 - valLoss: 0.6250941753387451 - trainLoss: 0.615136444568634\n",
      "cnt: 0 - valLoss: 0.6247005462646484 - trainLoss: 0.6147040128707886\n",
      "cnt: 0 - valLoss: 0.6243093013763428 - trainLoss: 0.614274263381958\n",
      "cnt: 0 - valLoss: 0.6239187121391296 - trainLoss: 0.6138466000556946\n",
      "cnt: 0 - valLoss: 0.6235278248786926 - trainLoss: 0.6134200692176819\n",
      "cnt: 0 - valLoss: 0.6231380105018616 - trainLoss: 0.6129948496818542\n",
      "cnt: 0 - valLoss: 0.6227489113807678 - trainLoss: 0.6125720143318176\n",
      "cnt: 0 - valLoss: 0.6223599314689636 - trainLoss: 0.6121509075164795\n",
      "cnt: 0 - valLoss: 0.6219714283943176 - trainLoss: 0.6117303371429443\n",
      "cnt: 0 - valLoss: 0.6215832829475403 - trainLoss: 0.6113107204437256\n",
      "cnt: 0 - valLoss: 0.6211954951286316 - trainLoss: 0.6108915209770203\n",
      "cnt: 0 - valLoss: 0.6208072304725647 - trainLoss: 0.6104730367660522\n",
      "cnt: 0 - valLoss: 0.6204193830490112 - trainLoss: 0.6100543141365051\n",
      "cnt: 0 - valLoss: 0.620032012462616 - trainLoss: 0.6096362471580505\n",
      "cnt: 0 - valLoss: 0.6196450591087341 - trainLoss: 0.6092186570167542\n",
      "cnt: 0 - valLoss: 0.6192592978477478 - trainLoss: 0.608801543712616\n",
      "cnt: 0 - valLoss: 0.6188735961914062 - trainLoss: 0.6083860993385315\n",
      "cnt: 0 - valLoss: 0.6184877753257751 - trainLoss: 0.6079708933830261\n",
      "cnt: 0 - valLoss: 0.6181020140647888 - trainLoss: 0.6075553297996521\n",
      "cnt: 0 - valLoss: 0.6177161931991577 - trainLoss: 0.607140064239502\n",
      "cnt: 0 - valLoss: 0.6173306107521057 - trainLoss: 0.6067251563072205\n",
      "cnt: 0 - valLoss: 0.6169448494911194 - trainLoss: 0.6063106060028076\n",
      "cnt: 0 - valLoss: 0.6165592670440674 - trainLoss: 0.6058962941169739\n",
      "cnt: 0 - valLoss: 0.6161734461784363 - trainLoss: 0.6054823398590088\n",
      "cnt: 0 - valLoss: 0.6157878041267395 - trainLoss: 0.6050687432289124\n",
      "cnt: 0 - valLoss: 0.6154019236564636 - trainLoss: 0.6046553254127502\n",
      "cnt: 0 - valLoss: 0.6150153875350952 - trainLoss: 0.6042420864105225\n",
      "cnt: 0 - valLoss: 0.614628791809082 - trainLoss: 0.6038288474082947\n",
      "cnt: 0 - valLoss: 0.6142423152923584 - trainLoss: 0.6034159660339355\n",
      "cnt: 0 - valLoss: 0.6138558387756348 - trainLoss: 0.603003203868866\n",
      "cnt: 0 - valLoss: 0.6134691834449768 - trainLoss: 0.6025906801223755\n",
      "cnt: 0 - valLoss: 0.613082230091095 - trainLoss: 0.6021777391433716\n",
      "cnt: 0 - valLoss: 0.6126958131790161 - trainLoss: 0.6017645001411438\n",
      "cnt: 0 - valLoss: 0.6123093962669373 - trainLoss: 0.601351797580719\n",
      "cnt: 0 - valLoss: 0.6119225025177002 - trainLoss: 0.6009393334388733\n",
      "cnt: 0 - valLoss: 0.6115356683731079 - trainLoss: 0.6005271673202515\n",
      "cnt: 0 - valLoss: 0.6111485958099365 - trainLoss: 0.6001150012016296\n",
      "cnt: 0 - valLoss: 0.6107613444328308 - trainLoss: 0.5997025966644287\n",
      "cnt: 0 - valLoss: 0.6103737354278564 - trainLoss: 0.5992898344993591\n",
      "cnt: 0 - valLoss: 0.6099863648414612 - trainLoss: 0.5988768935203552\n",
      "cnt: 0 - valLoss: 0.6095989346504211 - trainLoss: 0.59846431016922\n",
      "cnt: 0 - valLoss: 0.6092114448547363 - trainLoss: 0.5980517864227295\n",
      "cnt: 0 - valLoss: 0.6088239550590515 - trainLoss: 0.5976393818855286\n",
      "cnt: 0 - valLoss: 0.6084365844726562 - trainLoss: 0.5972270369529724\n",
      "cnt: 0 - valLoss: 0.6080490350723267 - trainLoss: 0.5968149304389954\n",
      "cnt: 0 - valLoss: 0.6076606512069702 - trainLoss: 0.5964024662971497\n",
      "cnt: 0 - valLoss: 0.6072719097137451 - trainLoss: 0.5959894061088562\n",
      "cnt: 0 - valLoss: 0.6068831086158752 - trainLoss: 0.595576286315918\n",
      "cnt: 0 - valLoss: 0.6064941883087158 - trainLoss: 0.5951630473136902\n",
      "cnt: 0 - valLoss: 0.606104850769043 - trainLoss: 0.5947498083114624\n",
      "cnt: 0 - valLoss: 0.605715274810791 - trainLoss: 0.594336211681366\n",
      "cnt: 0 - valLoss: 0.6053252220153809 - trainLoss: 0.593921959400177\n",
      "cnt: 0 - valLoss: 0.6049349904060364 - trainLoss: 0.5935074090957642\n",
      "cnt: 0 - valLoss: 0.6045446991920471 - trainLoss: 0.59309321641922\n",
      "cnt: 0 - valLoss: 0.6041537523269653 - trainLoss: 0.592678964138031\n",
      "cnt: 0 - valLoss: 0.6037627458572388 - trainLoss: 0.5922642946243286\n",
      "cnt: 0 - valLoss: 0.6033715605735779 - trainLoss: 0.5918498039245605\n",
      "cnt: 0 - valLoss: 0.60298091173172 - trainLoss: 0.5914350748062134\n",
      "cnt: 0 - valLoss: 0.6025905609130859 - trainLoss: 0.5910206437110901\n",
      "cnt: 0 - valLoss: 0.6022002696990967 - trainLoss: 0.5906065702438354\n",
      "cnt: 0 - valLoss: 0.6018099784851074 - trainLoss: 0.5901926755905151\n",
      "cnt: 0 - valLoss: 0.6014196276664734 - trainLoss: 0.5897787809371948\n",
      "cnt: 0 - valLoss: 0.6010293960571289 - trainLoss: 0.5893646478652954\n",
      "cnt: 0 - valLoss: 0.6006388068199158 - trainLoss: 0.5889506936073303\n",
      "cnt: 0 - valLoss: 0.6002475023269653 - trainLoss: 0.5885367393493652\n",
      "cnt: 0 - valLoss: 0.5998561382293701 - trainLoss: 0.5881226658821106\n",
      "cnt: 0 - valLoss: 0.5994644165039062 - trainLoss: 0.5877086520195007\n",
      "cnt: 0 - valLoss: 0.5990725755691528 - trainLoss: 0.5872940421104431\n",
      "cnt: 0 - valLoss: 0.5986806154251099 - trainLoss: 0.5868792533874512\n",
      "cnt: 0 - valLoss: 0.5982885360717773 - trainLoss: 0.5864643454551697\n",
      "cnt: 0 - valLoss: 0.5978965163230896 - trainLoss: 0.5860496759414673\n",
      "cnt: 0 - valLoss: 0.597503662109375 - trainLoss: 0.5856348872184753\n",
      "cnt: 0 - valLoss: 0.5971109867095947 - trainLoss: 0.585219144821167\n",
      "cnt: 0 - valLoss: 0.596718430519104 - trainLoss: 0.5848036408424377\n",
      "cnt: 0 - valLoss: 0.5963255167007446 - trainLoss: 0.5843880772590637\n",
      "cnt: 0 - valLoss: 0.5959325432777405 - trainLoss: 0.583972156047821\n",
      "cnt: 0 - valLoss: 0.5955395102500916 - trainLoss: 0.5835564732551575\n",
      "cnt: 0 - valLoss: 0.5951465964317322 - trainLoss: 0.5831409096717834\n",
      "cnt: 0 - valLoss: 0.5947539806365967 - trainLoss: 0.5827254056930542\n",
      "cnt: 0 - valLoss: 0.5943613648414612 - trainLoss: 0.582309901714325\n",
      "cnt: 0 - valLoss: 0.5939692258834839 - trainLoss: 0.5818944573402405\n",
      "cnt: 0 - valLoss: 0.5935771465301514 - trainLoss: 0.5814790725708008\n",
      "cnt: 0 - valLoss: 0.5931848287582397 - trainLoss: 0.5810645818710327\n",
      "cnt: 0 - valLoss: 0.5927923917770386 - trainLoss: 0.580649733543396\n",
      "cnt: 0 - valLoss: 0.5923997759819031 - trainLoss: 0.5802348256111145\n",
      "cnt: 0 - valLoss: 0.5920066237449646 - trainLoss: 0.5798199772834778\n",
      "cnt: 0 - valLoss: 0.5916128158569336 - trainLoss: 0.5794052481651306\n",
      "cnt: 0 - valLoss: 0.5912187099456787 - trainLoss: 0.5789902806282043\n",
      "cnt: 0 - valLoss: 0.5908243060112 - trainLoss: 0.5785748958587646\n",
      "cnt: 0 - valLoss: 0.5904295444488525 - trainLoss: 0.578159511089325\n",
      "cnt: 0 - valLoss: 0.5900349617004395 - trainLoss: 0.5777439475059509\n",
      "cnt: 0 - valLoss: 0.5896403193473816 - trainLoss: 0.5773288011550903\n",
      "cnt: 0 - valLoss: 0.5892452597618103 - trainLoss: 0.5769139528274536\n",
      "cnt: 0 - valLoss: 0.5888499617576599 - trainLoss: 0.5764991641044617\n",
      "cnt: 0 - valLoss: 0.58845454454422 - trainLoss: 0.5760841965675354\n",
      "cnt: 0 - valLoss: 0.5880591869354248 - trainLoss: 0.5756687521934509\n",
      "cnt: 0 - valLoss: 0.5876643061637878 - trainLoss: 0.5752536654472351\n",
      "cnt: 0 - valLoss: 0.5872695446014404 - trainLoss: 0.5748389959335327\n",
      "cnt: 0 - valLoss: 0.5868745446205139 - trainLoss: 0.5744245052337646\n",
      "cnt: 0 - valLoss: 0.5864791870117188 - trainLoss: 0.5740100145339966\n",
      "cnt: 0 - valLoss: 0.5860834121704102 - trainLoss: 0.5735957026481628\n",
      "cnt: 0 - valLoss: 0.5856871604919434 - trainLoss: 0.57318115234375\n",
      "cnt: 0 - valLoss: 0.585290789604187 - trainLoss: 0.5727660059928894\n",
      "cnt: 0 - valLoss: 0.5848942399024963 - trainLoss: 0.5723507404327393\n",
      "cnt: 0 - valLoss: 0.5844979882240295 - trainLoss: 0.5719349980354309\n",
      "cnt: 0 - valLoss: 0.5841001868247986 - trainLoss: 0.5715199708938599\n",
      "cnt: 0 - valLoss: 0.5836978554725647 - trainLoss: 0.5711050033569336\n",
      "cnt: 0 - valLoss: 0.5832955837249756 - trainLoss: 0.5706881880760193\n",
      "cnt: 0 - valLoss: 0.582892656326294 - trainLoss: 0.5702717900276184\n",
      "cnt: 0 - valLoss: 0.5824896097183228 - trainLoss: 0.5698553919792175\n",
      "cnt: 0 - valLoss: 0.5820862054824829 - trainLoss: 0.5694392919540405\n",
      "cnt: 0 - valLoss: 0.5816826224327087 - trainLoss: 0.5690237283706665\n",
      "cnt: 0 - valLoss: 0.581278920173645 - trainLoss: 0.5686084032058716\n",
      "cnt: 0 - valLoss: 0.5808743238449097 - trainLoss: 0.5681930184364319\n",
      "cnt: 0 - valLoss: 0.5804696083068848 - trainLoss: 0.5677770972251892\n",
      "cnt: 0 - valLoss: 0.5800649523735046 - trainLoss: 0.5673614144325256\n",
      "cnt: 0 - valLoss: 0.5796599984169006 - trainLoss: 0.566945493221283\n",
      "cnt: 0 - valLoss: 0.5792545080184937 - trainLoss: 0.5665290951728821\n",
      "cnt: 0 - valLoss: 0.5788493752479553 - trainLoss: 0.566112220287323\n",
      "cnt: 0 - valLoss: 0.5784445405006409 - trainLoss: 0.5656954646110535\n",
      "cnt: 0 - valLoss: 0.5780397057533264 - trainLoss: 0.5652792453765869\n",
      "cnt: 0 - valLoss: 0.5776347517967224 - trainLoss: 0.5648632645606995\n",
      "cnt: 0 - valLoss: 0.5772298574447632 - trainLoss: 0.5644477009773254\n",
      "cnt: 0 - valLoss: 0.576824963092804 - trainLoss: 0.5640317797660828\n",
      "cnt: 0 - valLoss: 0.5764208436012268 - trainLoss: 0.5636160969734192\n",
      "cnt: 0 - valLoss: 0.5760165452957153 - trainLoss: 0.5632011294364929\n",
      "cnt: 0 - valLoss: 0.575612485408783 - trainLoss: 0.5627864599227905\n",
      "cnt: 0 - valLoss: 0.5752084255218506 - trainLoss: 0.562372088432312\n",
      "cnt: 0 - valLoss: 0.574804961681366 - trainLoss: 0.5619581937789917\n",
      "cnt: 0 - valLoss: 0.5744012594223022 - trainLoss: 0.5615440607070923\n",
      "cnt: 0 - valLoss: 0.5739979147911072 - trainLoss: 0.561129629611969\n",
      "cnt: 0 - valLoss: 0.573594331741333 - trainLoss: 0.56071537733078\n",
      "cnt: 0 - valLoss: 0.5731908679008484 - trainLoss: 0.5603011846542358\n",
      "cnt: 0 - valLoss: 0.5727876424789429 - trainLoss: 0.5598872900009155\n",
      "cnt: 0 - valLoss: 0.5723844766616821 - trainLoss: 0.5594735741615295\n",
      "cnt: 0 - valLoss: 0.5719817280769348 - trainLoss: 0.5590601563453674\n",
      "cnt: 0 - valLoss: 0.57157963514328 - trainLoss: 0.5586474537849426\n",
      "cnt: 0 - valLoss: 0.5711779594421387 - trainLoss: 0.5582355856895447\n",
      "cnt: 0 - valLoss: 0.570776641368866 - trainLoss: 0.5578241348266602\n",
      "cnt: 0 - valLoss: 0.5703755021095276 - trainLoss: 0.5574129819869995\n",
      "cnt: 0 - valLoss: 0.5699746608734131 - trainLoss: 0.5570023059844971\n",
      "cnt: 0 - valLoss: 0.5695745348930359 - trainLoss: 0.5565923452377319\n",
      "cnt: 0 - valLoss: 0.569174587726593 - trainLoss: 0.5561830401420593\n",
      "cnt: 0 - valLoss: 0.5687752962112427 - trainLoss: 0.5557742714881897\n",
      "cnt: 0 - valLoss: 0.5683757066726685 - trainLoss: 0.5553661584854126\n",
      "cnt: 0 - valLoss: 0.5679765939712524 - trainLoss: 0.5549582839012146\n",
      "cnt: 0 - valLoss: 0.5675780177116394 - trainLoss: 0.5545510053634644\n",
      "cnt: 0 - valLoss: 0.5671802163124084 - trainLoss: 0.5541442632675171\n",
      "cnt: 0 - valLoss: 0.5667828917503357 - trainLoss: 0.5537382364273071\n",
      "cnt: 0 - valLoss: 0.5663861632347107 - trainLoss: 0.5533325672149658\n",
      "cnt: 0 - valLoss: 0.5659900903701782 - trainLoss: 0.5529273748397827\n",
      "cnt: 0 - valLoss: 0.5655938982963562 - trainLoss: 0.5525227785110474\n",
      "cnt: 0 - valLoss: 0.5651976466178894 - trainLoss: 0.5521185398101807\n",
      "cnt: 0 - valLoss: 0.5648021697998047 - trainLoss: 0.5517144799232483\n",
      "cnt: 0 - valLoss: 0.5644068121910095 - trainLoss: 0.5513112545013428\n",
      "cnt: 0 - valLoss: 0.5640116333961487 - trainLoss: 0.550908088684082\n",
      "cnt: 0 - valLoss: 0.5636173486709595 - trainLoss: 0.5505050420761108\n",
      "cnt: 0 - valLoss: 0.5632234215736389 - trainLoss: 0.5501023530960083\n",
      "cnt: 0 - valLoss: 0.5628302097320557 - trainLoss: 0.549700140953064\n",
      "cnt: 0 - valLoss: 0.5624374151229858 - trainLoss: 0.5492990016937256\n",
      "cnt: 0 - valLoss: 0.5620448589324951 - trainLoss: 0.5488982796669006\n",
      "cnt: 0 - valLoss: 0.5616530179977417 - trainLoss: 0.5484975576400757\n",
      "cnt: 0 - valLoss: 0.5612621307373047 - trainLoss: 0.5480974912643433\n",
      "cnt: 0 - valLoss: 0.5608722567558289 - trainLoss: 0.5476981997489929\n",
      "cnt: 0 - valLoss: 0.5604833364486694 - trainLoss: 0.5472996830940247\n",
      "cnt: 0 - valLoss: 0.560095489025116 - trainLoss: 0.5469021201133728\n",
      "cnt: 0 - valLoss: 0.5597084760665894 - trainLoss: 0.5465051531791687\n",
      "cnt: 0 - valLoss: 0.5593225359916687 - trainLoss: 0.5461089015007019\n",
      "cnt: 0 - valLoss: 0.5589371919631958 - trainLoss: 0.5457135438919067\n",
      "cnt: 0 - valLoss: 0.5585522651672363 - trainLoss: 0.5453192591667175\n",
      "cnt: 0 - valLoss: 0.5581675171852112 - trainLoss: 0.5449256300926208\n",
      "cnt: 0 - valLoss: 0.5577827095985413 - trainLoss: 0.5445321798324585\n",
      "cnt: 0 - valLoss: 0.5573980808258057 - trainLoss: 0.544139564037323\n",
      "cnt: 0 - valLoss: 0.5570141673088074 - trainLoss: 0.54374760389328\n",
      "cnt: 0 - valLoss: 0.5566306710243225 - trainLoss: 0.5433564782142639\n",
      "cnt: 0 - valLoss: 0.5562478303909302 - trainLoss: 0.5429657101631165\n",
      "cnt: 0 - valLoss: 0.5558658242225647 - trainLoss: 0.5425754189491272\n",
      "cnt: 0 - valLoss: 0.5554841756820679 - trainLoss: 0.5421857833862305\n",
      "cnt: 0 - valLoss: 0.5551032423973083 - trainLoss: 0.5417968034744263\n",
      "cnt: 0 - valLoss: 0.5547235608100891 - trainLoss: 0.5414089560508728\n",
      "cnt: 0 - valLoss: 0.5543442368507385 - trainLoss: 0.541022002696991\n",
      "cnt: 0 - valLoss: 0.5539652705192566 - trainLoss: 0.5406355261802673\n",
      "cnt: 0 - valLoss: 0.5535867810249329 - trainLoss: 0.5402494072914124\n",
      "cnt: 0 - valLoss: 0.5532090663909912 - trainLoss: 0.5398637056350708\n",
      "cnt: 0 - valLoss: 0.5528298616409302 - trainLoss: 0.5394789576530457\n",
      "cnt: 0 - valLoss: 0.5524501204490662 - trainLoss: 0.5390946865081787\n",
      "cnt: 0 - valLoss: 0.5520710945129395 - trainLoss: 0.5387106537818909\n",
      "cnt: 0 - valLoss: 0.5516926646232605 - trainLoss: 0.5383277535438538\n",
      "cnt: 0 - valLoss: 0.5513145923614502 - trainLoss: 0.537945568561554\n",
      "cnt: 0 - valLoss: 0.5509365797042847 - trainLoss: 0.537564218044281\n",
      "cnt: 0 - valLoss: 0.5505586862564087 - trainLoss: 0.5371832251548767\n",
      "cnt: 0 - valLoss: 0.5501810908317566 - trainLoss: 0.5368026494979858\n",
      "cnt: 0 - valLoss: 0.5498040914535522 - trainLoss: 0.5364224314689636\n",
      "cnt: 0 - valLoss: 0.5494282841682434 - trainLoss: 0.5360429883003235\n",
      "cnt: 0 - valLoss: 0.5490527153015137 - trainLoss: 0.5356644988059998\n",
      "cnt: 0 - valLoss: 0.5486778020858765 - trainLoss: 0.5352861881256104\n",
      "cnt: 0 - valLoss: 0.5483036041259766 - trainLoss: 0.5349084734916687\n",
      "cnt: 0 - valLoss: 0.5479303002357483 - trainLoss: 0.5345315933227539\n",
      "cnt: 0 - valLoss: 0.5475575923919678 - trainLoss: 0.5341556072235107\n",
      "cnt: 0 - valLoss: 0.5471854209899902 - trainLoss: 0.5337803363800049\n",
      "cnt: 0 - valLoss: 0.54681396484375 - trainLoss: 0.5334057807922363\n",
      "cnt: 0 - valLoss: 0.5464432835578918 - trainLoss: 0.5330319404602051\n",
      "cnt: 0 - valLoss: 0.5460731387138367 - trainLoss: 0.5326589941978455\n",
      "cnt: 0 - valLoss: 0.5457035303115845 - trainLoss: 0.532286524772644\n",
      "cnt: 0 - valLoss: 0.5453349351882935 - trainLoss: 0.5319147109985352\n",
      "cnt: 0 - valLoss: 0.5449671745300293 - trainLoss: 0.5315437316894531\n",
      "cnt: 0 - valLoss: 0.544600248336792 - trainLoss: 0.5311735272407532\n",
      "cnt: 0 - valLoss: 0.544234037399292 - trainLoss: 0.5308040976524353\n",
      "cnt: 0 - valLoss: 0.543868362903595 - trainLoss: 0.5304355025291443\n",
      "cnt: 0 - valLoss: 0.54350346326828 - trainLoss: 0.5300673246383667\n",
      "cnt: 0 - valLoss: 0.5431390404701233 - trainLoss: 0.5297000408172607\n",
      "cnt: 0 - valLoss: 0.5427752733230591 - trainLoss: 0.5293334722518921\n",
      "cnt: 0 - valLoss: 0.5424124598503113 - trainLoss: 0.5289677381515503\n",
      "cnt: 0 - valLoss: 0.5420504808425903 - trainLoss: 0.5286029577255249\n",
      "cnt: 0 - valLoss: 0.5416902303695679 - trainLoss: 0.5282393097877502\n",
      "cnt: 0 - valLoss: 0.541330873966217 - trainLoss: 0.527877151966095\n",
      "cnt: 0 - valLoss: 0.5409721732139587 - trainLoss: 0.5275158286094666\n",
      "cnt: 0 - valLoss: 0.5406142473220825 - trainLoss: 0.5271555185317993\n",
      "cnt: 0 - valLoss: 0.5402570962905884 - trainLoss: 0.5267958641052246\n",
      "cnt: 0 - valLoss: 0.5399004817008972 - trainLoss: 0.526436984539032\n",
      "cnt: 0 - valLoss: 0.5395445227622986 - trainLoss: 0.5260786414146423\n",
      "cnt: 0 - valLoss: 0.5391893982887268 - trainLoss: 0.5257213115692139\n",
      "cnt: 0 - valLoss: 0.5388351082801819 - trainLoss: 0.525364875793457\n",
      "cnt: 0 - valLoss: 0.5384816527366638 - trainLoss: 0.5250094532966614\n",
      "cnt: 0 - valLoss: 0.5381289720535278 - trainLoss: 0.5246549844741821\n",
      "cnt: 0 - valLoss: 0.5377772450447083 - trainLoss: 0.5243015885353088\n",
      "cnt: 0 - valLoss: 0.537426233291626 - trainLoss: 0.5239490270614624\n",
      "cnt: 0 - valLoss: 0.5370758771896362 - trainLoss: 0.523597240447998\n",
      "cnt: 0 - valLoss: 0.5367263555526733 - trainLoss: 0.5232460498809814\n",
      "cnt: 0 - valLoss: 0.5363777279853821 - trainLoss: 0.522895872592926\n",
      "cnt: 0 - valLoss: 0.5360296964645386 - trainLoss: 0.522546648979187\n",
      "cnt: 0 - valLoss: 0.5356826186180115 - trainLoss: 0.5221981406211853\n",
      "cnt: 0 - valLoss: 0.5353362560272217 - trainLoss: 0.5218507051467896\n",
      "cnt: 0 - valLoss: 0.5349907875061035 - trainLoss: 0.5215044021606445\n",
      "cnt: 0 - valLoss: 0.5346461534500122 - trainLoss: 0.5211589336395264\n",
      "cnt: 0 - valLoss: 0.534302294254303 - trainLoss: 0.5208144187927246\n",
      "cnt: 0 - valLoss: 0.5339592099189758 - trainLoss: 0.5204707384109497\n",
      "cnt: 0 - valLoss: 0.5336170792579651 - trainLoss: 0.5201281309127808\n",
      "cnt: 0 - valLoss: 0.5332757830619812 - trainLoss: 0.5197863578796387\n",
      "cnt: 0 - valLoss: 0.5329355001449585 - trainLoss: 0.5194457769393921\n",
      "cnt: 0 - valLoss: 0.5325960516929626 - trainLoss: 0.5191063284873962\n",
      "cnt: 0 - valLoss: 0.5322574973106384 - trainLoss: 0.5187678933143616\n",
      "cnt: 0 - valLoss: 0.5319198966026306 - trainLoss: 0.5184304714202881\n",
      "cnt: 0 - valLoss: 0.5315831899642944 - trainLoss: 0.5180941820144653\n",
      "cnt: 0 - valLoss: 0.5312474370002747 - trainLoss: 0.517758846282959\n",
      "cnt: 0 - valLoss: 0.530912458896637 - trainLoss: 0.5174245238304138\n",
      "cnt: 0 - valLoss: 0.5305784344673157 - trainLoss: 0.5170910358428955\n",
      "cnt: 0 - valLoss: 0.5302453637123108 - trainLoss: 0.5167586207389832\n",
      "cnt: 0 - valLoss: 0.529913067817688 - trainLoss: 0.5164271593093872\n",
      "cnt: 0 - valLoss: 0.5295817255973816 - trainLoss: 0.5160965323448181\n",
      "cnt: 0 - valLoss: 0.5292508602142334 - trainLoss: 0.5157670974731445\n",
      "cnt: 0 - valLoss: 0.5289208889007568 - trainLoss: 0.5154386758804321\n",
      "cnt: 0 - valLoss: 0.5285918712615967 - trainLoss: 0.5151110291481018\n",
      "cnt: 0 - valLoss: 0.5282636880874634 - trainLoss: 0.5147843956947327\n",
      "cnt: 0 - valLoss: 0.5279365181922913 - trainLoss: 0.5144585967063904\n",
      "cnt: 0 - valLoss: 0.527610182762146 - trainLoss: 0.5141338109970093\n",
      "cnt: 0 - valLoss: 0.5272846221923828 - trainLoss: 0.513809859752655\n",
      "cnt: 0 - valLoss: 0.5269599556922913 - trainLoss: 0.5134869813919067\n",
      "cnt: 0 - valLoss: 0.5266360640525818 - trainLoss: 0.5131650567054749\n",
      "cnt: 0 - valLoss: 0.5263135433197021 - trainLoss: 0.5128440856933594\n",
      "cnt: 0 - valLoss: 0.5259920358657837 - trainLoss: 0.5125243067741394\n",
      "cnt: 0 - valLoss: 0.525671660900116 - trainLoss: 0.5122057199478149\n",
      "cnt: 0 - valLoss: 0.5253527164459229 - trainLoss: 0.5118883848190308\n",
      "cnt: 0 - valLoss: 0.5250347256660461 - trainLoss: 0.5115724205970764\n",
      "cnt: 0 - valLoss: 0.5247177481651306 - trainLoss: 0.5112574696540833\n",
      "cnt: 0 - valLoss: 0.5244016647338867 - trainLoss: 0.510943591594696\n",
      "cnt: 0 - valLoss: 0.5240861177444458 - trainLoss: 0.5106306672096252\n",
      "cnt: 0 - valLoss: 0.5237714052200317 - trainLoss: 0.5103183388710022\n",
      "cnt: 0 - valLoss: 0.5234578251838684 - trainLoss: 0.5100069046020508\n",
      "cnt: 0 - valLoss: 0.5231456160545349 - trainLoss: 0.5096967816352844\n",
      "cnt: 0 - valLoss: 0.5228344798088074 - trainLoss: 0.509387731552124\n",
      "cnt: 0 - valLoss: 0.5225240588188171 - trainLoss: 0.50907963514328\n",
      "cnt: 0 - valLoss: 0.5222147107124329 - trainLoss: 0.508772611618042\n",
      "cnt: 0 - valLoss: 0.5219064950942993 - trainLoss: 0.5084666013717651\n",
      "cnt: 0 - valLoss: 0.5215994715690613 - trainLoss: 0.5081616640090942\n",
      "cnt: 0 - valLoss: 0.5212931632995605 - trainLoss: 0.5078577995300293\n",
      "cnt: 0 - valLoss: 0.5209875106811523 - trainLoss: 0.5075549483299255\n",
      "cnt: 0 - valLoss: 0.5206829309463501 - trainLoss: 0.5072529315948486\n",
      "cnt: 0 - valLoss: 0.5203794240951538 - trainLoss: 0.506952166557312\n",
      "cnt: 0 - valLoss: 0.5200769901275635 - trainLoss: 0.5066525936126709\n",
      "cnt: 0 - valLoss: 0.5197755098342896 - trainLoss: 0.5063541531562805\n",
      "cnt: 0 - valLoss: 0.5194750428199768 - trainLoss: 0.5060567855834961\n",
      "cnt: 0 - valLoss: 0.5191755294799805 - trainLoss: 0.5057605504989624\n",
      "cnt: 0 - valLoss: 0.5188769698143005 - trainLoss: 0.5054652690887451\n",
      "cnt: 0 - valLoss: 0.518579363822937 - trainLoss: 0.5051709413528442\n",
      "cnt: 0 - valLoss: 0.5182828903198242 - trainLoss: 0.5048776268959045\n",
      "cnt: 0 - valLoss: 0.5179875493049622 - trainLoss: 0.5045855641365051\n",
      "cnt: 0 - valLoss: 0.5176929235458374 - trainLoss: 0.5042945742607117\n",
      "cnt: 0 - valLoss: 0.5173993706703186 - trainLoss: 0.5040045976638794\n",
      "cnt: 0 - valLoss: 0.5171061754226685 - trainLoss: 0.5037157535552979\n",
      "cnt: 0 - valLoss: 0.516814112663269 - trainLoss: 0.5034278035163879\n",
      "cnt: 0 - valLoss: 0.5165229439735413 - trainLoss: 0.503140926361084\n",
      "cnt: 0 - valLoss: 0.5162326693534851 - trainLoss: 0.5028551816940308\n",
      "cnt: 0 - valLoss: 0.5159435868263245 - trainLoss: 0.5025704503059387\n",
      "cnt: 0 - valLoss: 0.5156554579734802 - trainLoss: 0.5022867918014526\n",
      "cnt: 0 - valLoss: 0.5153682231903076 - trainLoss: 0.5020042061805725\n",
      "cnt: 0 - valLoss: 0.5150819420814514 - trainLoss: 0.5017226934432983\n",
      "cnt: 0 - valLoss: 0.5147968530654907 - trainLoss: 0.5014422535896301\n",
      "cnt: 0 - valLoss: 0.5145135521888733 - trainLoss: 0.5011631846427917\n",
      "cnt: 0 - valLoss: 0.514231264591217 - trainLoss: 0.5008856654167175\n",
      "cnt: 0 - valLoss: 0.5139501690864563 - trainLoss: 0.5006093978881836\n",
      "cnt: 0 - valLoss: 0.5136701464653015 - trainLoss: 0.5003342628479004\n",
      "cnt: 0 - valLoss: 0.5133909583091736 - trainLoss: 0.5000602602958679\n",
      "cnt: 0 - valLoss: 0.5131128430366516 - trainLoss: 0.49978721141815186\n",
      "cnt: 0 - valLoss: 0.512835681438446 - trainLoss: 0.49951523542404175\n",
      "cnt: 0 - valLoss: 0.5125595331192017 - trainLoss: 0.4992446005344391\n",
      "cnt: 0 - valLoss: 0.5122841596603394 - trainLoss: 0.49897500872612\n",
      "cnt: 0 - valLoss: 0.5120096206665039 - trainLoss: 0.4987064301967621\n",
      "cnt: 0 - valLoss: 0.5117358565330505 - trainLoss: 0.49843889474868774\n",
      "cnt: 0 - valLoss: 0.5114629864692688 - trainLoss: 0.49817243218421936\n",
      "cnt: 0 - valLoss: 0.5111913681030273 - trainLoss: 0.4979068636894226\n",
      "cnt: 0 - valLoss: 0.5109208822250366 - trainLoss: 0.49764230847358704\n",
      "cnt: 0 - valLoss: 0.5106512308120728 - trainLoss: 0.49737876653671265\n",
      "cnt: 0 - valLoss: 0.5103825330734253 - trainLoss: 0.4971162974834442\n",
      "cnt: 0 - valLoss: 0.5101146697998047 - trainLoss: 0.49685487151145935\n",
      "cnt: 0 - valLoss: 0.5098476409912109 - trainLoss: 0.4965943694114685\n",
      "cnt: 0 - valLoss: 0.5095816850662231 - trainLoss: 0.49633485078811646\n",
      "cnt: 0 - valLoss: 0.509316623210907 - trainLoss: 0.49607643485069275\n",
      "cnt: 0 - valLoss: 0.509052574634552 - trainLoss: 0.4958190321922302\n",
      "cnt: 0 - valLoss: 0.5087899565696716 - trainLoss: 0.49556291103363037\n",
      "cnt: 0 - valLoss: 0.5085283517837524 - trainLoss: 0.4953082203865051\n",
      "cnt: 0 - valLoss: 0.5082677006721497 - trainLoss: 0.4950546324253082\n",
      "cnt: 0 - valLoss: 0.5080080628395081 - trainLoss: 0.49480199813842773\n",
      "cnt: 0 - valLoss: 0.5077494382858276 - trainLoss: 0.4945503771305084\n",
      "cnt: 0 - valLoss: 0.5074918866157532 - trainLoss: 0.49429985880851746\n",
      "cnt: 0 - valLoss: 0.5072349309921265 - trainLoss: 0.4940502941608429\n",
      "cnt: 0 - valLoss: 0.5069791674613953 - trainLoss: 0.49380171298980713\n",
      "cnt: 0 - valLoss: 0.5067242980003357 - trainLoss: 0.4935542345046997\n",
      "cnt: 0 - valLoss: 0.506470263004303 - trainLoss: 0.4933077096939087\n",
      "cnt: 0 - valLoss: 0.5062170028686523 - trainLoss: 0.4930620789527893\n",
      "cnt: 0 - valLoss: 0.5059648156166077 - trainLoss: 0.49281740188598633\n",
      "cnt: 0 - valLoss: 0.5057138800621033 - trainLoss: 0.4925738573074341\n",
      "cnt: 0 - valLoss: 0.5054638981819153 - trainLoss: 0.49233147501945496\n",
      "cnt: 0 - valLoss: 0.5052149891853333 - trainLoss: 0.49209001660346985\n",
      "cnt: 0 - valLoss: 0.5049667954444885 - trainLoss: 0.49184975028038025\n",
      "cnt: 0 - valLoss: 0.5047197341918945 - trainLoss: 0.4916103184223175\n",
      "cnt: 0 - valLoss: 0.5044736266136169 - trainLoss: 0.49137189984321594\n",
      "cnt: 0 - valLoss: 0.5042284727096558 - trainLoss: 0.49113455414772034\n",
      "cnt: 0 - valLoss: 0.5039843320846558 - trainLoss: 0.4908982217311859\n",
      "cnt: 0 - valLoss: 0.5037412047386169 - trainLoss: 0.49066293239593506\n",
      "cnt: 0 - valLoss: 0.503498911857605 - trainLoss: 0.4904286861419678\n",
      "cnt: 0 - valLoss: 0.5032576322555542 - trainLoss: 0.4901953935623169\n",
      "cnt: 0 - valLoss: 0.5030173659324646 - trainLoss: 0.4899631440639496\n",
      "cnt: 0 - valLoss: 0.5027778744697571 - trainLoss: 0.4897318482398987\n",
      "cnt: 0 - valLoss: 0.5025398135185242 - trainLoss: 0.4895014762878418\n",
      "cnt: 0 - valLoss: 0.502302885055542 - trainLoss: 0.4892719089984894\n",
      "cnt: 0 - valLoss: 0.5020669102668762 - trainLoss: 0.4890434443950653\n",
      "cnt: 0 - valLoss: 0.5018316507339478 - trainLoss: 0.4888159930706024\n",
      "cnt: 0 - valLoss: 0.5015974640846252 - trainLoss: 0.4885895550251007\n",
      "cnt: 0 - valLoss: 0.5013641715049744 - trainLoss: 0.48836395144462585\n",
      "cnt: 0 - valLoss: 0.5011315941810608 - trainLoss: 0.48813897371292114\n",
      "cnt: 0 - valLoss: 0.5009000897407532 - trainLoss: 0.4879150390625\n",
      "cnt: 0 - valLoss: 0.5006696581840515 - trainLoss: 0.48769211769104004\n",
      "cnt: 0 - valLoss: 0.5004397034645081 - trainLoss: 0.4874700903892517\n",
      "cnt: 0 - valLoss: 0.500211238861084 - trainLoss: 0.48724889755249023\n",
      "cnt: 0 - valLoss: 0.499983549118042 - trainLoss: 0.48702865839004517\n",
      "cnt: 0 - valLoss: 0.4997568428516388 - trainLoss: 0.48680925369262695\n",
      "cnt: 0 - valLoss: 0.4995311498641968 - trainLoss: 0.48659080266952515\n",
      "cnt: 0 - valLoss: 0.4993063807487488 - trainLoss: 0.4863732159137726\n",
      "cnt: 0 - valLoss: 0.49908286333084106 - trainLoss: 0.48615655303001404\n",
      "cnt: 0 - valLoss: 0.4988602101802826 - trainLoss: 0.485941082239151\n",
      "cnt: 0 - valLoss: 0.4986386001110077 - trainLoss: 0.48572656512260437\n",
      "cnt: 0 - valLoss: 0.4984179437160492 - trainLoss: 0.4855131506919861\n",
      "cnt: 0 - valLoss: 0.4981982111930847 - trainLoss: 0.4853006601333618\n",
      "cnt: 0 - valLoss: 0.49797937273979187 - trainLoss: 0.48508912324905396\n",
      "cnt: 0 - valLoss: 0.4977613687515259 - trainLoss: 0.48487862944602966\n",
      "cnt: 0 - valLoss: 0.4975442588329315 - trainLoss: 0.48466911911964417\n",
      "cnt: 0 - valLoss: 0.497327983379364 - trainLoss: 0.48446059226989746\n",
      "cnt: 0 - valLoss: 0.4971127212047577 - trainLoss: 0.48425307869911194\n",
      "cnt: 0 - valLoss: 0.4968984127044678 - trainLoss: 0.4840465486049652\n",
      "cnt: 0 - valLoss: 0.49668505787849426 - trainLoss: 0.4838410019874573\n",
      "cnt: 0 - valLoss: 0.49647241830825806 - trainLoss: 0.48363640904426575\n",
      "cnt: 0 - valLoss: 0.4962606132030487 - trainLoss: 0.4834328293800354\n",
      "cnt: 0 - valLoss: 0.4960496425628662 - trainLoss: 0.4832301437854767\n",
      "cnt: 0 - valLoss: 0.49583956599235535 - trainLoss: 0.48302820324897766\n",
      "cnt: 0 - valLoss: 0.495630145072937 - trainLoss: 0.48282724618911743\n",
      "cnt: 0 - valLoss: 0.49542155861854553 - trainLoss: 0.48262718319892883\n",
      "cnt: 0 - valLoss: 0.49521398544311523 - trainLoss: 0.48242804408073425\n",
      "cnt: 0 - valLoss: 0.4950074553489685 - trainLoss: 0.4822298586368561\n",
      "cnt: 0 - valLoss: 0.4948018491268158 - trainLoss: 0.4820326864719391\n",
      "cnt: 0 - valLoss: 0.4945971369743347 - trainLoss: 0.4818364083766937\n",
      "cnt: 0 - valLoss: 0.4943932294845581 - trainLoss: 0.48164102435112\n",
      "cnt: 0 - valLoss: 0.4941902756690979 - trainLoss: 0.48144659399986267\n",
      "cnt: 0 - valLoss: 0.4939882755279541 - trainLoss: 0.48125311732292175\n",
      "cnt: 0 - valLoss: 0.4937872290611267 - trainLoss: 0.4810607135295868\n",
      "cnt: 0 - valLoss: 0.49358707666397095 - trainLoss: 0.4808691442012787\n",
      "cnt: 0 - valLoss: 0.49338796734809875 - trainLoss: 0.4806785583496094\n",
      "cnt: 0 - valLoss: 0.4931900203227997 - trainLoss: 0.48048898577690125\n",
      "cnt: 0 - valLoss: 0.49299290776252747 - trainLoss: 0.4803002178668976\n",
      "cnt: 0 - valLoss: 0.4927966594696045 - trainLoss: 0.4801125228404999\n",
      "cnt: 0 - valLoss: 0.49260130524635315 - trainLoss: 0.47992584109306335\n",
      "cnt: 0 - valLoss: 0.49240678548812866 - trainLoss: 0.47974008321762085\n",
      "cnt: 0 - valLoss: 0.49221327900886536 - trainLoss: 0.47955521941185\n",
      "cnt: 0 - valLoss: 0.49202069640159607 - trainLoss: 0.47937142848968506\n",
      "cnt: 0 - valLoss: 0.4918288290500641 - trainLoss: 0.47918859124183655\n",
      "cnt: 0 - valLoss: 0.4916374385356903 - trainLoss: 0.47900664806365967\n",
      "cnt: 0 - valLoss: 0.4914470911026001 - trainLoss: 0.4788253903388977\n",
      "cnt: 0 - valLoss: 0.4912574589252472 - trainLoss: 0.47864511609077454\n",
      "cnt: 0 - valLoss: 0.4910687506198883 - trainLoss: 0.478465735912323\n",
      "cnt: 0 - valLoss: 0.4908808469772339 - trainLoss: 0.47828736901283264\n",
      "cnt: 0 - valLoss: 0.49069395661354065 - trainLoss: 0.47810983657836914\n",
      "cnt: 0 - valLoss: 0.4905078411102295 - trainLoss: 0.47793304920196533\n",
      "cnt: 0 - valLoss: 0.49032288789749146 - trainLoss: 0.47775712609291077\n",
      "cnt: 0 - valLoss: 0.49013853073120117 - trainLoss: 0.47758203744888306\n",
      "cnt: 0 - valLoss: 0.48995521664619446 - trainLoss: 0.47740820050239563\n",
      "cnt: 0 - valLoss: 0.4897726774215698 - trainLoss: 0.47723519802093506\n",
      "cnt: 0 - valLoss: 0.48959100246429443 - trainLoss: 0.47706297039985657\n",
      "cnt: 0 - valLoss: 0.48941004276275635 - trainLoss: 0.47689154744148254\n",
      "cnt: 0 - valLoss: 0.48922985792160034 - trainLoss: 0.4767209589481354\n",
      "cnt: 0 - valLoss: 0.48905059695243835 - trainLoss: 0.47655126452445984\n",
      "cnt: 0 - valLoss: 0.488872230052948 - trainLoss: 0.47638246417045593\n",
      "cnt: 0 - valLoss: 0.4886946678161621 - trainLoss: 0.4762144684791565\n",
      "cnt: 0 - valLoss: 0.488518089056015 - trainLoss: 0.47604742646217346\n",
      "cnt: 0 - valLoss: 0.48834228515625 - trainLoss: 0.4758812487125397\n",
      "cnt: 0 - valLoss: 0.48816728591918945 - trainLoss: 0.4757160246372223\n",
      "cnt: 0 - valLoss: 0.4879930913448334 - trainLoss: 0.4755518138408661\n",
      "cnt: 0 - valLoss: 0.48781970143318176 - trainLoss: 0.4753883183002472\n",
      "cnt: 0 - valLoss: 0.4876471161842346 - trainLoss: 0.47522568702697754\n",
      "cnt: 0 - valLoss: 0.4874754548072815 - trainLoss: 0.4750640094280243\n",
      "cnt: 0 - valLoss: 0.4873046278953552 - trainLoss: 0.4749031066894531\n",
      "cnt: 0 - valLoss: 0.487134724855423 - trainLoss: 0.4747430980205536\n",
      "cnt: 0 - valLoss: 0.48696544766426086 - trainLoss: 0.4745838940143585\n",
      "cnt: 0 - valLoss: 0.48679685592651367 - trainLoss: 0.4744254946708679\n",
      "cnt: 0 - valLoss: 0.4866292476654053 - trainLoss: 0.47426801919937134\n",
      "cnt: 0 - valLoss: 0.48646241426467896 - trainLoss: 0.4741114675998688\n",
      "cnt: 0 - valLoss: 0.4862964153289795 - trainLoss: 0.4739556908607483\n",
      "cnt: 0 - valLoss: 0.4861311912536621 - trainLoss: 0.4738006591796875\n",
      "cnt: 0 - valLoss: 0.48596683144569397 - trainLoss: 0.47364625334739685\n",
      "cnt: 0 - valLoss: 0.4858032763004303 - trainLoss: 0.47349268198013306\n",
      "cnt: 0 - valLoss: 0.4856404960155487 - trainLoss: 0.4733399748802185\n",
      "cnt: 0 - valLoss: 0.4854784309864044 - trainLoss: 0.4731878638267517\n",
      "cnt: 0 - valLoss: 0.4853171706199646 - trainLoss: 0.47303664684295654\n",
      "cnt: 0 - valLoss: 0.48515668511390686 - trainLoss: 0.4728861451148987\n",
      "cnt: 0 - valLoss: 0.4849970042705536 - trainLoss: 0.4727364480495453\n",
      "cnt: 0 - valLoss: 0.4848381280899048 - trainLoss: 0.47258758544921875\n",
      "cnt: 0 - valLoss: 0.4846801161766052 - trainLoss: 0.47243955731391907\n",
      "cnt: 0 - valLoss: 0.4845227897167206 - trainLoss: 0.4722920060157776\n",
      "cnt: 0 - valLoss: 0.4843664765357971 - trainLoss: 0.47214528918266296\n",
      "cnt: 0 - valLoss: 0.4842108190059662 - trainLoss: 0.4719993770122528\n",
      "cnt: 0 - valLoss: 0.48405587673187256 - trainLoss: 0.47185418009757996\n",
      "cnt: 0 - valLoss: 0.48390159010887146 - trainLoss: 0.4717097282409668\n",
      "cnt: 0 - valLoss: 0.4837479591369629 - trainLoss: 0.47156593203544617\n",
      "cnt: 0 - valLoss: 0.4835950434207916 - trainLoss: 0.47142294049263\n",
      "cnt: 0 - valLoss: 0.48344284296035767 - trainLoss: 0.47128060460090637\n",
      "cnt: 0 - valLoss: 0.4832914471626282 - trainLoss: 0.47113901376724243\n",
      "cnt: 0 - valLoss: 0.48314085602760315 - trainLoss: 0.47099822759628296\n",
      "cnt: 0 - valLoss: 0.48299092054367065 - trainLoss: 0.47085824608802795\n",
      "cnt: 0 - valLoss: 0.48284170031547546 - trainLoss: 0.47071897983551025\n",
      "cnt: 0 - valLoss: 0.4826931059360504 - trainLoss: 0.47058045864105225\n",
      "cnt: 0 - valLoss: 0.4825451970100403 - trainLoss: 0.4704427123069763\n",
      "cnt: 0 - valLoss: 0.48239797353744507 - trainLoss: 0.4703056216239929\n",
      "cnt: 0 - valLoss: 0.4822514057159424 - trainLoss: 0.47016921639442444\n",
      "cnt: 0 - valLoss: 0.4821056127548218 - trainLoss: 0.4700334966182709\n",
      "cnt: 0 - valLoss: 0.48196062445640564 - trainLoss: 0.46989867091178894\n",
      "cnt: 0 - valLoss: 0.4818163812160492 - trainLoss: 0.46976438164711\n",
      "cnt: 0 - valLoss: 0.4816727042198181 - trainLoss: 0.46963071823120117\n",
      "cnt: 0 - valLoss: 0.48152956366539 - trainLoss: 0.46949782967567444\n",
      "cnt: 0 - valLoss: 0.48138725757598877 - trainLoss: 0.46936553716659546\n",
      "cnt: 0 - valLoss: 0.4812455177307129 - trainLoss: 0.4692339301109314\n",
      "cnt: 0 - valLoss: 0.48110446333885193 - trainLoss: 0.46910300850868225\n",
      "cnt: 0 - valLoss: 0.4809640049934387 - trainLoss: 0.46897271275520325\n",
      "cnt: 0 - valLoss: 0.4808241128921509 - trainLoss: 0.46884313225746155\n",
      "cnt: 0 - valLoss: 0.48068487644195557 - trainLoss: 0.4687141478061676\n",
      "cnt: 0 - valLoss: 0.48054635524749756 - trainLoss: 0.46858587861061096\n",
      "cnt: 0 - valLoss: 0.4804084599018097 - trainLoss: 0.46845826506614685\n",
      "cnt: 0 - valLoss: 0.48027125000953674 - trainLoss: 0.46833130717277527\n",
      "cnt: 0 - valLoss: 0.48013466596603394 - trainLoss: 0.46820497512817383\n",
      "cnt: 0 - valLoss: 0.47999852895736694 - trainLoss: 0.46807923913002014\n",
      "cnt: 0 - valLoss: 0.4798631966114044 - trainLoss: 0.46795418858528137\n",
      "cnt: 0 - valLoss: 0.4797285497188568 - trainLoss: 0.46782973408699036\n",
      "cnt: 0 - valLoss: 0.47959452867507935 - trainLoss: 0.4677059054374695\n",
      "cnt: 0 - valLoss: 0.47946110367774963 - trainLoss: 0.46758270263671875\n",
      "cnt: 0 - valLoss: 0.4793281555175781 - trainLoss: 0.46746015548706055\n",
      "cnt: 0 - valLoss: 0.47919580340385437 - trainLoss: 0.46733802556991577\n",
      "cnt: 0 - valLoss: 0.479064017534256 - trainLoss: 0.4672165513038635\n",
      "cnt: 0 - valLoss: 0.47893282771110535 - trainLoss: 0.4670957028865814\n",
      "cnt: 0 - valLoss: 0.47880223393440247 - trainLoss: 0.4669753909111023\n",
      "cnt: 0 - valLoss: 0.4786720871925354 - trainLoss: 0.4668557047843933\n",
      "cnt: 0 - valLoss: 0.47854265570640564 - trainLoss: 0.4667365849018097\n",
      "cnt: 0 - valLoss: 0.47841405868530273 - trainLoss: 0.4666179120540619\n",
      "cnt: 0 - valLoss: 0.47828611731529236 - trainLoss: 0.4664997160434723\n",
      "cnt: 0 - valLoss: 0.47815874218940735 - trainLoss: 0.46638214588165283\n",
      "cnt: 0 - valLoss: 0.4780319035053253 - trainLoss: 0.46626517176628113\n",
      "cnt: 0 - valLoss: 0.47790566086769104 - trainLoss: 0.46614870429039\n",
      "cnt: 0 - valLoss: 0.47777998447418213 - trainLoss: 0.46603286266326904\n",
      "cnt: 0 - valLoss: 0.4776548743247986 - trainLoss: 0.4659176766872406\n",
      "cnt: 0 - valLoss: 0.4775303900241852 - trainLoss: 0.46580299735069275\n",
      "cnt: 0 - valLoss: 0.47740638256073 - trainLoss: 0.4656888544559479\n",
      "cnt: 0 - valLoss: 0.477282851934433 - trainLoss: 0.46557527780532837\n",
      "cnt: 0 - valLoss: 0.47715994715690613 - trainLoss: 0.46546226739883423\n",
      "cnt: 0 - valLoss: 0.477037638425827 - trainLoss: 0.46534982323646545\n",
      "cnt: 0 - valLoss: 0.4769156873226166 - trainLoss: 0.4652378261089325\n",
      "cnt: 0 - valLoss: 0.47679421305656433 - trainLoss: 0.46512624621391296\n",
      "cnt: 0 - valLoss: 0.47667357325553894 - trainLoss: 0.4650152027606964\n",
      "cnt: 0 - valLoss: 0.4765535891056061 - trainLoss: 0.46490478515625\n",
      "cnt: 0 - valLoss: 0.4764341413974762 - trainLoss: 0.4647950530052185\n",
      "cnt: 0 - valLoss: 0.4763152599334717 - trainLoss: 0.46468567848205566\n",
      "cnt: 0 - valLoss: 0.47619664669036865 - trainLoss: 0.46457695960998535\n",
      "cnt: 0 - valLoss: 0.4760785698890686 - trainLoss: 0.4644685387611389\n",
      "cnt: 0 - valLoss: 0.4759608209133148 - trainLoss: 0.46436071395874023\n",
      "cnt: 0 - valLoss: 0.475843608379364 - trainLoss: 0.46425339579582214\n",
      "cnt: 0 - valLoss: 0.47572699189186096 - trainLoss: 0.4641466736793518\n",
      "cnt: 0 - valLoss: 0.4756108522415161 - trainLoss: 0.46404045820236206\n",
      "cnt: 0 - valLoss: 0.47549524903297424 - trainLoss: 0.4639347791671753\n",
      "cnt: 0 - valLoss: 0.47538021206855774 - trainLoss: 0.46382957696914673\n",
      "cnt: 0 - valLoss: 0.47526565194129944 - trainLoss: 0.46372488141059875\n",
      "cnt: 0 - valLoss: 0.47515159845352173 - trainLoss: 0.46362075209617615\n",
      "cnt: 0 - valLoss: 0.4750381112098694 - trainLoss: 0.46351712942123413\n",
      "cnt: 0 - valLoss: 0.47492513060569763 - trainLoss: 0.4634140729904175\n",
      "cnt: 0 - valLoss: 0.4748125970363617 - trainLoss: 0.46331140398979187\n",
      "cnt: 0 - valLoss: 0.4747006893157959 - trainLoss: 0.4632093608379364\n",
      "cnt: 0 - valLoss: 0.47458937764167786 - trainLoss: 0.46310773491859436\n",
      "cnt: 0 - valLoss: 0.4744783937931061 - trainLoss: 0.4630065858364105\n",
      "cnt: 0 - valLoss: 0.47436773777008057 - trainLoss: 0.46290597319602966\n",
      "cnt: 0 - valLoss: 0.47425761818885803 - trainLoss: 0.46280571818351746\n",
      "cnt: 0 - valLoss: 0.4741480350494385 - trainLoss: 0.4627060890197754\n",
      "cnt: 0 - valLoss: 0.4740390181541443 - trainLoss: 0.4626069664955139\n",
      "cnt: 0 - valLoss: 0.4739304482936859 - trainLoss: 0.46250826120376587\n",
      "cnt: 0 - valLoss: 0.47382235527038574 - trainLoss: 0.4624100625514984\n",
      "cnt: 0 - valLoss: 0.4737147092819214 - trainLoss: 0.462312251329422\n",
      "cnt: 0 - valLoss: 0.4736076295375824 - trainLoss: 0.46221494674682617\n",
      "cnt: 0 - valLoss: 0.47350093722343445 - trainLoss: 0.462117999792099\n",
      "cnt: 0 - valLoss: 0.4733947217464447 - trainLoss: 0.4620215594768524\n",
      "cnt: 0 - valLoss: 0.47328895330429077 - trainLoss: 0.4619255065917969\n",
      "cnt: 0 - valLoss: 0.4731837809085846 - trainLoss: 0.46182993054389954\n",
      "cnt: 0 - valLoss: 0.4730791449546814 - trainLoss: 0.4617348611354828\n",
      "cnt: 0 - valLoss: 0.47297486662864685 - trainLoss: 0.46164029836654663\n",
      "cnt: 0 - valLoss: 0.4728710651397705 - trainLoss: 0.4615460932254791\n",
      "cnt: 0 - valLoss: 0.47276774048805237 - trainLoss: 0.4614523947238922\n",
      "cnt: 0 - valLoss: 0.4726649224758148 - trainLoss: 0.46135908365249634\n",
      "cnt: 0 - valLoss: 0.4725624918937683 - trainLoss: 0.46126624941825867\n",
      "cnt: 0 - valLoss: 0.4724605083465576 - trainLoss: 0.4611739218235016\n",
      "cnt: 0 - valLoss: 0.4723587930202484 - trainLoss: 0.4610818326473236\n",
      "cnt: 0 - valLoss: 0.47225746512413025 - trainLoss: 0.46099013090133667\n",
      "cnt: 0 - valLoss: 0.4721566140651703 - trainLoss: 0.4608989655971527\n",
      "cnt: 0 - valLoss: 0.47205621004104614 - trainLoss: 0.4608081877231598\n",
      "cnt: 0 - valLoss: 0.4719562828540802 - trainLoss: 0.4607178270816803\n",
      "cnt: 0 - valLoss: 0.4718567430973053 - trainLoss: 0.4606279134750366\n",
      "cnt: 0 - valLoss: 0.471757709980011 - trainLoss: 0.4605385363101959\n",
      "cnt: 0 - valLoss: 0.4716591238975525 - trainLoss: 0.4604495167732239\n",
      "cnt: 0 - valLoss: 0.47156089544296265 - trainLoss: 0.4603610336780548\n",
      "cnt: 0 - valLoss: 0.4714632034301758 - trainLoss: 0.4602729082107544\n",
      "cnt: 0 - valLoss: 0.47136586904525757 - trainLoss: 0.4601852297782898\n",
      "cnt: 0 - valLoss: 0.47126901149749756 - trainLoss: 0.4600979685783386\n",
      "cnt: 0 - valLoss: 0.47117260098457336 - trainLoss: 0.4600111246109009\n",
      "cnt: 0 - valLoss: 0.4710765480995178 - trainLoss: 0.45992475748062134\n",
      "cnt: 0 - valLoss: 0.4709809124469757 - trainLoss: 0.45983877778053284\n",
      "cnt: 0 - valLoss: 0.47088566422462463 - trainLoss: 0.4597531855106354\n",
      "cnt: 0 - valLoss: 0.47079095244407654 - trainLoss: 0.45966818928718567\n",
      "cnt: 0 - valLoss: 0.4706965982913971 - trainLoss: 0.4595836102962494\n",
      "cnt: 0 - valLoss: 0.4706026613712311 - trainLoss: 0.4594995081424713\n",
      "cnt: 0 - valLoss: 0.4705091118812561 - trainLoss: 0.45941582322120667\n",
      "cnt: 0 - valLoss: 0.4704161286354065 - trainLoss: 0.45933258533477783\n",
      "cnt: 0 - valLoss: 0.4703235626220703 - trainLoss: 0.45924970507621765\n",
      "cnt: 0 - valLoss: 0.4702313542366028 - trainLoss: 0.4591672420501709\n",
      "cnt: 0 - valLoss: 0.4701394736766815 - trainLoss: 0.4590851366519928\n",
      "cnt: 0 - valLoss: 0.4700479805469513 - trainLoss: 0.4590034484863281\n",
      "cnt: 0 - valLoss: 0.4699568748474121 - trainLoss: 0.4589221179485321\n",
      "cnt: 0 - valLoss: 0.46986594796180725 - trainLoss: 0.45884111523628235\n",
      "cnt: 0 - valLoss: 0.4697754681110382 - trainLoss: 0.45876044034957886\n",
      "cnt: 0 - valLoss: 0.46968525648117065 - trainLoss: 0.45868009328842163\n",
      "cnt: 0 - valLoss: 0.4695955812931061 - trainLoss: 0.45860031247138977\n",
      "cnt: 0 - valLoss: 0.4695062041282654 - trainLoss: 0.4585210382938385\n",
      "cnt: 0 - valLoss: 0.46941709518432617 - trainLoss: 0.4584420919418335\n",
      "cnt: 0 - valLoss: 0.4693284034729004 - trainLoss: 0.4583636522293091\n",
      "cnt: 0 - valLoss: 0.46924012899398804 - trainLoss: 0.45828551054000854\n",
      "cnt: 0 - valLoss: 0.4691525399684906 - trainLoss: 0.45820775628089905\n",
      "cnt: 0 - valLoss: 0.4690651595592499 - trainLoss: 0.45813027024269104\n",
      "cnt: 0 - valLoss: 0.4689783453941345 - trainLoss: 0.45805320143699646\n",
      "cnt: 0 - valLoss: 0.4688917398452759 - trainLoss: 0.45797646045684814\n",
      "cnt: 0 - valLoss: 0.4688057005405426 - trainLoss: 0.4579000771045685\n",
      "cnt: 0 - valLoss: 0.4687199294567108 - trainLoss: 0.4578240215778351\n",
      "cnt: 0 - valLoss: 0.46863460540771484 - trainLoss: 0.4577483534812927\n",
      "cnt: 0 - valLoss: 0.4685496389865875 - trainLoss: 0.457673043012619\n",
      "cnt: 0 - valLoss: 0.46846505999565125 - trainLoss: 0.4575980603694916\n",
      "cnt: 0 - valLoss: 0.46838074922561646 - trainLoss: 0.45752355456352234\n",
      "cnt: 0 - valLoss: 0.4682968258857727 - trainLoss: 0.4574492871761322\n",
      "cnt: 0 - valLoss: 0.4682132601737976 - trainLoss: 0.45737534761428833\n",
      "cnt: 0 - valLoss: 0.4681299924850464 - trainLoss: 0.4573017954826355\n",
      "cnt: 0 - valLoss: 0.4680470824241638 - trainLoss: 0.45722854137420654\n",
      "cnt: 0 - valLoss: 0.4679645597934723 - trainLoss: 0.4571555554866791\n",
      "cnt: 0 - valLoss: 0.4678823947906494 - trainLoss: 0.45708295702934265\n",
      "cnt: 0 - valLoss: 0.4678005874156952 - trainLoss: 0.4570106565952301\n",
      "cnt: 0 - valLoss: 0.4677193760871887 - trainLoss: 0.45693856477737427\n",
      "cnt: 0 - valLoss: 0.4676382839679718 - trainLoss: 0.45686662197113037\n",
      "cnt: 0 - valLoss: 0.46755751967430115 - trainLoss: 0.4567950367927551\n",
      "cnt: 0 - valLoss: 0.46747711300849915 - trainLoss: 0.45672357082366943\n",
      "cnt: 0 - valLoss: 0.467397004365921 - trainLoss: 0.4566524624824524\n",
      "cnt: 0 - valLoss: 0.46731704473495483 - trainLoss: 0.4565817415714264\n",
      "cnt: 0 - valLoss: 0.4672373831272125 - trainLoss: 0.45651131868362427\n",
      "cnt: 0 - valLoss: 0.4671582281589508 - trainLoss: 0.4564412534236908\n",
      "cnt: 0 - valLoss: 0.46707940101623535 - trainLoss: 0.4563714563846588\n",
      "cnt: 0 - valLoss: 0.46700096130371094 - trainLoss: 0.4563019573688507\n",
      "cnt: 0 - valLoss: 0.4669227600097656 - trainLoss: 0.4562326967716217\n",
      "cnt: 0 - valLoss: 0.4668448567390442 - trainLoss: 0.4561637043952942\n",
      "cnt: 0 - valLoss: 0.466767281293869 - trainLoss: 0.4560950696468353\n",
      "cnt: 0 - valLoss: 0.4666900038719177 - trainLoss: 0.45602670311927795\n",
      "cnt: 0 - valLoss: 0.4666129946708679 - trainLoss: 0.45595869421958923\n",
      "cnt: 0 - valLoss: 0.46653616428375244 - trainLoss: 0.4558908939361572\n",
      "cnt: 0 - valLoss: 0.46645984053611755 - trainLoss: 0.45582345128059387\n",
      "cnt: 0 - valLoss: 0.46638375520706177 - trainLoss: 0.45575621724128723\n",
      "cnt: 0 - valLoss: 0.46630796790122986 - trainLoss: 0.45568931102752686\n",
      "cnt: 0 - valLoss: 0.4662324786186218 - trainLoss: 0.4556226134300232\n",
      "cnt: 0 - valLoss: 0.4661572277545929 - trainLoss: 0.4555562436580658\n",
      "cnt: 0 - valLoss: 0.46608221530914307 - trainLoss: 0.4554901123046875\n",
      "cnt: 0 - valLoss: 0.46600744128227234 - trainLoss: 0.4554241895675659\n",
      "cnt: 0 - valLoss: 0.4659329950809479 - trainLoss: 0.45535850524902344\n",
      "cnt: 0 - valLoss: 0.46585872769355774 - trainLoss: 0.45529302954673767\n",
      "cnt: 0 - valLoss: 0.4657846689224243 - trainLoss: 0.455227792263031\n",
      "cnt: 0 - valLoss: 0.46571075916290283 - trainLoss: 0.45516282320022583\n",
      "cnt: 0 - valLoss: 0.46563720703125 - trainLoss: 0.4550981819629669\n",
      "cnt: 0 - valLoss: 0.46556389331817627 - trainLoss: 0.45503365993499756\n",
      "cnt: 0 - valLoss: 0.46549078822135925 - trainLoss: 0.45496946573257446\n",
      "cnt: 0 - valLoss: 0.4654179513454437 - trainLoss: 0.4549054801464081\n",
      "cnt: 0 - valLoss: 0.4653454124927521 - trainLoss: 0.45484182238578796\n",
      "cnt: 0 - valLoss: 0.4652731120586395 - trainLoss: 0.4547783434391022\n",
      "cnt: 0 - valLoss: 0.46520116925239563 - trainLoss: 0.45471513271331787\n",
      "cnt: 0 - valLoss: 0.4651300013065338 - trainLoss: 0.4546520411968231\n",
      "cnt: 0 - valLoss: 0.4650590121746063 - trainLoss: 0.45458894968032837\n",
      "cnt: 0 - valLoss: 0.4649882912635803 - trainLoss: 0.4545261561870575\n",
      "cnt: 0 - valLoss: 0.46491768956184387 - trainLoss: 0.45446348190307617\n",
      "cnt: 0 - valLoss: 0.46484729647636414 - trainLoss: 0.45440107583999634\n",
      "cnt: 0 - valLoss: 0.4647771418094635 - trainLoss: 0.4543389678001404\n",
      "cnt: 0 - valLoss: 0.46470725536346436 - trainLoss: 0.4542770981788635\n",
      "cnt: 0 - valLoss: 0.46463748812675476 - trainLoss: 0.45421546697616577\n",
      "cnt: 0 - valLoss: 0.4645678400993347 - trainLoss: 0.45415398478507996\n",
      "cnt: 0 - valLoss: 0.46449849009513855 - trainLoss: 0.4540928602218628\n",
      "cnt: 0 - valLoss: 0.46442940831184387 - trainLoss: 0.45403191447257996\n",
      "cnt: 0 - valLoss: 0.4643605649471283 - trainLoss: 0.4539712369441986\n",
      "cnt: 0 - valLoss: 0.4642919898033142 - trainLoss: 0.45391082763671875\n",
      "cnt: 0 - valLoss: 0.46422359347343445 - trainLoss: 0.4538506865501404\n",
      "cnt: 0 - valLoss: 0.464155375957489 - trainLoss: 0.45379072427749634\n",
      "cnt: 0 - valLoss: 0.46408745646476746 - trainLoss: 0.4537310004234314\n",
      "cnt: 0 - valLoss: 0.4640197455883026 - trainLoss: 0.45367148518562317\n",
      "cnt: 0 - valLoss: 0.46395227313041687 - trainLoss: 0.4536122679710388\n",
      "cnt: 0 - valLoss: 0.463885098695755 - trainLoss: 0.4535532295703888\n",
      "cnt: 0 - valLoss: 0.46381816267967224 - trainLoss: 0.45349442958831787\n",
      "cnt: 0 - valLoss: 0.4637514352798462 - trainLoss: 0.4534359574317932\n",
      "cnt: 0 - valLoss: 0.46368497610092163 - trainLoss: 0.4533776640892029\n",
      "cnt: 0 - valLoss: 0.4636186957359314 - trainLoss: 0.4533194899559021\n",
      "cnt: 0 - valLoss: 0.46355268359184265 - trainLoss: 0.45326170325279236\n",
      "cnt: 0 - valLoss: 0.46348685026168823 - trainLoss: 0.45320403575897217\n",
      "cnt: 0 - valLoss: 0.4634212553501129 - trainLoss: 0.4531466066837311\n",
      "cnt: 0 - valLoss: 0.4633559584617615 - trainLoss: 0.4530893862247467\n",
      "cnt: 0 - valLoss: 0.46329084038734436 - trainLoss: 0.45303237438201904\n",
      "cnt: 0 - valLoss: 0.46322593092918396 - trainLoss: 0.4529755413532257\n",
      "cnt: 0 - valLoss: 0.4631611406803131 - trainLoss: 0.4529187083244324\n",
      "cnt: 0 - valLoss: 0.4630964398384094 - trainLoss: 0.4528622329235077\n",
      "cnt: 0 - valLoss: 0.46303194761276245 - trainLoss: 0.4528060257434845\n",
      "cnt: 0 - valLoss: 0.46296772360801697 - trainLoss: 0.45274999737739563\n",
      "cnt: 0 - valLoss: 0.46290352940559387 - trainLoss: 0.4526941478252411\n",
      "cnt: 0 - valLoss: 0.4628395736217499 - trainLoss: 0.4526384770870209\n",
      "cnt: 0 - valLoss: 0.4627757668495178 - trainLoss: 0.4525830149650574\n",
      "cnt: 0 - valLoss: 0.46271222829818726 - trainLoss: 0.4525277018547058\n",
      "cnt: 0 - valLoss: 0.4626488983631134 - trainLoss: 0.45247262716293335\n",
      "cnt: 0 - valLoss: 0.46258580684661865 - trainLoss: 0.4524177014827728\n",
      "cnt: 0 - valLoss: 0.4625229835510254 - trainLoss: 0.4523630440235138\n",
      "cnt: 0 - valLoss: 0.4624602496623993 - trainLoss: 0.4523085057735443\n",
      "cnt: 0 - valLoss: 0.46239787340164185 - trainLoss: 0.45225417613983154\n",
      "cnt: 0 - valLoss: 0.4623357057571411 - trainLoss: 0.4521999955177307\n",
      "cnt: 0 - valLoss: 0.46227365732192993 - trainLoss: 0.452146053314209\n",
      "cnt: 0 - valLoss: 0.46221181750297546 - trainLoss: 0.4520922899246216\n",
      "cnt: 0 - valLoss: 0.4621502459049225 - trainLoss: 0.4520387053489685\n",
      "cnt: 0 - valLoss: 0.4620888829231262 - trainLoss: 0.4519854187965393\n",
      "cnt: 0 - valLoss: 0.4620276689529419 - trainLoss: 0.45193231105804443\n",
      "cnt: 0 - valLoss: 0.46196669340133667 - trainLoss: 0.45187944173812866\n",
      "cnt: 0 - valLoss: 0.46190589666366577 - trainLoss: 0.45182672142982483\n",
      "cnt: 0 - valLoss: 0.4618452787399292 - trainLoss: 0.4517742991447449\n",
      "cnt: 0 - valLoss: 0.46178489923477173 - trainLoss: 0.4517219662666321\n",
      "cnt: 0 - valLoss: 0.4617246687412262 - trainLoss: 0.4516700208187103\n",
      "cnt: 0 - valLoss: 0.46166476607322693 - trainLoss: 0.4516181945800781\n",
      "cnt: 0 - valLoss: 0.46160516142845154 - trainLoss: 0.451566606760025\n",
      "cnt: 0 - valLoss: 0.4615454375743866 - trainLoss: 0.45151519775390625\n",
      "cnt: 0 - valLoss: 0.46148595213890076 - trainLoss: 0.4514639377593994\n",
      "cnt: 0 - valLoss: 0.46142664551734924 - trainLoss: 0.4514128863811493\n",
      "cnt: 0 - valLoss: 0.461367666721344 - trainLoss: 0.4513620138168335\n",
      "cnt: 0 - valLoss: 0.4613087475299835 - trainLoss: 0.4513113498687744\n",
      "cnt: 0 - valLoss: 0.46125006675720215 - trainLoss: 0.45126089453697205\n",
      "cnt: 0 - valLoss: 0.4611915349960327 - trainLoss: 0.45121055841445923\n",
      "cnt: 0 - valLoss: 0.4611331820487976 - trainLoss: 0.4511604905128479\n",
      "cnt: 0 - valLoss: 0.4610750377178192 - trainLoss: 0.4511105418205261\n",
      "cnt: 0 - valLoss: 0.46101704239845276 - trainLoss: 0.45106077194213867\n",
      "cnt: 0 - valLoss: 0.46095943450927734 - trainLoss: 0.4510112702846527\n",
      "cnt: 0 - valLoss: 0.46090197563171387 - trainLoss: 0.4509619176387787\n",
      "cnt: 0 - valLoss: 0.4608446955680847 - trainLoss: 0.4509127736091614\n",
      "cnt: 0 - valLoss: 0.46078774333000183 - trainLoss: 0.450863778591156\n",
      "cnt: 0 - valLoss: 0.46073079109191895 - trainLoss: 0.45081496238708496\n",
      "cnt: 0 - valLoss: 0.4606740474700928 - trainLoss: 0.4507662355899811\n",
      "cnt: 0 - valLoss: 0.46061763167381287 - trainLoss: 0.4507177174091339\n",
      "cnt: 0 - valLoss: 0.4605613350868225 - trainLoss: 0.4506693482398987\n",
      "cnt: 0 - valLoss: 0.4605052173137665 - trainLoss: 0.45062118768692017\n",
      "cnt: 0 - valLoss: 0.46044930815696716 - trainLoss: 0.450573205947876\n",
      "cnt: 0 - valLoss: 0.4603937268257141 - trainLoss: 0.4505253732204437\n",
      "cnt: 0 - valLoss: 0.46033838391304016 - trainLoss: 0.45047780871391296\n",
      "cnt: 0 - valLoss: 0.46028321981430054 - trainLoss: 0.45043039321899414\n",
      "cnt: 0 - valLoss: 0.4602281153202057 - trainLoss: 0.4503832459449768\n",
      "cnt: 0 - valLoss: 0.46017327904701233 - trainLoss: 0.45033636689186096\n",
      "cnt: 0 - valLoss: 0.4601185917854309 - trainLoss: 0.45028960704803467\n",
      "cnt: 0 - valLoss: 0.46006420254707336 - trainLoss: 0.4502430856227875\n",
      "cnt: 0 - valLoss: 0.460009902715683 - trainLoss: 0.450196772813797\n",
      "cnt: 0 - valLoss: 0.4599553644657135 - trainLoss: 0.4501506984233856\n",
      "cnt: 0 - valLoss: 0.45990100502967834 - trainLoss: 0.45010513067245483\n",
      "cnt: 0 - valLoss: 0.4598468244075775 - trainLoss: 0.4500596821308136\n",
      "cnt: 0 - valLoss: 0.4597927927970886 - trainLoss: 0.4500144124031067\n",
      "cnt: 0 - valLoss: 0.45973899960517883 - trainLoss: 0.44996926188468933\n",
      "cnt: 0 - valLoss: 0.459685355424881 - trainLoss: 0.44992437958717346\n",
      "cnt: 0 - valLoss: 0.4596319794654846 - trainLoss: 0.44987958669662476\n",
      "cnt: 0 - valLoss: 0.4595786929130554 - trainLoss: 0.4498348832130432\n",
      "cnt: 0 - valLoss: 0.4595257043838501 - trainLoss: 0.449790358543396\n",
      "cnt: 0 - valLoss: 0.4594731032848358 - trainLoss: 0.44974595308303833\n",
      "cnt: 0 - valLoss: 0.45942071080207825 - trainLoss: 0.4497015178203583\n",
      "cnt: 0 - valLoss: 0.45936840772628784 - trainLoss: 0.449657142162323\n",
      "cnt: 0 - valLoss: 0.45931634306907654 - trainLoss: 0.4496130347251892\n",
      "cnt: 0 - valLoss: 0.4592644274234772 - trainLoss: 0.44956907629966736\n",
      "cnt: 0 - valLoss: 0.45921269059181213 - trainLoss: 0.4495250880718231\n",
      "cnt: 0 - valLoss: 0.45916107296943665 - trainLoss: 0.449481338262558\n",
      "cnt: 0 - valLoss: 0.459109365940094 - trainLoss: 0.4494377672672272\n",
      "cnt: 0 - valLoss: 0.4590578079223633 - trainLoss: 0.4493941366672516\n",
      "cnt: 0 - valLoss: 0.4590064585208893 - trainLoss: 0.44935059547424316\n",
      "cnt: 0 - valLoss: 0.4589551091194153 - trainLoss: 0.44930726289749146\n",
      "cnt: 0 - valLoss: 0.458903968334198 - trainLoss: 0.44926413893699646\n",
      "cnt: 0 - valLoss: 0.4588530361652374 - trainLoss: 0.44922110438346863\n",
      "cnt: 0 - valLoss: 0.4588022530078888 - trainLoss: 0.4491782486438751\n",
      "cnt: 0 - valLoss: 0.4587515890598297 - trainLoss: 0.44913551211357117\n",
      "cnt: 0 - valLoss: 0.45870116353034973 - trainLoss: 0.44909292459487915\n",
      "cnt: 0 - valLoss: 0.4586509168148041 - trainLoss: 0.44905051589012146\n",
      "cnt: 0 - valLoss: 0.45860081911087036 - trainLoss: 0.44900816679000854\n",
      "cnt: 0 - valLoss: 0.4585508406162262 - trainLoss: 0.44896599650382996\n",
      "cnt: 0 - valLoss: 0.45850101113319397 - trainLoss: 0.4489240050315857\n",
      "cnt: 0 - valLoss: 0.45845136046409607 - trainLoss: 0.448882132768631\n",
      "cnt: 0 - valLoss: 0.45840179920196533 - trainLoss: 0.44884034991264343\n",
      "cnt: 0 - valLoss: 0.4583524465560913 - trainLoss: 0.4487987458705902\n",
      "cnt: 0 - valLoss: 0.4583032727241516 - trainLoss: 0.4487571716308594\n",
      "cnt: 0 - valLoss: 0.4582541882991791 - trainLoss: 0.44871580600738525\n",
      "cnt: 0 - valLoss: 0.4582052230834961 - trainLoss: 0.4486745297908783\n",
      "cnt: 0 - valLoss: 0.4581563472747803 - trainLoss: 0.4486333429813385\n",
      "cnt: 0 - valLoss: 0.4581076204776764 - trainLoss: 0.4485922157764435\n",
      "cnt: 0 - valLoss: 0.4580589532852173 - trainLoss: 0.4485511779785156\n",
      "cnt: 0 - valLoss: 0.4580104351043701 - trainLoss: 0.4485102891921997\n",
      "cnt: 0 - valLoss: 0.45796215534210205 - trainLoss: 0.4484696090221405\n",
      "cnt: 0 - valLoss: 0.45791393518447876 - trainLoss: 0.4484289586544037\n",
      "cnt: 0 - valLoss: 0.45786625146865845 - trainLoss: 0.4483884274959564\n",
      "cnt: 0 - valLoss: 0.45781874656677246 - trainLoss: 0.4483480155467987\n",
      "cnt: 0 - valLoss: 0.457771360874176 - trainLoss: 0.4483076333999634\n",
      "cnt: 0 - valLoss: 0.45772409439086914 - trainLoss: 0.4482674300670624\n",
      "cnt: 0 - valLoss: 0.4576769471168518 - trainLoss: 0.44822725653648376\n",
      "cnt: 0 - valLoss: 0.4576300084590912 - trainLoss: 0.4481872022151947\n",
      "cnt: 0 - valLoss: 0.45758292078971863 - trainLoss: 0.4481472969055176\n",
      "cnt: 0 - valLoss: 0.4575360417366028 - trainLoss: 0.44810739159584045\n",
      "cnt: 0 - valLoss: 0.4574893116950989 - trainLoss: 0.44806763529777527\n",
      "cnt: 0 - valLoss: 0.4574425518512726 - trainLoss: 0.44802796840667725\n",
      "cnt: 0 - valLoss: 0.45739585161209106 - trainLoss: 0.44798845052719116\n",
      "cnt: 0 - valLoss: 0.45734933018684387 - trainLoss: 0.4479491412639618\n",
      "cnt: 0 - valLoss: 0.45730292797088623 - trainLoss: 0.4479099214076996\n",
      "cnt: 0 - valLoss: 0.45725664496421814 - trainLoss: 0.4478707015514374\n",
      "cnt: 0 - valLoss: 0.4572104811668396 - trainLoss: 0.4478316605091095\n",
      "cnt: 0 - valLoss: 0.4571644365787506 - trainLoss: 0.447792649269104\n",
      "cnt: 0 - valLoss: 0.45711854100227356 - trainLoss: 0.44775381684303284\n",
      "cnt: 0 - valLoss: 0.45707276463508606 - trainLoss: 0.44771507382392883\n",
      "cnt: 0 - valLoss: 0.45702725648880005 - trainLoss: 0.44767647981643677\n",
      "cnt: 0 - valLoss: 0.4569817781448364 - trainLoss: 0.44763800501823425\n",
      "cnt: 0 - valLoss: 0.4569365382194519 - trainLoss: 0.4475996494293213\n",
      "cnt: 0 - valLoss: 0.45689165592193604 - trainLoss: 0.4475613534450531\n",
      "cnt: 0 - valLoss: 0.4568469524383545 - trainLoss: 0.44752317667007446\n",
      "cnt: 0 - valLoss: 0.4568023681640625 - trainLoss: 0.4474850296974182\n",
      "cnt: 0 - valLoss: 0.45675787329673767 - trainLoss: 0.4474470615386963\n",
      "cnt: 0 - valLoss: 0.4567134976387024 - trainLoss: 0.44740912318229675\n",
      "cnt: 0 - valLoss: 0.45666924118995667 - trainLoss: 0.44737136363983154\n",
      "cnt: 0 - valLoss: 0.4566251039505005 - trainLoss: 0.4473336637020111\n",
      "cnt: 0 - valLoss: 0.456581175327301 - trainLoss: 0.4472960829734802\n",
      "cnt: 0 - valLoss: 0.4565373659133911 - trainLoss: 0.4472585916519165\n",
      "cnt: 0 - valLoss: 0.4564935863018036 - trainLoss: 0.44722113013267517\n",
      "cnt: 0 - valLoss: 0.4564499258995056 - trainLoss: 0.4471838176250458\n",
      "cnt: 0 - valLoss: 0.45640644431114197 - trainLoss: 0.44714659452438354\n",
      "cnt: 0 - valLoss: 0.4563630521297455 - trainLoss: 0.4471094310283661\n",
      "cnt: 0 - valLoss: 0.45631974935531616 - trainLoss: 0.4470723867416382\n",
      "cnt: 0 - valLoss: 0.4562765061855316 - trainLoss: 0.44703546166419983\n",
      "cnt: 0 - valLoss: 0.45623350143432617 - trainLoss: 0.4469986855983734\n",
      "cnt: 0 - valLoss: 0.4561905264854431 - trainLoss: 0.44696202874183655\n",
      "cnt: 0 - valLoss: 0.45614761114120483 - trainLoss: 0.44692543148994446\n",
      "cnt: 0 - valLoss: 0.4561048150062561 - trainLoss: 0.4468889534473419\n",
      "cnt: 0 - valLoss: 0.45606204867362976 - trainLoss: 0.4468526244163513\n",
      "cnt: 0 - valLoss: 0.4560193717479706 - trainLoss: 0.4468163847923279\n",
      "cnt: 0 - valLoss: 0.45597681403160095 - trainLoss: 0.446780264377594\n",
      "cnt: 0 - valLoss: 0.45593443512916565 - trainLoss: 0.4467441737651825\n",
      "cnt: 0 - valLoss: 0.4558922350406647 - trainLoss: 0.44670820236206055\n",
      "cnt: 0 - valLoss: 0.45585015416145325 - trainLoss: 0.4466722905635834\n",
      "cnt: 0 - valLoss: 0.45580822229385376 - trainLoss: 0.446636438369751\n",
      "cnt: 0 - valLoss: 0.4557664394378662 - trainLoss: 0.44660070538520813\n",
      "cnt: 0 - valLoss: 0.4557248055934906 - trainLoss: 0.44656500220298767\n",
      "cnt: 0 - valLoss: 0.4556833803653717 - trainLoss: 0.44652947783470154\n",
      "cnt: 0 - valLoss: 0.4556419849395752 - trainLoss: 0.4464939832687378\n",
      "cnt: 0 - valLoss: 0.4556007385253906 - trainLoss: 0.4464586079120636\n",
      "cnt: 0 - valLoss: 0.4555596709251404 - trainLoss: 0.4464232623577118\n",
      "cnt: 0 - valLoss: 0.45551854372024536 - trainLoss: 0.44638803601264954\n",
      "cnt: 0 - valLoss: 0.45547765493392944 - trainLoss: 0.44635292887687683\n",
      "cnt: 0 - valLoss: 0.4554368257522583 - trainLoss: 0.4463179111480713\n",
      "cnt: 0 - valLoss: 0.45539599657058716 - trainLoss: 0.4462830126285553\n",
      "cnt: 0 - valLoss: 0.45535534620285034 - trainLoss: 0.4462481737136841\n",
      "cnt: 0 - valLoss: 0.4553147852420807 - trainLoss: 0.4462134540081024\n",
      "cnt: 0 - valLoss: 0.4552741050720215 - trainLoss: 0.44617873430252075\n",
      "cnt: 0 - valLoss: 0.4552333950996399 - trainLoss: 0.44614410400390625\n",
      "cnt: 0 - valLoss: 0.45519283413887024 - trainLoss: 0.4461095631122589\n",
      "cnt: 0 - valLoss: 0.45515215396881104 - trainLoss: 0.4460751414299011\n",
      "cnt: 0 - valLoss: 0.4551117420196533 - trainLoss: 0.44604071974754333\n",
      "cnt: 0 - valLoss: 0.45507141947746277 - trainLoss: 0.4460064768791199\n",
      "cnt: 0 - valLoss: 0.45503121614456177 - trainLoss: 0.445972204208374\n",
      "cnt: 0 - valLoss: 0.45499107241630554 - trainLoss: 0.4459381103515625\n",
      "cnt: 0 - valLoss: 0.4549510180950165 - trainLoss: 0.44590407609939575\n",
      "cnt: 0 - valLoss: 0.4549112021923065 - trainLoss: 0.4458700716495514\n",
      "cnt: 0 - valLoss: 0.45487141609191895 - trainLoss: 0.4458361864089966\n",
      "cnt: 0 - valLoss: 0.45483162999153137 - trainLoss: 0.44580239057540894\n",
      "cnt: 0 - valLoss: 0.4547920823097229 - trainLoss: 0.44576865434646606\n",
      "cnt: 0 - valLoss: 0.45475253462791443 - trainLoss: 0.44573500752449036\n",
      "cnt: 0 - valLoss: 0.4547130763530731 - trainLoss: 0.445701539516449\n",
      "cnt: 0 - valLoss: 0.4546736180782318 - trainLoss: 0.44566816091537476\n",
      "cnt: 0 - valLoss: 0.454634428024292 - trainLoss: 0.4456348419189453\n",
      "cnt: 0 - valLoss: 0.45459526777267456 - trainLoss: 0.44560158252716064\n",
      "cnt: 0 - valLoss: 0.4545561969280243 - trainLoss: 0.445568323135376\n",
      "cnt: 0 - valLoss: 0.4545171856880188 - trainLoss: 0.4455351233482361\n",
      "cnt: 0 - valLoss: 0.4544782340526581 - trainLoss: 0.4455019533634186\n",
      "cnt: 0 - valLoss: 0.4544394016265869 - trainLoss: 0.4454689025878906\n",
      "cnt: 0 - valLoss: 0.4544006586074829 - trainLoss: 0.44543594121932983\n",
      "cnt: 0 - valLoss: 0.4543618857860565 - trainLoss: 0.44540297985076904\n",
      "cnt: 0 - valLoss: 0.4543231725692749 - trainLoss: 0.4453701972961426\n",
      "cnt: 0 - valLoss: 0.4542844891548157 - trainLoss: 0.4453374445438385\n",
      "cnt: 0 - valLoss: 0.454245924949646 - trainLoss: 0.4453047811985016\n",
      "cnt: 0 - valLoss: 0.45420733094215393 - trainLoss: 0.44527220726013184\n",
      "cnt: 0 - valLoss: 0.4541688859462738 - trainLoss: 0.445239782333374\n",
      "cnt: 0 - valLoss: 0.4541311264038086 - trainLoss: 0.4452073574066162\n",
      "cnt: 0 - valLoss: 0.45409348607063293 - trainLoss: 0.4451751708984375\n",
      "cnt: 0 - valLoss: 0.454056054353714 - trainLoss: 0.44514307379722595\n",
      "cnt: 0 - valLoss: 0.45401865243911743 - trainLoss: 0.44511109590530396\n",
      "cnt: 0 - valLoss: 0.4539814293384552 - trainLoss: 0.4450792074203491\n",
      "cnt: 0 - valLoss: 0.45394429564476013 - trainLoss: 0.44504743814468384\n",
      "cnt: 0 - valLoss: 0.45390722155570984 - trainLoss: 0.44501572847366333\n",
      "cnt: 0 - valLoss: 0.4538702666759491 - trainLoss: 0.4449840486049652\n",
      "cnt: 0 - valLoss: 0.45383331179618835 - trainLoss: 0.44495245814323425\n",
      "cnt: 0 - valLoss: 0.45379638671875 - trainLoss: 0.44492095708847046\n",
      "cnt: 0 - valLoss: 0.4537596106529236 - trainLoss: 0.44488945603370667\n",
      "cnt: 0 - valLoss: 0.45372292399406433 - trainLoss: 0.4448580741882324\n",
      "cnt: 0 - valLoss: 0.45368629693984985 - trainLoss: 0.44482672214508057\n",
      "cnt: 0 - valLoss: 0.45364972949028015 - trainLoss: 0.4447954595088959\n",
      "cnt: 0 - valLoss: 0.45361328125 - trainLoss: 0.44476431608200073\n",
      "cnt: 0 - valLoss: 0.4535769522190094 - trainLoss: 0.4447331428527832\n",
      "cnt: 0 - valLoss: 0.4535405933856964 - trainLoss: 0.44470205903053284\n",
      "cnt: 0 - valLoss: 0.4535044729709625 - trainLoss: 0.44467103481292725\n",
      "cnt: 0 - valLoss: 0.4534684419631958 - trainLoss: 0.4446401298046112\n",
      "cnt: 0 - valLoss: 0.45343253016471863 - trainLoss: 0.44460922479629517\n",
      "cnt: 0 - valLoss: 0.453396737575531 - trainLoss: 0.4445784091949463\n",
      "cnt: 0 - valLoss: 0.45336106419563293 - trainLoss: 0.44454750418663025\n",
      "cnt: 0 - valLoss: 0.45332539081573486 - trainLoss: 0.44451668858528137\n",
      "cnt: 0 - valLoss: 0.45328980684280396 - trainLoss: 0.4444860816001892\n",
      "cnt: 0 - valLoss: 0.4532541036605835 - trainLoss: 0.44445547461509705\n",
      "cnt: 0 - valLoss: 0.45321857929229736 - trainLoss: 0.44442495703697205\n",
      "cnt: 0 - valLoss: 0.45318305492401123 - trainLoss: 0.44439446926116943\n",
      "cnt: 0 - valLoss: 0.4531475305557251 - trainLoss: 0.44436410069465637\n",
      "cnt: 0 - valLoss: 0.45311209559440613 - trainLoss: 0.4443337023258209\n",
      "cnt: 0 - valLoss: 0.45307669043540955 - trainLoss: 0.444303423166275\n",
      "cnt: 0 - valLoss: 0.45304131507873535 - trainLoss: 0.4442732036113739\n",
      "cnt: 0 - valLoss: 0.4530061185359955 - trainLoss: 0.44424301385879517\n",
      "cnt: 0 - valLoss: 0.45297086238861084 - trainLoss: 0.4442128837108612\n",
      "cnt: 0 - valLoss: 0.45293572545051575 - trainLoss: 0.4441828727722168\n",
      "cnt: 0 - valLoss: 0.45290061831474304 - trainLoss: 0.44415283203125\n",
      "cnt: 0 - valLoss: 0.4528656303882599 - trainLoss: 0.44412291049957275\n",
      "cnt: 0 - valLoss: 0.4528305232524872 - trainLoss: 0.44409307837486267\n",
      "cnt: 0 - valLoss: 0.45279553532600403 - trainLoss: 0.4440631866455078\n",
      "cnt: 0 - valLoss: 0.45276057720184326 - trainLoss: 0.4440334141254425\n",
      "cnt: 0 - valLoss: 0.4527258574962616 - trainLoss: 0.4440036714076996\n",
      "cnt: 0 - valLoss: 0.45269155502319336 - trainLoss: 0.4439738392829895\n",
      "cnt: 0 - valLoss: 0.45265743136405945 - trainLoss: 0.4439437985420227\n",
      "cnt: 0 - valLoss: 0.45262330770492554 - trainLoss: 0.4439138174057007\n",
      "cnt: 0 - valLoss: 0.45258933305740356 - trainLoss: 0.44388389587402344\n",
      "cnt: 0 - valLoss: 0.45255541801452637 - trainLoss: 0.4438540041446686\n",
      "cnt: 0 - valLoss: 0.4525214433670044 - trainLoss: 0.4438241720199585\n",
      "cnt: 0 - valLoss: 0.45248761773109436 - trainLoss: 0.44379445910453796\n",
      "cnt: 0 - valLoss: 0.4524537920951843 - trainLoss: 0.44376471638679504\n",
      "cnt: 0 - valLoss: 0.45242011547088623 - trainLoss: 0.44373512268066406\n",
      "cnt: 0 - valLoss: 0.4523871839046478 - trainLoss: 0.4437052309513092\n",
      "cnt: 0 - valLoss: 0.4523543417453766 - trainLoss: 0.4436749815940857\n",
      "cnt: 0 - valLoss: 0.45232152938842773 - trainLoss: 0.44364479184150696\n",
      "cnt: 0 - valLoss: 0.45228880643844604 - trainLoss: 0.44361457228660583\n",
      "cnt: 0 - valLoss: 0.45225608348846436 - trainLoss: 0.44358450174331665\n",
      "cnt: 0 - valLoss: 0.4522235095500946 - trainLoss: 0.44355446100234985\n",
      "cnt: 0 - valLoss: 0.4521906077861786 - trainLoss: 0.44352447986602783\n",
      "cnt: 0 - valLoss: 0.45215821266174316 - trainLoss: 0.4434945583343506\n",
      "cnt: 0 - valLoss: 0.4521258771419525 - trainLoss: 0.4434644281864166\n",
      "cnt: 0 - valLoss: 0.4520936608314514 - trainLoss: 0.44343438744544983\n",
      "cnt: 0 - valLoss: 0.4520614445209503 - trainLoss: 0.4434044659137726\n",
      "cnt: 0 - valLoss: 0.452029287815094 - trainLoss: 0.44337454438209534\n",
      "cnt: 0 - valLoss: 0.45199716091156006 - trainLoss: 0.44334471225738525\n",
      "cnt: 0 - valLoss: 0.45196494460105896 - trainLoss: 0.44331496953964233\n",
      "cnt: 0 - valLoss: 0.45193272829055786 - trainLoss: 0.4432853162288666\n",
      "cnt: 0 - valLoss: 0.4519006311893463 - trainLoss: 0.44325578212738037\n",
      "cnt: 0 - valLoss: 0.45186862349510193 - trainLoss: 0.44322633743286133\n",
      "cnt: 0 - valLoss: 0.4518366754055023 - trainLoss: 0.4431969225406647\n",
      "cnt: 0 - valLoss: 0.4518047273159027 - trainLoss: 0.4431675672531128\n",
      "cnt: 0 - valLoss: 0.4517728090286255 - trainLoss: 0.4431382715702057\n",
      "cnt: 0 - valLoss: 0.4517408609390259 - trainLoss: 0.44310906529426575\n",
      "cnt: 0 - valLoss: 0.45170897245407104 - trainLoss: 0.4430798888206482\n",
      "cnt: 0 - valLoss: 0.4516771733760834 - trainLoss: 0.4430508315563202\n",
      "cnt: 0 - valLoss: 0.45164552330970764 - trainLoss: 0.4430217742919922\n",
      "cnt: 0 - valLoss: 0.45161381363868713 - trainLoss: 0.44299277663230896\n",
      "cnt: 0 - valLoss: 0.45158204436302185 - trainLoss: 0.44296392798423767\n",
      "cnt: 0 - valLoss: 0.45155030488967896 - trainLoss: 0.4429350197315216\n",
      "cnt: 0 - valLoss: 0.45151862502098083 - trainLoss: 0.4429062008857727\n",
      "cnt: 0 - valLoss: 0.45148640871047974 - trainLoss: 0.44287750124931335\n",
      "cnt: 0 - valLoss: 0.4514544606208801 - trainLoss: 0.4428485035896301\n",
      "cnt: 0 - valLoss: 0.4514223039150238 - trainLoss: 0.4428192973136902\n",
      "cnt: 0 - valLoss: 0.45139026641845703 - trainLoss: 0.44279009103775024\n",
      "cnt: 0 - valLoss: 0.4513583481311798 - trainLoss: 0.44276100397109985\n",
      "cnt: 0 - valLoss: 0.451326459646225 - trainLoss: 0.44273194670677185\n",
      "cnt: 0 - valLoss: 0.45129457116127014 - trainLoss: 0.44270285964012146\n",
      "cnt: 0 - valLoss: 0.4512627422809601 - trainLoss: 0.4426738917827606\n",
      "cnt: 0 - valLoss: 0.45123109221458435 - trainLoss: 0.44264495372772217\n",
      "cnt: 0 - valLoss: 0.451199471950531 - trainLoss: 0.44261589646339417\n",
      "cnt: 0 - valLoss: 0.45116761326789856 - trainLoss: 0.4425869882106781\n",
      "cnt: 0 - valLoss: 0.45113587379455566 - trainLoss: 0.4425579905509949\n",
      "cnt: 0 - valLoss: 0.45110422372817993 - trainLoss: 0.4425290822982788\n",
      "cnt: 0 - valLoss: 0.45107248425483704 - trainLoss: 0.44250017404556274\n",
      "cnt: 0 - valLoss: 0.45104098320007324 - trainLoss: 0.44247132539749146\n",
      "cnt: 0 - valLoss: 0.4510095715522766 - trainLoss: 0.44244253635406494\n",
      "cnt: 0 - valLoss: 0.4509781301021576 - trainLoss: 0.44241365790367126\n",
      "cnt: 0 - valLoss: 0.45094677805900574 - trainLoss: 0.44238486886024475\n",
      "cnt: 0 - valLoss: 0.45091545581817627 - trainLoss: 0.4423561692237854\n",
      "cnt: 0 - valLoss: 0.4508841931819916 - trainLoss: 0.4423275589942932\n",
      "cnt: 0 - valLoss: 0.45085281133651733 - trainLoss: 0.4422989785671234\n",
      "cnt: 0 - valLoss: 0.45082157850265503 - trainLoss: 0.442270427942276\n",
      "cnt: 0 - valLoss: 0.4507903456687927 - trainLoss: 0.4422420561313629\n",
      "cnt: 0 - valLoss: 0.45075926184654236 - trainLoss: 0.44221359491348267\n",
      "cnt: 0 - valLoss: 0.450728178024292 - trainLoss: 0.4421852231025696\n",
      "cnt: 0 - valLoss: 0.450697124004364 - trainLoss: 0.44215691089630127\n",
      "cnt: 0 - valLoss: 0.4506661295890808 - trainLoss: 0.44212865829467773\n",
      "cnt: 0 - valLoss: 0.4506351053714752 - trainLoss: 0.4421004354953766\n",
      "cnt: 0 - valLoss: 0.45060408115386963 - trainLoss: 0.4420722424983978\n",
      "cnt: 0 - valLoss: 0.45057305693626404 - trainLoss: 0.44204404950141907\n",
      "cnt: 0 - valLoss: 0.4505422115325928 - trainLoss: 0.4420158565044403\n",
      "cnt: 0 - valLoss: 0.4505113363265991 - trainLoss: 0.44198769330978394\n",
      "cnt: 0 - valLoss: 0.450480580329895 - trainLoss: 0.4419596195220947\n",
      "cnt: 0 - valLoss: 0.45044979453086853 - trainLoss: 0.4419316053390503\n",
      "cnt: 0 - valLoss: 0.4504188895225525 - trainLoss: 0.44190362095832825\n",
      "cnt: 0 - valLoss: 0.4503878951072693 - trainLoss: 0.4418756663799286\n",
      "cnt: 0 - valLoss: 0.45035699009895325 - trainLoss: 0.4418478310108185\n",
      "cnt: 0 - valLoss: 0.4503260850906372 - trainLoss: 0.4418199956417084\n",
      "cnt: 0 - valLoss: 0.4502952992916107 - trainLoss: 0.44179221987724304\n",
      "cnt: 0 - valLoss: 0.4502648413181305 - trainLoss: 0.4417644441127777\n",
      "cnt: 0 - valLoss: 0.4502345025539398 - trainLoss: 0.4417365491390228\n",
      "cnt: 0 - valLoss: 0.45020419359207153 - trainLoss: 0.44170883297920227\n",
      "cnt: 0 - valLoss: 0.4501740038394928 - trainLoss: 0.4416811168193817\n",
      "cnt: 0 - valLoss: 0.45014381408691406 - trainLoss: 0.44165343046188354\n",
      "cnt: 0 - valLoss: 0.4501137137413025 - trainLoss: 0.4416257441043854\n",
      "cnt: 0 - valLoss: 0.4500836431980133 - trainLoss: 0.441598117351532\n",
      "cnt: 0 - valLoss: 0.4500536024570465 - trainLoss: 0.4415704905986786\n",
      "cnt: 0 - valLoss: 0.4500236213207245 - trainLoss: 0.44154298305511475\n",
      "cnt: 0 - valLoss: 0.44999366998672485 - trainLoss: 0.4415154457092285\n",
      "cnt: 0 - valLoss: 0.44996377825737 - trainLoss: 0.44148802757263184\n",
      "cnt: 0 - valLoss: 0.4499339461326599 - trainLoss: 0.44146057963371277\n",
      "cnt: 0 - valLoss: 0.449904203414917 - trainLoss: 0.44143325090408325\n",
      "cnt: 0 - valLoss: 0.44987452030181885 - trainLoss: 0.44140592217445374\n",
      "cnt: 0 - valLoss: 0.4498448669910431 - trainLoss: 0.44137856364250183\n",
      "cnt: 0 - valLoss: 0.4498152732849121 - trainLoss: 0.4413513243198395\n",
      "cnt: 0 - valLoss: 0.4497857391834259 - trainLoss: 0.4413240849971771\n",
      "cnt: 0 - valLoss: 0.4497562646865845 - trainLoss: 0.44129690527915955\n",
      "cnt: 0 - valLoss: 0.44972679018974304 - trainLoss: 0.4412696957588196\n",
      "cnt: 0 - valLoss: 0.4496977925300598 - trainLoss: 0.4412424564361572\n",
      "cnt: 0 - valLoss: 0.4496687352657318 - trainLoss: 0.44121503829956055\n",
      "cnt: 0 - valLoss: 0.44963976740837097 - trainLoss: 0.44118767976760864\n",
      "cnt: 0 - valLoss: 0.4496108591556549 - trainLoss: 0.4411604106426239\n",
      "cnt: 0 - valLoss: 0.44958198070526123 - trainLoss: 0.44113317131996155\n",
      "cnt: 0 - valLoss: 0.44955316185951233 - trainLoss: 0.4411059617996216\n",
      "cnt: 0 - valLoss: 0.4495243430137634 - trainLoss: 0.441078782081604\n",
      "cnt: 0 - valLoss: 0.4494956433773041 - trainLoss: 0.4410516321659088\n",
      "cnt: 0 - valLoss: 0.44946685433387756 - trainLoss: 0.4410244822502136\n",
      "cnt: 0 - valLoss: 0.4494381546974182 - trainLoss: 0.4409973919391632\n",
      "cnt: 0 - valLoss: 0.4494094252586365 - trainLoss: 0.44097036123275757\n",
      "cnt: 0 - valLoss: 0.44938069581985474 - trainLoss: 0.44094330072402954\n",
      "cnt: 0 - valLoss: 0.4493521749973297 - trainLoss: 0.4409162998199463\n",
      "cnt: 0 - valLoss: 0.4493235647678375 - trainLoss: 0.44088926911354065\n",
      "cnt: 0 - valLoss: 0.4492950439453125 - trainLoss: 0.4408622086048126\n",
      "cnt: 0 - valLoss: 0.4492665231227875 - trainLoss: 0.44083520770072937\n",
      "cnt: 0 - valLoss: 0.44923803210258484 - trainLoss: 0.44080832600593567\n",
      "cnt: 0 - valLoss: 0.449209600687027 - trainLoss: 0.4407814145088196\n",
      "cnt: 0 - valLoss: 0.44918113946914673 - trainLoss: 0.44075456261634827\n",
      "cnt: 0 - valLoss: 0.4491526782512665 - trainLoss: 0.4407278299331665\n",
      "cnt: 0 - valLoss: 0.44912421703338623 - trainLoss: 0.44070106744766235\n",
      "cnt: 0 - valLoss: 0.4490959346294403 - trainLoss: 0.44067442417144775\n",
      "cnt: 0 - valLoss: 0.44906753301620483 - trainLoss: 0.44064781069755554\n",
      "cnt: 0 - valLoss: 0.44903936982154846 - trainLoss: 0.44062119722366333\n",
      "cnt: 0 - valLoss: 0.4490111172199249 - trainLoss: 0.44059470295906067\n",
      "cnt: 0 - valLoss: 0.4489828944206238 - trainLoss: 0.4405681788921356\n",
      "cnt: 0 - valLoss: 0.4489547312259674 - trainLoss: 0.4405417740345001\n",
      "cnt: 0 - valLoss: 0.44892656803131104 - trainLoss: 0.4405154585838318\n",
      "cnt: 0 - valLoss: 0.4488983154296875 - trainLoss: 0.4404890537261963\n",
      "cnt: 0 - valLoss: 0.448869913816452 - trainLoss: 0.44046270847320557\n",
      "cnt: 0 - valLoss: 0.4488414525985718 - trainLoss: 0.4404364228248596\n",
      "cnt: 0 - valLoss: 0.4488130807876587 - trainLoss: 0.44041013717651367\n",
      "cnt: 0 - valLoss: 0.4487847089767456 - trainLoss: 0.4403839111328125\n",
      "cnt: 0 - valLoss: 0.4487563967704773 - trainLoss: 0.4403577446937561\n",
      "cnt: 0 - valLoss: 0.448728084564209 - trainLoss: 0.4403315782546997\n",
      "cnt: 0 - valLoss: 0.44869983196258545 - trainLoss: 0.44030556082725525\n",
      "cnt: 0 - valLoss: 0.4486716389656067 - trainLoss: 0.44027945399284363\n",
      "cnt: 0 - valLoss: 0.44864341616630554 - trainLoss: 0.44025343656539917\n",
      "cnt: 0 - valLoss: 0.4486151337623596 - trainLoss: 0.4402274191379547\n",
      "cnt: 0 - valLoss: 0.4485867917537689 - trainLoss: 0.4402015507221222\n",
      "cnt: 0 - valLoss: 0.4485585391521454 - trainLoss: 0.4401756823062897\n",
      "cnt: 0 - valLoss: 0.4485301673412323 - trainLoss: 0.44014984369277954\n",
      "cnt: 0 - valLoss: 0.44850170612335205 - trainLoss: 0.4401242136955261\n",
      "cnt: 0 - valLoss: 0.4484732449054718 - trainLoss: 0.4400986433029175\n",
      "cnt: 0 - valLoss: 0.44844478368759155 - trainLoss: 0.44007307291030884\n",
      "cnt: 0 - valLoss: 0.4484162926673889 - trainLoss: 0.44004759192466736\n",
      "cnt: 0 - valLoss: 0.44838783144950867 - trainLoss: 0.44002214074134827\n",
      "cnt: 0 - valLoss: 0.4483594298362732 - trainLoss: 0.43999677896499634\n",
      "cnt: 0 - valLoss: 0.4483310878276825 - trainLoss: 0.439971387386322\n",
      "cnt: 0 - valLoss: 0.44830286502838135 - trainLoss: 0.43994611501693726\n",
      "cnt: 0 - valLoss: 0.44827479124069214 - trainLoss: 0.4399207532405853\n",
      "cnt: 0 - valLoss: 0.4482467770576477 - trainLoss: 0.43989554047584534\n",
      "cnt: 0 - valLoss: 0.44821885228157043 - trainLoss: 0.4398702383041382\n",
      "cnt: 0 - valLoss: 0.4481908977031708 - trainLoss: 0.4398450255393982\n",
      "cnt: 0 - valLoss: 0.44816306233406067 - trainLoss: 0.4398198127746582\n",
      "cnt: 0 - valLoss: 0.4481351673603058 - trainLoss: 0.4397946894168854\n",
      "cnt: 0 - valLoss: 0.4481072723865509 - trainLoss: 0.439769446849823\n",
      "cnt: 0 - valLoss: 0.4480794370174408 - trainLoss: 0.43974441289901733\n",
      "cnt: 0 - valLoss: 0.4480518102645874 - trainLoss: 0.4397193193435669\n",
      "cnt: 0 - valLoss: 0.4480241537094116 - trainLoss: 0.43969422578811646\n",
      "cnt: 0 - valLoss: 0.4479965567588806 - trainLoss: 0.4396691918373108\n",
      "cnt: 0 - valLoss: 0.4479689598083496 - trainLoss: 0.4396441876888275\n",
      "cnt: 0 - valLoss: 0.4479413330554962 - trainLoss: 0.4396192729473114\n",
      "cnt: 0 - valLoss: 0.4479137659072876 - trainLoss: 0.4395943582057953\n",
      "cnt: 0 - valLoss: 0.4478861093521118 - trainLoss: 0.43956947326660156\n",
      "cnt: 0 - valLoss: 0.4478585124015808 - trainLoss: 0.4395446181297302\n",
      "cnt: 0 - valLoss: 0.44783100485801697 - trainLoss: 0.43951982259750366\n",
      "cnt: 0 - valLoss: 0.4478035569190979 - trainLoss: 0.4394950866699219\n",
      "cnt: 0 - valLoss: 0.4477761387825012 - trainLoss: 0.4394703507423401\n",
      "cnt: 0 - valLoss: 0.44774875044822693 - trainLoss: 0.4394455850124359\n",
      "cnt: 0 - valLoss: 0.4477214515209198 - trainLoss: 0.4394209384918213\n",
      "cnt: 0 - valLoss: 0.44769424200057983 - trainLoss: 0.4393962323665619\n",
      "cnt: 0 - valLoss: 0.44766688346862793 - trainLoss: 0.43937164545059204\n",
      "cnt: 0 - valLoss: 0.44763946533203125 - trainLoss: 0.4393470883369446\n",
      "cnt: 0 - valLoss: 0.44761204719543457 - trainLoss: 0.4393225312232971\n",
      "cnt: 0 - valLoss: 0.44758474826812744 - trainLoss: 0.4392980933189392\n",
      "cnt: 0 - valLoss: 0.447557657957077 - trainLoss: 0.4392736852169037\n",
      "cnt: 0 - valLoss: 0.44753050804138184 - trainLoss: 0.43924933671951294\n",
      "cnt: 0 - valLoss: 0.4475034773349762 - trainLoss: 0.4392249882221222\n",
      "cnt: 0 - valLoss: 0.4474765658378601 - trainLoss: 0.4392007291316986\n",
      "cnt: 0 - valLoss: 0.447449654340744 - trainLoss: 0.4391763508319855\n",
      "cnt: 0 - valLoss: 0.4474228620529175 - trainLoss: 0.4391520619392395\n",
      "cnt: 0 - valLoss: 0.4473961591720581 - trainLoss: 0.43912771344184875\n",
      "cnt: 0 - valLoss: 0.4473694860935211 - trainLoss: 0.4391033947467804\n",
      "cnt: 0 - valLoss: 0.4473429024219513 - trainLoss: 0.43907907605171204\n",
      "cnt: 0 - valLoss: 0.4473161995410919 - trainLoss: 0.43905481696128845\n",
      "cnt: 0 - valLoss: 0.44728973507881165 - trainLoss: 0.4390306770801544\n",
      "cnt: 0 - valLoss: 0.44726327061653137 - trainLoss: 0.439006507396698\n",
      "cnt: 0 - valLoss: 0.4472368359565735 - trainLoss: 0.43898239731788635\n",
      "cnt: 0 - valLoss: 0.4472103416919708 - trainLoss: 0.4389582872390747\n",
      "cnt: 0 - valLoss: 0.4471837282180786 - trainLoss: 0.4389342665672302\n",
      "cnt: 0 - valLoss: 0.4471571445465088 - trainLoss: 0.4389103055000305\n",
      "cnt: 0 - valLoss: 0.44713065028190613 - trainLoss: 0.4388864040374756\n",
      "cnt: 0 - valLoss: 0.44710421562194824 - trainLoss: 0.4388624429702759\n",
      "cnt: 0 - valLoss: 0.4470777213573456 - trainLoss: 0.43883857131004333\n",
      "cnt: 0 - valLoss: 0.4470512270927429 - trainLoss: 0.438814640045166\n",
      "cnt: 0 - valLoss: 0.4470248520374298 - trainLoss: 0.43879079818725586\n",
      "cnt: 0 - valLoss: 0.4469985365867615 - trainLoss: 0.4387669563293457\n",
      "cnt: 0 - valLoss: 0.44697245955467224 - trainLoss: 0.438742995262146\n",
      "cnt: 0 - valLoss: 0.4469464123249054 - trainLoss: 0.4387190341949463\n",
      "cnt: 0 - valLoss: 0.4469205141067505 - trainLoss: 0.4386950731277466\n",
      "cnt: 0 - valLoss: 0.4468943178653717 - trainLoss: 0.4386711120605469\n",
      "cnt: 0 - valLoss: 0.4468681514263153 - trainLoss: 0.43864724040031433\n",
      "cnt: 0 - valLoss: 0.44684210419654846 - trainLoss: 0.4386232793331146\n",
      "cnt: 0 - valLoss: 0.44681593775749207 - trainLoss: 0.43859922885894775\n",
      "cnt: 0 - valLoss: 0.44678986072540283 - trainLoss: 0.43857541680336\n",
      "cnt: 0 - valLoss: 0.44676369428634644 - trainLoss: 0.4385516345500946\n",
      "cnt: 0 - valLoss: 0.4467375576496124 - trainLoss: 0.4385278522968292\n",
      "cnt: 0 - valLoss: 0.4467115104198456 - trainLoss: 0.43850409984588623\n",
      "cnt: 0 - valLoss: 0.44668543338775635 - trainLoss: 0.4384803771972656\n",
      "cnt: 0 - valLoss: 0.44665950536727905 - trainLoss: 0.4384567439556122\n",
      "cnt: 0 - valLoss: 0.44663354754447937 - trainLoss: 0.4384332299232483\n",
      "cnt: 0 - valLoss: 0.44660767912864685 - trainLoss: 0.4384097158908844\n",
      "cnt: 0 - valLoss: 0.44658172130584717 - trainLoss: 0.4383861720561981\n",
      "cnt: 0 - valLoss: 0.44655582308769226 - trainLoss: 0.43836280703544617\n",
      "cnt: 0 - valLoss: 0.4465300738811493 - trainLoss: 0.43833932280540466\n",
      "cnt: 0 - valLoss: 0.4465043246746063 - trainLoss: 0.4383159577846527\n",
      "cnt: 0 - valLoss: 0.4464787244796753 - trainLoss: 0.43829256296157837\n",
      "cnt: 0 - valLoss: 0.4464530646800995 - trainLoss: 0.4382692575454712\n",
      "cnt: 0 - valLoss: 0.4464273750782013 - trainLoss: 0.4382459819316864\n",
      "cnt: 0 - valLoss: 0.4464017450809479 - trainLoss: 0.43822282552719116\n",
      "cnt: 0 - valLoss: 0.44637608528137207 - trainLoss: 0.4381996989250183\n",
      "cnt: 0 - valLoss: 0.4463503956794739 - trainLoss: 0.43817663192749023\n",
      "cnt: 0 - valLoss: 0.44632482528686523 - trainLoss: 0.4381536841392517\n",
      "cnt: 0 - valLoss: 0.44629931449890137 - trainLoss: 0.4381307363510132\n",
      "cnt: 0 - valLoss: 0.4462737739086151 - trainLoss: 0.4381079077720642\n",
      "cnt: 0 - valLoss: 0.44624829292297363 - trainLoss: 0.43808501958847046\n",
      "cnt: 0 - valLoss: 0.4462229311466217 - trainLoss: 0.4380621910095215\n",
      "cnt: 0 - valLoss: 0.4461974501609802 - trainLoss: 0.4380393326282501\n",
      "cnt: 0 - valLoss: 0.44617196917533875 - trainLoss: 0.4380165636539459\n",
      "cnt: 0 - valLoss: 0.44614651799201965 - trainLoss: 0.4379936754703522\n",
      "cnt: 0 - valLoss: 0.44612112641334534 - trainLoss: 0.4379708170890808\n",
      "cnt: 0 - valLoss: 0.446095734834671 - trainLoss: 0.43794798851013184\n",
      "cnt: 0 - valLoss: 0.44607046246528625 - trainLoss: 0.4379251301288605\n",
      "cnt: 0 - valLoss: 0.4460452198982239 - trainLoss: 0.4379023611545563\n",
      "cnt: 0 - valLoss: 0.4460201561450958 - trainLoss: 0.4378795027732849\n",
      "cnt: 0 - valLoss: 0.4459948241710663 - trainLoss: 0.4378568232059479\n",
      "cnt: 0 - valLoss: 0.44596967101097107 - trainLoss: 0.43783432245254517\n",
      "cnt: 0 - valLoss: 0.4459446370601654 - trainLoss: 0.43781188130378723\n",
      "cnt: 0 - valLoss: 0.4459194540977478 - trainLoss: 0.4377894401550293\n",
      "cnt: 0 - valLoss: 0.4458945691585541 - trainLoss: 0.4377669394016266\n",
      "cnt: 0 - valLoss: 0.44586968421936035 - trainLoss: 0.43774452805519104\n",
      "cnt: 0 - valLoss: 0.44584473967552185 - trainLoss: 0.4377220869064331\n",
      "cnt: 0 - valLoss: 0.4458199739456177 - trainLoss: 0.43769964575767517\n",
      "cnt: 0 - valLoss: 0.4457949995994568 - trainLoss: 0.4376772940158844\n",
      "cnt: 0 - valLoss: 0.4457700848579407 - trainLoss: 0.43765509128570557\n",
      "cnt: 0 - valLoss: 0.44574517011642456 - trainLoss: 0.4376329183578491\n",
      "cnt: 0 - valLoss: 0.4457204341888428 - trainLoss: 0.43761077523231506\n",
      "cnt: 0 - valLoss: 0.445695698261261 - trainLoss: 0.43758857250213623\n",
      "cnt: 0 - valLoss: 0.44567108154296875 - trainLoss: 0.4375664293766022\n",
      "cnt: 0 - valLoss: 0.4456464350223541 - trainLoss: 0.43754419684410095\n",
      "cnt: 0 - valLoss: 0.4456218481063843 - trainLoss: 0.43752211332321167\n",
      "cnt: 0 - valLoss: 0.44559726119041443 - trainLoss: 0.4375\n",
      "cnt: 0 - valLoss: 0.44557279348373413 - trainLoss: 0.43747788667678833\n",
      "cnt: 0 - valLoss: 0.44554823637008667 - trainLoss: 0.43745580315589905\n",
      "cnt: 0 - valLoss: 0.44552287459373474 - trainLoss: 0.4374338686466217\n",
      "cnt: 0 - valLoss: 0.44549745321273804 - trainLoss: 0.4374121129512787\n",
      "cnt: 0 - valLoss: 0.44547179341316223 - trainLoss: 0.4373902976512909\n",
      "cnt: 0 - valLoss: 0.44544607400894165 - trainLoss: 0.43736863136291504\n",
      "cnt: 0 - valLoss: 0.44542041420936584 - trainLoss: 0.43734702467918396\n",
      "cnt: 0 - valLoss: 0.4453947842121124 - trainLoss: 0.43732550740242004\n",
      "cnt: 0 - valLoss: 0.4453692138195038 - trainLoss: 0.4373040199279785\n",
      "cnt: 0 - valLoss: 0.44534361362457275 - trainLoss: 0.4372825622558594\n",
      "cnt: 0 - valLoss: 0.4453180730342865 - trainLoss: 0.4372611343860626\n",
      "cnt: 0 - valLoss: 0.44529256224632263 - trainLoss: 0.4372398257255554\n",
      "cnt: 0 - valLoss: 0.44526711106300354 - trainLoss: 0.43721839785575867\n",
      "cnt: 0 - valLoss: 0.44524163007736206 - trainLoss: 0.4371970593929291\n",
      "cnt: 0 - valLoss: 0.44521623849868774 - trainLoss: 0.4371757507324219\n",
      "cnt: 0 - valLoss: 0.4451908767223358 - trainLoss: 0.43715453147888184\n",
      "cnt: 0 - valLoss: 0.44516560435295105 - trainLoss: 0.4371333122253418\n",
      "cnt: 0 - valLoss: 0.44514036178588867 - trainLoss: 0.43711206316947937\n",
      "cnt: 0 - valLoss: 0.44511517882347107 - trainLoss: 0.4370909035205841\n",
      "cnt: 0 - valLoss: 0.4450901448726654 - trainLoss: 0.43706971406936646\n",
      "cnt: 0 - valLoss: 0.44506511092185974 - trainLoss: 0.4370485842227936\n",
      "cnt: 0 - valLoss: 0.44504013657569885 - trainLoss: 0.4370274543762207\n",
      "cnt: 0 - valLoss: 0.4450152814388275 - trainLoss: 0.4370063245296478\n",
      "cnt: 0 - valLoss: 0.4449904263019562 - trainLoss: 0.4369853138923645\n",
      "cnt: 0 - valLoss: 0.444965660572052 - trainLoss: 0.4369642436504364\n",
      "cnt: 0 - valLoss: 0.444940984249115 - trainLoss: 0.4369431436061859\n",
      "cnt: 0 - valLoss: 0.444916307926178 - trainLoss: 0.436922162771225\n",
      "cnt: 0 - valLoss: 0.44489172101020813 - trainLoss: 0.43690118193626404\n",
      "cnt: 0 - valLoss: 0.44486722350120544 - trainLoss: 0.4368802011013031\n",
      "cnt: 0 - valLoss: 0.44484275579452515 - trainLoss: 0.43685927987098694\n",
      "cnt: 0 - valLoss: 0.4448183476924896 - trainLoss: 0.43683838844299316\n",
      "cnt: 0 - valLoss: 0.44479402899742126 - trainLoss: 0.436817467212677\n",
      "cnt: 0 - valLoss: 0.44476965069770813 - trainLoss: 0.4367965757846832\n",
      "cnt: 0 - valLoss: 0.44474539160728455 - trainLoss: 0.43677574396133423\n",
      "cnt: 0 - valLoss: 0.44472119212150574 - trainLoss: 0.43675491213798523\n",
      "cnt: 0 - valLoss: 0.44469696283340454 - trainLoss: 0.436734139919281\n",
      "cnt: 0 - valLoss: 0.44467294216156006 - trainLoss: 0.4367132782936096\n",
      "cnt: 0 - valLoss: 0.4446488320827484 - trainLoss: 0.43669256567955017\n",
      "cnt: 0 - valLoss: 0.44462451338768005 - trainLoss: 0.43667173385620117\n",
      "cnt: 0 - valLoss: 0.44460034370422363 - trainLoss: 0.4366510808467865\n",
      "cnt: 0 - valLoss: 0.4445761740207672 - trainLoss: 0.43663039803504944\n",
      "cnt: 0 - valLoss: 0.44455212354660034 - trainLoss: 0.4366097152233124\n",
      "cnt: 0 - valLoss: 0.44452810287475586 - trainLoss: 0.4365891218185425\n",
      "cnt: 0 - valLoss: 0.44450417160987854 - trainLoss: 0.4365685284137726\n",
      "cnt: 0 - valLoss: 0.444480299949646 - trainLoss: 0.4365479350090027\n",
      "cnt: 0 - valLoss: 0.44445639848709106 - trainLoss: 0.43652740120887756\n",
      "cnt: 0 - valLoss: 0.44443264603614807 - trainLoss: 0.43650686740875244\n",
      "cnt: 0 - valLoss: 0.4444088339805603 - trainLoss: 0.4364863634109497\n",
      "cnt: 0 - valLoss: 0.44438499212265015 - trainLoss: 0.43646591901779175\n",
      "cnt: 0 - valLoss: 0.44436126947402954 - trainLoss: 0.4364454746246338\n",
      "cnt: 0 - valLoss: 0.44433754682540894 - trainLoss: 0.4364250898361206\n",
      "cnt: 0 - valLoss: 0.44431379437446594 - trainLoss: 0.4364047050476074\n",
      "cnt: 0 - valLoss: 0.4442901015281677 - trainLoss: 0.4363843500614166\n",
      "cnt: 0 - valLoss: 0.44426649808883667 - trainLoss: 0.43636399507522583\n",
      "cnt: 0 - valLoss: 0.444242924451828 - trainLoss: 0.4363436698913574\n",
      "cnt: 0 - valLoss: 0.4442194700241089 - trainLoss: 0.4363233745098114\n",
      "cnt: 0 - valLoss: 0.4441959857940674 - trainLoss: 0.4363030791282654\n",
      "cnt: 0 - valLoss: 0.44417262077331543 - trainLoss: 0.43628281354904175\n",
      "cnt: 0 - valLoss: 0.4441492259502411 - trainLoss: 0.4362625777721405\n",
      "cnt: 0 - valLoss: 0.44412603974342346 - trainLoss: 0.43624237179756165\n",
      "cnt: 0 - valLoss: 0.44410282373428345 - trainLoss: 0.4362221658229828\n",
      "cnt: 0 - valLoss: 0.4440796673297882 - trainLoss: 0.43620193004608154\n",
      "cnt: 0 - valLoss: 0.4440564215183258 - trainLoss: 0.43618178367614746\n",
      "cnt: 0 - valLoss: 0.44403332471847534 - trainLoss: 0.4361615777015686\n",
      "cnt: 0 - valLoss: 0.4440102279186249 - trainLoss: 0.4361414909362793\n",
      "cnt: 0 - valLoss: 0.4439871907234192 - trainLoss: 0.4361213147640228\n",
      "cnt: 0 - valLoss: 0.4439641237258911 - trainLoss: 0.43610116839408875\n",
      "cnt: 0 - valLoss: 0.443941205739975 - trainLoss: 0.4360811412334442\n",
      "cnt: 0 - valLoss: 0.44391828775405884 - trainLoss: 0.4360610246658325\n",
      "cnt: 0 - valLoss: 0.44389548897743225 - trainLoss: 0.4360409379005432\n",
      "cnt: 0 - valLoss: 0.44387269020080566 - trainLoss: 0.4360208809375763\n",
      "cnt: 0 - valLoss: 0.44385001063346863 - trainLoss: 0.43600088357925415\n",
      "cnt: 0 - valLoss: 0.44382721185684204 - trainLoss: 0.4359808564186096\n",
      "cnt: 0 - valLoss: 0.4438049793243408 - trainLoss: 0.435960978269577\n",
      "cnt: 0 - valLoss: 0.4437826871871948 - trainLoss: 0.43594110012054443\n",
      "cnt: 0 - valLoss: 0.44376039505004883 - trainLoss: 0.435921311378479\n",
      "cnt: 0 - valLoss: 0.44373819231987 - trainLoss: 0.4359014928340912\n",
      "cnt: 0 - valLoss: 0.4437158703804016 - trainLoss: 0.435881644487381\n",
      "cnt: 0 - valLoss: 0.4436936676502228 - trainLoss: 0.43586182594299316\n",
      "cnt: 0 - valLoss: 0.443671315908432 - trainLoss: 0.4358420670032501\n",
      "cnt: 0 - valLoss: 0.443649023771286 - trainLoss: 0.4358223080635071\n",
      "cnt: 0 - valLoss: 0.44362708926200867 - trainLoss: 0.43580254912376404\n",
      "cnt: 0 - valLoss: 0.44360506534576416 - trainLoss: 0.43578261137008667\n",
      "cnt: 0 - valLoss: 0.44358307123184204 - trainLoss: 0.4357627034187317\n",
      "cnt: 0 - valLoss: 0.4435611963272095 - trainLoss: 0.4357428550720215\n",
      "cnt: 0 - valLoss: 0.4435392916202545 - trainLoss: 0.4357229471206665\n",
      "cnt: 0 - valLoss: 0.44351726770401 - trainLoss: 0.4357030689716339\n",
      "cnt: 0 - valLoss: 0.44349542260169983 - trainLoss: 0.4356832206249237\n",
      "cnt: 0 - valLoss: 0.4434734880924225 - trainLoss: 0.43566346168518066\n",
      "cnt: 0 - valLoss: 0.4434516131877899 - trainLoss: 0.43564367294311523\n",
      "cnt: 0 - valLoss: 0.44342976808547974 - trainLoss: 0.4356239438056946\n",
      "cnt: 0 - valLoss: 0.4434078633785248 - trainLoss: 0.43560418486595154\n",
      "cnt: 0 - valLoss: 0.4433859586715698 - trainLoss: 0.43558457493782043\n",
      "cnt: 0 - valLoss: 0.4433641731739044 - trainLoss: 0.43556487560272217\n",
      "cnt: 0 - valLoss: 0.443342387676239 - trainLoss: 0.4355452060699463\n",
      "cnt: 0 - valLoss: 0.44332069158554077 - trainLoss: 0.4355255961418152\n",
      "cnt: 0 - valLoss: 0.4432990849018097 - trainLoss: 0.4355059862136841\n",
      "cnt: 0 - valLoss: 0.4432774484157562 - trainLoss: 0.43548643589019775\n",
      "cnt: 0 - valLoss: 0.4432559013366699 - trainLoss: 0.43546682596206665\n",
      "cnt: 0 - valLoss: 0.44323423504829407 - trainLoss: 0.4354473054409027\n",
      "cnt: 0 - valLoss: 0.4432128667831421 - trainLoss: 0.4354276955127716\n",
      "cnt: 0 - valLoss: 0.44319137930870056 - trainLoss: 0.43540796637535095\n",
      "cnt: 0 - valLoss: 0.4431700110435486 - trainLoss: 0.4353884160518646\n",
      "cnt: 0 - valLoss: 0.4431487023830414 - trainLoss: 0.4353688955307007\n",
      "cnt: 0 - valLoss: 0.4431273639202118 - trainLoss: 0.43534940481185913\n",
      "cnt: 0 - valLoss: 0.44310587644577026 - trainLoss: 0.43532976508140564\n",
      "cnt: 0 - valLoss: 0.4430844187736511 - trainLoss: 0.4353102743625641\n",
      "cnt: 0 - valLoss: 0.44306302070617676 - trainLoss: 0.43529075384140015\n",
      "cnt: 0 - valLoss: 0.44304159283638 - trainLoss: 0.4352712333202362\n",
      "cnt: 0 - valLoss: 0.44302016496658325 - trainLoss: 0.43525177240371704\n",
      "cnt: 0 - valLoss: 0.4429987072944641 - trainLoss: 0.4352323114871979\n",
      "cnt: 0 - valLoss: 0.44297733902931213 - trainLoss: 0.4352128803730011\n",
      "cnt: 0 - valLoss: 0.44295594096183777 - trainLoss: 0.4351935386657715\n",
      "cnt: 0 - valLoss: 0.4429345726966858 - trainLoss: 0.43517419695854187\n",
      "cnt: 0 - valLoss: 0.4429132342338562 - trainLoss: 0.43515485525131226\n",
      "cnt: 0 - valLoss: 0.44289201498031616 - trainLoss: 0.43513551354408264\n",
      "cnt: 0 - valLoss: 0.4428708553314209 - trainLoss: 0.4351162612438202\n",
      "cnt: 0 - valLoss: 0.4428497850894928 - trainLoss: 0.43509694933891296\n",
      "cnt: 0 - valLoss: 0.4428286850452423 - trainLoss: 0.43507757782936096\n",
      "cnt: 0 - valLoss: 0.4428076446056366 - trainLoss: 0.43505820631980896\n",
      "cnt: 0 - valLoss: 0.4427867531776428 - trainLoss: 0.43503889441490173\n",
      "cnt: 0 - valLoss: 0.44276586174964905 - trainLoss: 0.4350195527076721\n",
      "cnt: 0 - valLoss: 0.4427449703216553 - trainLoss: 0.43500030040740967\n",
      "cnt: 0 - valLoss: 0.4427241086959839 - trainLoss: 0.43498101830482483\n",
      "cnt: 0 - valLoss: 0.4427032768726349 - trainLoss: 0.43496179580688477\n",
      "cnt: 0 - valLoss: 0.44268250465393066 - trainLoss: 0.4349425733089447\n",
      "cnt: 0 - valLoss: 0.44266167283058167 - trainLoss: 0.4349234104156494\n",
      "cnt: 0 - valLoss: 0.44264084100723267 - trainLoss: 0.4349043071269989\n",
      "cnt: 0 - valLoss: 0.44262000918388367 - trainLoss: 0.434885174036026\n",
      "cnt: 0 - valLoss: 0.44259917736053467 - trainLoss: 0.4348661005496979\n",
      "cnt: 0 - valLoss: 0.4425784945487976 - trainLoss: 0.43484702706336975\n",
      "cnt: 0 - valLoss: 0.44255784153938293 - trainLoss: 0.4348280131816864\n",
      "cnt: 0 - valLoss: 0.44253718852996826 - trainLoss: 0.4348089396953583\n",
      "cnt: 0 - valLoss: 0.4425165355205536 - trainLoss: 0.43478989601135254\n",
      "cnt: 0 - valLoss: 0.4424959719181061 - trainLoss: 0.4347709119319916\n",
      "cnt: 0 - valLoss: 0.44247540831565857 - trainLoss: 0.4347519874572754\n",
      "cnt: 0 - valLoss: 0.4424549341201782 - trainLoss: 0.434733122587204\n",
      "cnt: 0 - valLoss: 0.4424344003200531 - trainLoss: 0.43471425771713257\n",
      "cnt: 0 - valLoss: 0.4424140155315399 - trainLoss: 0.43469542264938354\n",
      "cnt: 0 - valLoss: 0.44239360094070435 - trainLoss: 0.43467652797698975\n",
      "cnt: 0 - valLoss: 0.4423733949661255 - trainLoss: 0.4346577525138855\n",
      "cnt: 0 - valLoss: 0.44235312938690186 - trainLoss: 0.4346386790275574\n",
      "cnt: 0 - valLoss: 0.4423329830169678 - trainLoss: 0.4346196949481964\n",
      "cnt: 0 - valLoss: 0.4423128068447113 - trainLoss: 0.4346007704734802\n",
      "cnt: 0 - valLoss: 0.44229260087013245 - trainLoss: 0.4345819056034088\n",
      "cnt: 0 - valLoss: 0.44227248430252075 - trainLoss: 0.4345629811286926\n",
      "cnt: 0 - valLoss: 0.4422524869441986 - trainLoss: 0.4345441460609436\n",
      "cnt: 0 - valLoss: 0.442232221364975 - trainLoss: 0.4345253109931946\n",
      "cnt: 0 - valLoss: 0.44221198558807373 - trainLoss: 0.43450647592544556\n",
      "cnt: 0 - valLoss: 0.4421917498111725 - trainLoss: 0.43448781967163086\n",
      "cnt: 0 - valLoss: 0.44217154383659363 - trainLoss: 0.43446916341781616\n",
      "cnt: 0 - valLoss: 0.4421512186527252 - trainLoss: 0.4344504773616791\n",
      "cnt: 0 - valLoss: 0.442130982875824 - trainLoss: 0.4344318211078644\n",
      "cnt: 0 - valLoss: 0.44211071729660034 - trainLoss: 0.43441322445869446\n",
      "cnt: 0 - valLoss: 0.4420905113220215 - trainLoss: 0.43439462780952454\n",
      "cnt: 0 - valLoss: 0.442070335149765 - trainLoss: 0.434376060962677\n",
      "cnt: 0 - valLoss: 0.44205009937286377 - trainLoss: 0.4343574643135071\n",
      "cnt: 0 - valLoss: 0.4420298635959625 - trainLoss: 0.4343389868736267\n",
      "cnt: 0 - valLoss: 0.44200941920280457 - trainLoss: 0.43432050943374634\n",
      "cnt: 0 - valLoss: 0.4419889450073242 - trainLoss: 0.4343021512031555\n",
      "cnt: 0 - valLoss: 0.44196847081184387 - trainLoss: 0.4342838227748871\n",
      "cnt: 0 - valLoss: 0.4419480562210083 - trainLoss: 0.43426552414894104\n",
      "cnt: 0 - valLoss: 0.44192811846733093 - trainLoss: 0.434247225522995\n",
      "cnt: 0 - valLoss: 0.44190818071365356 - trainLoss: 0.4342287480831146\n",
      "cnt: 0 - valLoss: 0.4418882131576538 - trainLoss: 0.4342103600502014\n",
      "cnt: 0 - valLoss: 0.4418680965900421 - trainLoss: 0.43419191241264343\n",
      "cnt: 0 - valLoss: 0.4418480098247528 - trainLoss: 0.4341735243797302\n",
      "cnt: 0 - valLoss: 0.4418278634548187 - trainLoss: 0.4341551959514618\n",
      "cnt: 0 - valLoss: 0.44180774688720703 - trainLoss: 0.43413689732551575\n",
      "cnt: 0 - valLoss: 0.44178786873817444 - trainLoss: 0.43411850929260254\n",
      "cnt: 0 - valLoss: 0.4417678415775299 - trainLoss: 0.4341002106666565\n",
      "cnt: 0 - valLoss: 0.44174784421920776 - trainLoss: 0.43408194184303284\n",
      "cnt: 0 - valLoss: 0.44172775745391846 - trainLoss: 0.43406370282173157\n",
      "cnt: 0 - valLoss: 0.4417076110839844 - trainLoss: 0.4340454936027527\n",
      "cnt: 0 - valLoss: 0.4416874647140503 - trainLoss: 0.4340273141860962\n",
      "cnt: 0 - valLoss: 0.441667377948761 - trainLoss: 0.4340091645717621\n",
      "cnt: 0 - valLoss: 0.44164732098579407 - trainLoss: 0.4339909553527832\n",
      "cnt: 0 - valLoss: 0.44162729382514954 - trainLoss: 0.4339728355407715\n",
      "cnt: 0 - valLoss: 0.4416072964668274 - trainLoss: 0.43395471572875977\n",
      "cnt: 0 - valLoss: 0.4415871202945709 - trainLoss: 0.43393662571907043\n",
      "cnt: 0 - valLoss: 0.44156700372695923 - trainLoss: 0.4339185953140259\n",
      "cnt: 0 - valLoss: 0.4415469467639923 - trainLoss: 0.4339006245136261\n",
      "cnt: 0 - valLoss: 0.4415269196033478 - trainLoss: 0.4338826537132263\n",
      "cnt: 0 - valLoss: 0.44150683283805847 - trainLoss: 0.43386465311050415\n",
      "cnt: 0 - valLoss: 0.44148674607276917 - trainLoss: 0.43384668231010437\n",
      "cnt: 0 - valLoss: 0.44146689772605896 - trainLoss: 0.43382880091667175\n",
      "cnt: 0 - valLoss: 0.441446989774704 - trainLoss: 0.4338107109069824\n",
      "cnt: 0 - valLoss: 0.4414271414279938 - trainLoss: 0.4337927997112274\n",
      "cnt: 0 - valLoss: 0.44140732288360596 - trainLoss: 0.4337747097015381\n",
      "cnt: 0 - valLoss: 0.4413875639438629 - trainLoss: 0.4337567687034607\n",
      "cnt: 0 - valLoss: 0.4413677453994751 - trainLoss: 0.4337387681007385\n",
      "cnt: 0 - valLoss: 0.44134777784347534 - trainLoss: 0.4337209165096283\n",
      "cnt: 0 - valLoss: 0.4413277804851532 - trainLoss: 0.43370309472084045\n",
      "cnt: 0 - valLoss: 0.44130784273147583 - trainLoss: 0.433685302734375\n",
      "cnt: 0 - valLoss: 0.4412879943847656 - trainLoss: 0.43366751074790955\n",
      "cnt: 0 - valLoss: 0.44126805663108826 - trainLoss: 0.4336496889591217\n",
      "cnt: 0 - valLoss: 0.4412482976913452 - trainLoss: 0.43363189697265625\n",
      "cnt: 0 - valLoss: 0.441228449344635 - trainLoss: 0.4336141347885132\n",
      "cnt: 0 - valLoss: 0.44120875000953674 - trainLoss: 0.4335964024066925\n",
      "cnt: 0 - valLoss: 0.4411890506744385 - trainLoss: 0.4335786998271942\n",
      "cnt: 0 - valLoss: 0.441169410943985 - trainLoss: 0.4335609972476959\n",
      "cnt: 0 - valLoss: 0.4411498010158539 - trainLoss: 0.43354326486587524\n",
      "cnt: 0 - valLoss: 0.44113048911094666 - trainLoss: 0.43352556228637695\n",
      "cnt: 0 - valLoss: 0.44111111760139465 - trainLoss: 0.4335078299045563\n",
      "cnt: 0 - valLoss: 0.44109174609184265 - trainLoss: 0.43349015712738037\n",
      "cnt: 0 - valLoss: 0.44107240438461304 - trainLoss: 0.43347251415252686\n",
      "cnt: 0 - valLoss: 0.44105300307273865 - trainLoss: 0.43345484137535095\n",
      "cnt: 0 - valLoss: 0.4410336911678314 - trainLoss: 0.43343719840049744\n",
      "cnt: 0 - valLoss: 0.4410143196582794 - trainLoss: 0.4334196448326111\n",
      "cnt: 0 - valLoss: 0.4409950375556946 - trainLoss: 0.43340200185775757\n",
      "cnt: 0 - valLoss: 0.44097575545310974 - trainLoss: 0.43338441848754883\n",
      "cnt: 0 - valLoss: 0.4409564435482025 - trainLoss: 0.43336692452430725\n",
      "cnt: 0 - valLoss: 0.4409370422363281 - trainLoss: 0.4333493709564209\n",
      "cnt: 0 - valLoss: 0.4409176707267761 - trainLoss: 0.4333319067955017\n",
      "cnt: 0 - valLoss: 0.4408982992172241 - trainLoss: 0.4333144426345825\n",
      "cnt: 0 - valLoss: 0.4408789575099945 - trainLoss: 0.43329697847366333\n",
      "cnt: 0 - valLoss: 0.44085970520973206 - trainLoss: 0.43327948451042175\n",
      "cnt: 0 - valLoss: 0.44084054231643677 - trainLoss: 0.4332619607448578\n",
      "cnt: 0 - valLoss: 0.4408213794231415 - trainLoss: 0.4332444667816162\n",
      "cnt: 0 - valLoss: 0.4408023953437805 - trainLoss: 0.433227002620697\n",
      "cnt: 0 - valLoss: 0.44078341126441956 - trainLoss: 0.43320953845977783\n",
      "cnt: 0 - valLoss: 0.440764456987381 - trainLoss: 0.4331921339035034\n",
      "cnt: 0 - valLoss: 0.4407455027103424 - trainLoss: 0.433174729347229\n",
      "cnt: 0 - valLoss: 0.44072654843330383 - trainLoss: 0.433157354593277\n",
      "cnt: 0 - valLoss: 0.4407075345516205 - trainLoss: 0.4331400990486145\n",
      "cnt: 0 - valLoss: 0.4406885802745819 - trainLoss: 0.43312278389930725\n",
      "cnt: 0 - valLoss: 0.4406696856021881 - trainLoss: 0.43310555815696716\n",
      "cnt: 0 - valLoss: 0.44065073132514954 - trainLoss: 0.4330882430076599\n",
      "cnt: 0 - valLoss: 0.4406318664550781 - trainLoss: 0.4330710470676422\n",
      "cnt: 0 - valLoss: 0.4406130611896515 - trainLoss: 0.43305376172065735\n",
      "cnt: 0 - valLoss: 0.4405941367149353 - trainLoss: 0.43303656578063965\n",
      "cnt: 0 - valLoss: 0.4405750334262848 - trainLoss: 0.43301934003829956\n",
      "cnt: 0 - valLoss: 0.4405559003353119 - trainLoss: 0.43300217390060425\n",
      "cnt: 0 - valLoss: 0.4405367970466614 - trainLoss: 0.4329850673675537\n",
      "cnt: 0 - valLoss: 0.4405176639556885 - trainLoss: 0.4329679310321808\n",
      "cnt: 0 - valLoss: 0.44049856066703796 - trainLoss: 0.432950884103775\n",
      "cnt: 0 - valLoss: 0.44047948718070984 - trainLoss: 0.4329338073730469\n",
      "cnt: 0 - valLoss: 0.4404604434967041 - trainLoss: 0.4329167306423187\n",
      "cnt: 0 - valLoss: 0.440441370010376 - trainLoss: 0.43289971351623535\n",
      "cnt: 0 - valLoss: 0.44042229652404785 - trainLoss: 0.4328826367855072\n",
      "cnt: 0 - valLoss: 0.4404032230377197 - trainLoss: 0.43286553025245667\n",
      "cnt: 0 - valLoss: 0.4403842091560364 - trainLoss: 0.4328484535217285\n",
      "cnt: 0 - valLoss: 0.4403652250766754 - trainLoss: 0.43283140659332275\n",
      "cnt: 0 - valLoss: 0.4403461217880249 - trainLoss: 0.4328143894672394\n",
      "cnt: 0 - valLoss: 0.44032707810401917 - trainLoss: 0.4327974021434784\n",
      "cnt: 0 - valLoss: 0.44030800461769104 - trainLoss: 0.432780385017395\n",
      "cnt: 0 - valLoss: 0.44028905034065247 - trainLoss: 0.4327634274959564\n",
      "cnt: 0 - valLoss: 0.4402700662612915 - trainLoss: 0.43274641036987305\n",
      "cnt: 0 - valLoss: 0.44025111198425293 - trainLoss: 0.43272948265075684\n",
      "cnt: 0 - valLoss: 0.4402320683002472 - trainLoss: 0.4327125549316406\n",
      "cnt: 0 - valLoss: 0.44021308422088623 - trainLoss: 0.4326956272125244\n",
      "cnt: 0 - valLoss: 0.44019412994384766 - trainLoss: 0.432678759098053\n",
      "cnt: 0 - valLoss: 0.44017520546913147 - trainLoss: 0.43266189098358154\n",
      "cnt: 0 - valLoss: 0.44015631079673767 - trainLoss: 0.4326450526714325\n",
      "cnt: 0 - valLoss: 0.44013744592666626 - trainLoss: 0.43262818455696106\n",
      "cnt: 0 - valLoss: 0.44011861085891724 - trainLoss: 0.4326114058494568\n",
      "cnt: 0 - valLoss: 0.4400997757911682 - trainLoss: 0.43259453773498535\n",
      "cnt: 0 - valLoss: 0.4400808811187744 - trainLoss: 0.4325777292251587\n",
      "cnt: 0 - valLoss: 0.4400620460510254 - trainLoss: 0.4325609505176544\n",
      "cnt: 0 - valLoss: 0.440043181180954 - trainLoss: 0.43254417181015015\n",
      "cnt: 0 - valLoss: 0.4400244355201721 - trainLoss: 0.4325273931026459\n",
      "cnt: 0 - valLoss: 0.44000568985939026 - trainLoss: 0.4325106143951416\n",
      "cnt: 0 - valLoss: 0.43998703360557556 - trainLoss: 0.43249383568763733\n",
      "cnt: 0 - valLoss: 0.43996840715408325 - trainLoss: 0.43247711658477783\n",
      "cnt: 0 - valLoss: 0.43994981050491333 - trainLoss: 0.43246039748191833\n",
      "cnt: 0 - valLoss: 0.4399312436580658 - trainLoss: 0.43244367837905884\n",
      "cnt: 0 - valLoss: 0.43991267681121826 - trainLoss: 0.43242692947387695\n",
      "cnt: 0 - valLoss: 0.4398941397666931 - trainLoss: 0.43241026997566223\n",
      "cnt: 0 - valLoss: 0.4398757815361023 - trainLoss: 0.4323935806751251\n",
      "cnt: 0 - valLoss: 0.43985745310783386 - trainLoss: 0.4323769211769104\n",
      "cnt: 0 - valLoss: 0.43983909487724304 - trainLoss: 0.43236029148101807\n",
      "cnt: 0 - valLoss: 0.43982088565826416 - trainLoss: 0.43234360218048096\n",
      "cnt: 0 - valLoss: 0.4398026466369629 - trainLoss: 0.4323269724845886\n",
      "cnt: 0 - valLoss: 0.43978431820869446 - trainLoss: 0.4323103427886963\n",
      "cnt: 0 - valLoss: 0.43976595997810364 - trainLoss: 0.43229377269744873\n",
      "cnt: 0 - valLoss: 0.4397476315498352 - trainLoss: 0.43227720260620117\n",
      "cnt: 0 - valLoss: 0.439729243516922 - trainLoss: 0.43226057291030884\n",
      "cnt: 0 - valLoss: 0.4397108554840088 - trainLoss: 0.4322439730167389\n",
      "cnt: 0 - valLoss: 0.43969255685806274 - trainLoss: 0.43222740292549133\n",
      "cnt: 0 - valLoss: 0.4396742880344391 - trainLoss: 0.432210773229599\n",
      "cnt: 0 - valLoss: 0.4396560490131378 - trainLoss: 0.43219420313835144\n",
      "cnt: 0 - valLoss: 0.4396378695964813 - trainLoss: 0.4321776032447815\n",
      "cnt: 0 - valLoss: 0.43961960077285767 - trainLoss: 0.4321610927581787\n",
      "cnt: 0 - valLoss: 0.4396013617515564 - trainLoss: 0.4321444630622864\n",
      "cnt: 0 - valLoss: 0.4395831525325775 - trainLoss: 0.4321278929710388\n",
      "cnt: 0 - valLoss: 0.4395647644996643 - trainLoss: 0.43211129307746887\n",
      "cnt: 0 - valLoss: 0.43954646587371826 - trainLoss: 0.4320947229862213\n",
      "cnt: 0 - valLoss: 0.4395281672477722 - trainLoss: 0.43207818269729614\n",
      "cnt: 0 - valLoss: 0.43950986862182617 - trainLoss: 0.43206167221069336\n",
      "cnt: 0 - valLoss: 0.4394916296005249 - trainLoss: 0.4320451319217682\n",
      "cnt: 0 - valLoss: 0.439473420381546 - trainLoss: 0.4320286512374878\n",
      "cnt: 0 - valLoss: 0.4394552409648895 - trainLoss: 0.4320121109485626\n",
      "cnt: 0 - valLoss: 0.43943706154823303 - trainLoss: 0.4319956302642822\n",
      "cnt: 0 - valLoss: 0.43941885232925415 - trainLoss: 0.43197914958000183\n",
      "cnt: 0 - valLoss: 0.4394006133079529 - trainLoss: 0.43196266889572144\n",
      "cnt: 0 - valLoss: 0.4393824338912964 - trainLoss: 0.43194618821144104\n",
      "cnt: 0 - valLoss: 0.4393642842769623 - trainLoss: 0.4319297671318054\n",
      "cnt: 0 - valLoss: 0.43934616446495056 - trainLoss: 0.4319133162498474\n",
      "cnt: 0 - valLoss: 0.43932807445526123 - trainLoss: 0.4318968951702118\n",
      "cnt: 0 - valLoss: 0.4393100142478943 - trainLoss: 0.43188047409057617\n",
      "cnt: 0 - valLoss: 0.4392918646335602 - trainLoss: 0.43186408281326294\n",
      "cnt: 0 - valLoss: 0.4392739534378052 - trainLoss: 0.4318477511405945\n",
      "cnt: 0 - valLoss: 0.4392560124397278 - trainLoss: 0.43183135986328125\n",
      "cnt: 0 - valLoss: 0.43923795223236084 - trainLoss: 0.4318150281906128\n",
      "cnt: 0 - valLoss: 0.43922004103660583 - trainLoss: 0.43179869651794434\n",
      "cnt: 0 - valLoss: 0.43920189142227173 - trainLoss: 0.4317823648452759\n",
      "cnt: 0 - valLoss: 0.4391840100288391 - trainLoss: 0.4317660331726074\n",
      "cnt: 0 - valLoss: 0.4391661286354065 - trainLoss: 0.43174973130226135\n",
      "cnt: 0 - valLoss: 0.43914809823036194 - trainLoss: 0.43173345923423767\n",
      "cnt: 0 - valLoss: 0.4391303062438965 - trainLoss: 0.4317172169685364\n",
      "cnt: 0 - valLoss: 0.4391123950481415 - trainLoss: 0.4317009747028351\n",
      "cnt: 0 - valLoss: 0.4390946626663208 - trainLoss: 0.4316847324371338\n",
      "cnt: 0 - valLoss: 0.4390769302845001 - trainLoss: 0.43166854977607727\n",
      "cnt: 0 - valLoss: 0.4390590786933899 - trainLoss: 0.43165236711502075\n",
      "cnt: 0 - valLoss: 0.4390414357185364 - trainLoss: 0.43163612484931946\n",
      "cnt: 0 - valLoss: 0.43902361392974854 - trainLoss: 0.43161994218826294\n",
      "cnt: 0 - valLoss: 0.4390060007572174 - trainLoss: 0.4316037595272064\n",
      "cnt: 0 - valLoss: 0.43898817896842957 - trainLoss: 0.4315876066684723\n",
      "cnt: 0 - valLoss: 0.43897056579589844 - trainLoss: 0.43157151341438293\n",
      "cnt: 0 - valLoss: 0.438952773809433 - trainLoss: 0.4315553903579712\n",
      "cnt: 0 - valLoss: 0.43893519043922424 - trainLoss: 0.43153929710388184\n",
      "cnt: 0 - valLoss: 0.43891751766204834 - trainLoss: 0.4315232038497925\n",
      "cnt: 0 - valLoss: 0.4388999938964844 - trainLoss: 0.4315071105957031\n",
      "cnt: 0 - valLoss: 0.438882440328598 - trainLoss: 0.43149101734161377\n",
      "cnt: 0 - valLoss: 0.43886467814445496 - trainLoss: 0.4314749240875244\n",
      "cnt: 0 - valLoss: 0.43884724378585815 - trainLoss: 0.43145889043807983\n",
      "cnt: 0 - valLoss: 0.4388295114040375 - trainLoss: 0.43144285678863525\n",
      "cnt: 0 - valLoss: 0.4388118088245392 - trainLoss: 0.4314267933368683\n",
      "cnt: 0 - valLoss: 0.43879392743110657 - trainLoss: 0.43141084909439087\n",
      "cnt: 0 - valLoss: 0.43877625465393066 - trainLoss: 0.43139487504959106\n",
      "cnt: 0 - valLoss: 0.4387584328651428 - trainLoss: 0.43137893080711365\n",
      "cnt: 0 - valLoss: 0.4387407898902893 - trainLoss: 0.43136292695999146\n",
      "cnt: 0 - valLoss: 0.4387231171131134 - trainLoss: 0.4313470423221588\n",
      "cnt: 0 - valLoss: 0.43870556354522705 - trainLoss: 0.4313310980796814\n",
      "cnt: 0 - valLoss: 0.43868786096572876 - trainLoss: 0.431315153837204\n",
      "cnt: 0 - valLoss: 0.43867039680480957 - trainLoss: 0.43129926919937134\n",
      "cnt: 0 - valLoss: 0.43865275382995605 - trainLoss: 0.4312833249568939\n",
      "cnt: 0 - valLoss: 0.43863528966903687 - trainLoss: 0.4312674105167389\n",
      "cnt: 0 - valLoss: 0.4386177062988281 - trainLoss: 0.43125152587890625\n",
      "cnt: 0 - valLoss: 0.4386003613471985 - trainLoss: 0.43123558163642883\n",
      "cnt: 0 - valLoss: 0.4385828673839569 - trainLoss: 0.4312196969985962\n",
      "cnt: 0 - valLoss: 0.43856561183929443 - trainLoss: 0.43120378255844116\n",
      "cnt: 0 - valLoss: 0.43854817748069763 - trainLoss: 0.4311879575252533\n",
      "cnt: 0 - valLoss: 0.4385307729244232 - trainLoss: 0.43117210268974304\n",
      "cnt: 0 - valLoss: 0.43851324915885925 - trainLoss: 0.4311562478542328\n",
      "cnt: 0 - valLoss: 0.43849584460258484 - trainLoss: 0.43114036321640015\n",
      "cnt: 0 - valLoss: 0.43847835063934326 - trainLoss: 0.43112456798553467\n",
      "cnt: 0 - valLoss: 0.4384610652923584 - trainLoss: 0.4311087727546692\n",
      "cnt: 0 - valLoss: 0.4384436309337616 - trainLoss: 0.4310929775238037\n",
      "cnt: 0 - valLoss: 0.4384264349937439 - trainLoss: 0.43107712268829346\n",
      "cnt: 0 - valLoss: 0.4384090006351471 - trainLoss: 0.43106138706207275\n",
      "cnt: 0 - valLoss: 0.43839162588119507 - trainLoss: 0.4310455918312073\n",
      "cnt: 0 - valLoss: 0.43837442994117737 - trainLoss: 0.43102988600730896\n",
      "cnt: 0 - valLoss: 0.43835708498954773 - trainLoss: 0.43101412057876587\n",
      "cnt: 0 - valLoss: 0.4383399188518524 - trainLoss: 0.43099838495254517\n",
      "cnt: 0 - valLoss: 0.43832260370254517 - trainLoss: 0.43098267912864685\n",
      "cnt: 0 - valLoss: 0.43830540776252747 - trainLoss: 0.4309670031070709\n",
      "cnt: 0 - valLoss: 0.43828803300857544 - trainLoss: 0.4309512972831726\n",
      "cnt: 0 - valLoss: 0.4382708668708801 - trainLoss: 0.4309355914592743\n",
      "cnt: 0 - valLoss: 0.4382535219192505 - trainLoss: 0.43091997504234314\n",
      "cnt: 0 - valLoss: 0.43823620676994324 - trainLoss: 0.4309043884277344\n",
      "cnt: 0 - valLoss: 0.438218891620636 - trainLoss: 0.43088865280151367\n",
      "cnt: 0 - valLoss: 0.43820151686668396 - trainLoss: 0.43087297677993774\n",
      "cnt: 0 - valLoss: 0.43818429112434387 - trainLoss: 0.4308573007583618\n",
      "cnt: 0 - valLoss: 0.4381669759750366 - trainLoss: 0.43084168434143066\n",
      "cnt: 0 - valLoss: 0.43814989924430847 - trainLoss: 0.43082597851753235\n",
      "cnt: 0 - valLoss: 0.4381325840950012 - trainLoss: 0.4308103322982788\n",
      "cnt: 0 - valLoss: 0.4381153881549835 - trainLoss: 0.43079474568367004\n",
      "cnt: 0 - valLoss: 0.43809834122657776 - trainLoss: 0.4307791292667389\n",
      "cnt: 0 - valLoss: 0.4380810856819153 - trainLoss: 0.4307635426521301\n",
      "cnt: 0 - valLoss: 0.43806394934654236 - trainLoss: 0.430747926235199\n",
      "cnt: 0 - valLoss: 0.4380466938018799 - trainLoss: 0.430732399225235\n",
      "cnt: 0 - valLoss: 0.43802958726882935 - trainLoss: 0.4307168424129486\n",
      "cnt: 0 - valLoss: 0.43801236152648926 - trainLoss: 0.43070128560066223\n",
      "cnt: 0 - valLoss: 0.43799519538879395 - trainLoss: 0.43068578839302063\n",
      "cnt: 0 - valLoss: 0.43797823786735535 - trainLoss: 0.43067023158073425\n",
      "cnt: 0 - valLoss: 0.43796107172966003 - trainLoss: 0.4306546747684479\n",
      "cnt: 0 - valLoss: 0.4379441738128662 - trainLoss: 0.4306391179561615\n",
      "cnt: 0 - valLoss: 0.4379270672798157 - trainLoss: 0.4306235611438751\n",
      "cnt: 0 - valLoss: 0.4379100203514099 - trainLoss: 0.4306081235408783\n",
      "cnt: 0 - valLoss: 0.4378931224346161 - trainLoss: 0.4305925667285919\n",
      "cnt: 0 - valLoss: 0.4378760755062103 - trainLoss: 0.4305770993232727\n",
      "cnt: 0 - valLoss: 0.43785905838012695 - trainLoss: 0.4305616617202759\n",
      "cnt: 0 - valLoss: 0.4378422498703003 - trainLoss: 0.4305461347103119\n",
      "cnt: 0 - valLoss: 0.4378252625465393 - trainLoss: 0.4305306375026703\n",
      "cnt: 0 - valLoss: 0.43780842423439026 - trainLoss: 0.43051519989967346\n",
      "cnt: 0 - valLoss: 0.43779152631759644 - trainLoss: 0.43049973249435425\n",
      "cnt: 0 - valLoss: 0.43777453899383545 - trainLoss: 0.4304842948913574\n",
      "cnt: 0 - valLoss: 0.4377577304840088 - trainLoss: 0.430468887090683\n",
      "cnt: 0 - valLoss: 0.43774089217185974 - trainLoss: 0.43045341968536377\n",
      "cnt: 0 - valLoss: 0.43772396445274353 - trainLoss: 0.43043801188468933\n",
      "cnt: 0 - valLoss: 0.43770721554756165 - trainLoss: 0.4304225742816925\n",
      "cnt: 0 - valLoss: 0.4376903772354126 - trainLoss: 0.43040719628334045\n",
      "cnt: 0 - valLoss: 0.43767356872558594 - trainLoss: 0.430391788482666\n",
      "cnt: 0 - valLoss: 0.4376569390296936 - trainLoss: 0.4303763806819916\n",
      "cnt: 0 - valLoss: 0.43764010071754456 - trainLoss: 0.4303610026836395\n",
      "cnt: 0 - valLoss: 0.43762320280075073 - trainLoss: 0.4303455948829651\n",
      "cnt: 0 - valLoss: 0.4376062750816345 - trainLoss: 0.43033018708229065\n",
      "cnt: 0 - valLoss: 0.4375893175601959 - trainLoss: 0.4303147792816162\n",
      "cnt: 0 - valLoss: 0.43757250905036926 - trainLoss: 0.430299311876297\n",
      "cnt: 0 - valLoss: 0.43755558133125305 - trainLoss: 0.43028387427330017\n",
      "cnt: 0 - valLoss: 0.43753868341445923 - trainLoss: 0.4302684962749481\n",
      "cnt: 0 - valLoss: 0.4375220239162445 - trainLoss: 0.4302530586719513\n",
      "cnt: 0 - valLoss: 0.43750521540641785 - trainLoss: 0.43023765087127686\n",
      "cnt: 0 - valLoss: 0.4374883472919464 - trainLoss: 0.4302222430706024\n",
      "cnt: 0 - valLoss: 0.43747177720069885 - trainLoss: 0.43020689487457275\n",
      "cnt: 0 - valLoss: 0.4374549686908722 - trainLoss: 0.4301915168762207\n",
      "cnt: 0 - valLoss: 0.4374382197856903 - trainLoss: 0.43017616868019104\n",
      "cnt: 0 - valLoss: 0.4374215006828308 - trainLoss: 0.4301607608795166\n",
      "cnt: 0 - valLoss: 0.437404990196228 - trainLoss: 0.43014535307884216\n",
      "cnt: 0 - valLoss: 0.43738827109336853 - trainLoss: 0.4301300644874573\n",
      "cnt: 0 - valLoss: 0.43737155199050903 - trainLoss: 0.43011465668678284\n",
      "cnt: 0 - valLoss: 0.43735507130622864 - trainLoss: 0.43009933829307556\n",
      "cnt: 0 - valLoss: 0.43733835220336914 - trainLoss: 0.4300840198993683\n",
      "cnt: 0 - valLoss: 0.4373216927051544 - trainLoss: 0.4300686717033386\n",
      "cnt: 0 - valLoss: 0.4373052418231964 - trainLoss: 0.43005338311195374\n",
      "cnt: 0 - valLoss: 0.4372886121273041 - trainLoss: 0.43003806471824646\n",
      "cnt: 0 - valLoss: 0.43727201223373413 - trainLoss: 0.4300227463245392\n",
      "cnt: 0 - valLoss: 0.4372555613517761 - trainLoss: 0.4300074577331543\n",
      "cnt: 0 - valLoss: 0.4372389018535614 - trainLoss: 0.4299921989440918\n",
      "cnt: 0 - valLoss: 0.43722230195999146 - trainLoss: 0.4299769401550293\n",
      "cnt: 0 - valLoss: 0.4372057020664215 - trainLoss: 0.429961621761322\n",
      "cnt: 0 - valLoss: 0.4371892809867859 - trainLoss: 0.42994630336761475\n",
      "cnt: 0 - valLoss: 0.4371727705001831 - trainLoss: 0.42993104457855225\n",
      "cnt: 0 - valLoss: 0.43715620040893555 - trainLoss: 0.42991578578948975\n",
      "cnt: 0 - valLoss: 0.4371398389339447 - trainLoss: 0.42990052700042725\n",
      "cnt: 0 - valLoss: 0.4371238648891449 - trainLoss: 0.42988526821136475\n",
      "cnt: 0 - valLoss: 0.43710795044898987 - trainLoss: 0.4298698306083679\n",
      "cnt: 0 - valLoss: 0.4370920658111572 - trainLoss: 0.4298543632030487\n",
      "cnt: 0 - valLoss: 0.4370763301849365 - trainLoss: 0.4298389256000519\n",
      "cnt: 0 - valLoss: 0.43706053495407104 - trainLoss: 0.42982351779937744\n",
      "cnt: 0 - valLoss: 0.4370446503162384 - trainLoss: 0.42980799078941345\n",
      "cnt: 0 - valLoss: 0.43702882528305054 - trainLoss: 0.4297926127910614\n",
      "cnt: 0 - valLoss: 0.437013179063797 - trainLoss: 0.42977720499038696\n",
      "cnt: 0 - valLoss: 0.43699729442596436 - trainLoss: 0.42976173758506775\n",
      "cnt: 0 - valLoss: 0.4369814097881317 - trainLoss: 0.4297463893890381\n",
      "cnt: 0 - valLoss: 0.43696555495262146 - trainLoss: 0.4297310411930084\n",
      "cnt: 0 - valLoss: 0.43694987893104553 - trainLoss: 0.42971566319465637\n",
      "cnt: 0 - valLoss: 0.43693411350250244 - trainLoss: 0.4297003149986267\n",
      "cnt: 0 - valLoss: 0.4369184076786041 - trainLoss: 0.42968496680259705\n",
      "cnt: 0 - valLoss: 0.43690258264541626 - trainLoss: 0.429669588804245\n",
      "cnt: 0 - valLoss: 0.4368869960308075 - trainLoss: 0.4296543002128601\n",
      "cnt: 0 - valLoss: 0.436871200799942 - trainLoss: 0.42963898181915283\n",
      "cnt: 0 - valLoss: 0.43685540556907654 - trainLoss: 0.42962366342544556\n",
      "cnt: 0 - valLoss: 0.436839759349823 - trainLoss: 0.42960837483406067\n",
      "cnt: 0 - valLoss: 0.4368238151073456 - trainLoss: 0.4295930564403534\n",
      "cnt: 0 - valLoss: 0.43680813908576965 - trainLoss: 0.4295777678489685\n",
      "cnt: 0 - valLoss: 0.436792254447937 - trainLoss: 0.429562509059906\n",
      "cnt: 0 - valLoss: 0.43677663803100586 - trainLoss: 0.42954719066619873\n",
      "cnt: 0 - valLoss: 0.4367609918117523 - trainLoss: 0.42953187227249146\n",
      "cnt: 0 - valLoss: 0.43674519658088684 - trainLoss: 0.4295165240764618\n",
      "cnt: 0 - valLoss: 0.4367295801639557 - trainLoss: 0.42950117588043213\n",
      "cnt: 0 - valLoss: 0.4367138743400574 - trainLoss: 0.4294857680797577\n",
      "cnt: 0 - valLoss: 0.4366982579231262 - trainLoss: 0.4294704496860504\n",
      "cnt: 0 - valLoss: 0.4366825520992279 - trainLoss: 0.42945510149002075\n",
      "cnt: 0 - valLoss: 0.4366668462753296 - trainLoss: 0.42943984270095825\n",
      "cnt: 0 - valLoss: 0.43665122985839844 - trainLoss: 0.42942458391189575\n",
      "cnt: 0 - valLoss: 0.4366355538368225 - trainLoss: 0.4294092655181885\n",
      "cnt: 0 - valLoss: 0.4366200268268585 - trainLoss: 0.429394006729126\n",
      "cnt: 0 - valLoss: 0.43660441040992737 - trainLoss: 0.4293787181377411\n",
      "cnt: 0 - valLoss: 0.4365888237953186 - trainLoss: 0.4293634593486786\n",
      "cnt: 0 - valLoss: 0.43657320737838745 - trainLoss: 0.4293482005596161\n",
      "cnt: 0 - valLoss: 0.43655773997306824 - trainLoss: 0.429332971572876\n",
      "cnt: 0 - valLoss: 0.4365421235561371 - trainLoss: 0.4293177127838135\n",
      "cnt: 0 - valLoss: 0.4365265667438507 - trainLoss: 0.42930251359939575\n",
      "cnt: 0 - valLoss: 0.4365110397338867 - trainLoss: 0.42928725481033325\n",
      "cnt: 0 - valLoss: 0.43649548292160034 - trainLoss: 0.42927199602127075\n",
      "cnt: 0 - valLoss: 0.4364800751209259 - trainLoss: 0.4292568266391754\n",
      "cnt: 0 - valLoss: 0.4364645779132843 - trainLoss: 0.4292415678501129\n",
      "cnt: 0 - valLoss: 0.4364491105079651 - trainLoss: 0.4292263984680176\n",
      "cnt: 0 - valLoss: 0.4364336133003235 - trainLoss: 0.42921119928359985\n",
      "cnt: 0 - valLoss: 0.4364180862903595 - trainLoss: 0.4291960299015045\n",
      "cnt: 0 - valLoss: 0.43640249967575073 - trainLoss: 0.42918089032173157\n",
      "cnt: 0 - valLoss: 0.43638697266578674 - trainLoss: 0.42916572093963623\n",
      "cnt: 0 - valLoss: 0.43637144565582275 - trainLoss: 0.42915061116218567\n",
      "cnt: 0 - valLoss: 0.43635594844818115 - trainLoss: 0.42913544178009033\n",
      "cnt: 0 - valLoss: 0.43634045124053955 - trainLoss: 0.429120272397995\n",
      "cnt: 0 - valLoss: 0.43632495403289795 - trainLoss: 0.4291052222251892\n",
      "cnt: 0 - valLoss: 0.43630942702293396 - trainLoss: 0.42909011244773865\n",
      "cnt: 0 - valLoss: 0.43629390001296997 - trainLoss: 0.4290750026702881\n",
      "cnt: 0 - valLoss: 0.43627825379371643 - trainLoss: 0.4290598928928375\n",
      "cnt: 0 - valLoss: 0.43626272678375244 - trainLoss: 0.42904478311538696\n",
      "cnt: 0 - valLoss: 0.43624716997146606 - trainLoss: 0.42902976274490356\n",
      "cnt: 0 - valLoss: 0.43623170256614685 - trainLoss: 0.429014652967453\n",
      "cnt: 0 - valLoss: 0.4362161457538605 - trainLoss: 0.42899957299232483\n",
      "cnt: 0 - valLoss: 0.4362005889415741 - trainLoss: 0.42898452281951904\n",
      "cnt: 0 - valLoss: 0.4361851215362549 - trainLoss: 0.42896944284439087\n",
      "cnt: 0 - valLoss: 0.4361696243286133 - trainLoss: 0.4289543926715851\n",
      "cnt: 0 - valLoss: 0.43615415692329407 - trainLoss: 0.4289393424987793\n",
      "cnt: 0 - valLoss: 0.43613865971565247 - trainLoss: 0.4289243221282959\n",
      "cnt: 0 - valLoss: 0.43612316250801086 - trainLoss: 0.4289093017578125\n",
      "cnt: 0 - valLoss: 0.4361078441143036 - trainLoss: 0.42889419198036194\n",
      "cnt: 0 - valLoss: 0.4360923767089844 - trainLoss: 0.4288790822029114\n",
      "cnt: 0 - valLoss: 0.4360770285129547 - trainLoss: 0.428864061832428\n",
      "cnt: 0 - valLoss: 0.4360615909099579 - trainLoss: 0.4288490414619446\n",
      "cnt: 0 - valLoss: 0.43604588508605957 - trainLoss: 0.4288339912891388\n",
      "cnt: 0 - valLoss: 0.4360303580760956 - trainLoss: 0.4288189709186554\n",
      "cnt: 0 - valLoss: 0.43601465225219727 - trainLoss: 0.428803950548172\n",
      "cnt: 0 - valLoss: 0.4359991252422333 - trainLoss: 0.4287889897823334\n",
      "cnt: 0 - valLoss: 0.4359835386276245 - trainLoss: 0.42877396941185\n",
      "cnt: 0 - valLoss: 0.4359681010246277 - trainLoss: 0.42875900864601135\n",
      "cnt: 0 - valLoss: 0.43595245480537415 - trainLoss: 0.42874404788017273\n",
      "cnt: 0 - valLoss: 0.4359370768070221 - trainLoss: 0.42872902750968933\n",
      "cnt: 0 - valLoss: 0.4359215199947357 - trainLoss: 0.4287140667438507\n",
      "cnt: 0 - valLoss: 0.43590617179870605 - trainLoss: 0.42869916558265686\n",
      "cnt: 0 - valLoss: 0.4358905553817749 - trainLoss: 0.4286842346191406\n",
      "cnt: 0 - valLoss: 0.4358752369880676 - trainLoss: 0.428669273853302\n",
      "cnt: 0 - valLoss: 0.435859352350235 - trainLoss: 0.42865434288978577\n",
      "cnt: 0 - valLoss: 0.43584370613098145 - trainLoss: 0.4286394417285919\n",
      "cnt: 0 - valLoss: 0.4358278512954712 - trainLoss: 0.4286244809627533\n",
      "cnt: 0 - valLoss: 0.4358122646808624 - trainLoss: 0.42860960960388184\n",
      "cnt: 0 - valLoss: 0.43579646944999695 - trainLoss: 0.4285947382450104\n",
      "cnt: 0 - valLoss: 0.43578073382377625 - trainLoss: 0.42857980728149414\n",
      "cnt: 0 - valLoss: 0.43576499819755554 - trainLoss: 0.42856496572494507\n",
      "cnt: 0 - valLoss: 0.435749351978302 - trainLoss: 0.42855003476142883\n",
      "cnt: 0 - valLoss: 0.43573352694511414 - trainLoss: 0.428535133600235\n",
      "cnt: 0 - valLoss: 0.4357178509235382 - trainLoss: 0.42852020263671875\n",
      "cnt: 0 - valLoss: 0.43570220470428467 - trainLoss: 0.4285052716732025\n",
      "cnt: 0 - valLoss: 0.4356863796710968 - trainLoss: 0.42849037051200867\n",
      "cnt: 0 - valLoss: 0.43567079305648804 - trainLoss: 0.42847543954849243\n",
      "cnt: 0 - valLoss: 0.43565502762794495 - trainLoss: 0.4284605383872986\n",
      "cnt: 0 - valLoss: 0.4356394112110138 - trainLoss: 0.42844560742378235\n",
      "cnt: 0 - valLoss: 0.4356236755847931 - trainLoss: 0.4284307658672333\n",
      "cnt: 0 - valLoss: 0.4356080889701843 - trainLoss: 0.42841583490371704\n",
      "cnt: 0 - valLoss: 0.43559229373931885 - trainLoss: 0.4284009635448456\n",
      "cnt: 0 - valLoss: 0.435576856136322 - trainLoss: 0.4283860921859741\n",
      "cnt: 0 - valLoss: 0.4355611503124237 - trainLoss: 0.42837122082710266\n",
      "cnt: 0 - valLoss: 0.4355454444885254 - trainLoss: 0.4283563792705536\n",
      "cnt: 0 - valLoss: 0.4355299472808838 - trainLoss: 0.4283415377140045\n",
      "cnt: 0 - valLoss: 0.435514360666275 - trainLoss: 0.42832666635513306\n",
      "cnt: 0 - valLoss: 0.4354988634586334 - trainLoss: 0.42831185460090637\n",
      "cnt: 0 - valLoss: 0.4354832172393799 - trainLoss: 0.4282969832420349\n",
      "cnt: 0 - valLoss: 0.43546780943870544 - trainLoss: 0.428282231092453\n",
      "cnt: 0 - valLoss: 0.4354521930217743 - trainLoss: 0.42826735973358154\n",
      "cnt: 0 - valLoss: 0.43543657660484314 - trainLoss: 0.42825257778167725\n",
      "cnt: 0 - valLoss: 0.4354213774204254 - trainLoss: 0.42823776602745056\n",
      "cnt: 0 - valLoss: 0.4354059398174286 - trainLoss: 0.4282228946685791\n",
      "cnt: 0 - valLoss: 0.4353906214237213 - trainLoss: 0.4282080829143524\n",
      "cnt: 0 - valLoss: 0.43537530303001404 - trainLoss: 0.42819327116012573\n",
      "cnt: 0 - valLoss: 0.43535974621772766 - trainLoss: 0.42817845940589905\n",
      "cnt: 0 - valLoss: 0.4353445768356323 - trainLoss: 0.42816361784935\n",
      "cnt: 0 - valLoss: 0.43532925844192505 - trainLoss: 0.4281488060951233\n",
      "cnt: 0 - valLoss: 0.4353139102458954 - trainLoss: 0.4281339943408966\n",
      "cnt: 0 - valLoss: 0.4352985918521881 - trainLoss: 0.4281192719936371\n",
      "cnt: 0 - valLoss: 0.435283362865448 - trainLoss: 0.4281044006347656\n",
      "cnt: 0 - valLoss: 0.4352680444717407 - trainLoss: 0.4280896782875061\n",
      "cnt: 0 - valLoss: 0.4352527856826782 - trainLoss: 0.4280748665332794\n",
      "cnt: 0 - valLoss: 0.43523719906806946 - trainLoss: 0.4280600845813751\n",
      "cnt: 0 - valLoss: 0.4352220594882965 - trainLoss: 0.4280453324317932\n",
      "cnt: 0 - valLoss: 0.43520650267601013 - trainLoss: 0.4280305504798889\n",
      "cnt: 0 - valLoss: 0.4351913034915924 - trainLoss: 0.4280157685279846\n",
      "cnt: 0 - valLoss: 0.43517589569091797 - trainLoss: 0.4280010759830475\n",
      "cnt: 0 - valLoss: 0.43516090512275696 - trainLoss: 0.4279862344264984\n",
      "cnt: 0 - valLoss: 0.4351455867290497 - trainLoss: 0.4279714822769165\n",
      "cnt: 0 - valLoss: 0.4351305663585663 - trainLoss: 0.4279567003250122\n",
      "cnt: 0 - valLoss: 0.43511518836021423 - trainLoss: 0.4279418885707855\n",
      "cnt: 0 - valLoss: 0.43510016798973083 - trainLoss: 0.42792707681655884\n",
      "cnt: 0 - valLoss: 0.43508481979370117 - trainLoss: 0.42791223526000977\n",
      "cnt: 0 - valLoss: 0.4350697994232178 - trainLoss: 0.42789748311042786\n",
      "cnt: 0 - valLoss: 0.4350544214248657 - trainLoss: 0.42788267135620117\n",
      "cnt: 0 - valLoss: 0.4350394010543823 - trainLoss: 0.4278678894042969\n",
      "cnt: 0 - valLoss: 0.43502411246299744 - trainLoss: 0.42785316705703735\n",
      "cnt: 0 - valLoss: 0.43500933051109314 - trainLoss: 0.42783835530281067\n",
      "cnt: 0 - valLoss: 0.43499407172203064 - trainLoss: 0.4278234839439392\n",
      "cnt: 0 - valLoss: 0.4349791407585144 - trainLoss: 0.42780861258506775\n",
      "cnt: 0 - valLoss: 0.4349645972251892 - trainLoss: 0.42779380083084106\n",
      "cnt: 0 - valLoss: 0.4349500238895416 - trainLoss: 0.4277789294719696\n",
      "cnt: 0 - valLoss: 0.4349355399608612 - trainLoss: 0.42776405811309814\n",
      "cnt: 0 - valLoss: 0.434920996427536 - trainLoss: 0.4277491569519043\n",
      "cnt: 0 - valLoss: 0.4349064826965332 - trainLoss: 0.4277343153953552\n",
      "cnt: 0 - valLoss: 0.434891939163208 - trainLoss: 0.4277194142341614\n",
      "cnt: 0 - valLoss: 0.4348774254322052 - trainLoss: 0.4277046024799347\n",
      "cnt: 0 - valLoss: 0.43486297130584717 - trainLoss: 0.4276897609233856\n",
      "cnt: 0 - valLoss: 0.43484845757484436 - trainLoss: 0.42767491936683655\n",
      "cnt: 0 - valLoss: 0.43483400344848633 - trainLoss: 0.4276600778102875\n",
      "cnt: 0 - valLoss: 0.4348195195198059 - trainLoss: 0.427645206451416\n",
      "cnt: 0 - valLoss: 0.4348050355911255 - trainLoss: 0.42763039469718933\n",
      "cnt: 0 - valLoss: 0.4347905218601227 - trainLoss: 0.42761558294296265\n",
      "cnt: 0 - valLoss: 0.4347757399082184 - trainLoss: 0.42760077118873596\n",
      "cnt: 0 - valLoss: 0.4347606897354126 - trainLoss: 0.42758598923683167\n",
      "cnt: 0 - valLoss: 0.4347456395626068 - trainLoss: 0.4275713264942169\n",
      "cnt: 0 - valLoss: 0.4347306787967682 - trainLoss: 0.4275566041469574\n",
      "cnt: 0 - valLoss: 0.4347156584262848 - trainLoss: 0.42754194140434265\n",
      "cnt: 0 - valLoss: 0.4347008168697357 - trainLoss: 0.4275272488594055\n",
      "cnt: 0 - valLoss: 0.4346858263015747 - trainLoss: 0.427512526512146\n",
      "cnt: 0 - valLoss: 0.43467089533805847 - trainLoss: 0.42749786376953125\n",
      "cnt: 0 - valLoss: 0.43465593457221985 - trainLoss: 0.4274832308292389\n",
      "cnt: 0 - valLoss: 0.43464112281799316 - trainLoss: 0.42746856808662415\n",
      "cnt: 0 - valLoss: 0.43462613224983215 - trainLoss: 0.4274539351463318\n",
      "cnt: 0 - valLoss: 0.4346109926700592 - trainLoss: 0.4274393320083618\n",
      "cnt: 0 - valLoss: 0.43459609150886536 - trainLoss: 0.42742469906806946\n",
      "cnt: 0 - valLoss: 0.43458065390586853 - trainLoss: 0.4274101257324219\n",
      "cnt: 0 - valLoss: 0.43456509709358215 - trainLoss: 0.42739546298980713\n",
      "cnt: 0 - valLoss: 0.43454989790916443 - trainLoss: 0.42738088965415955\n",
      "cnt: 0 - valLoss: 0.4345344305038452 - trainLoss: 0.42736631631851196\n",
      "cnt: 0 - valLoss: 0.4345192611217499 - trainLoss: 0.4273517429828644\n",
      "cnt: 0 - valLoss: 0.43450379371643066 - trainLoss: 0.427337110042572\n",
      "cnt: 0 - valLoss: 0.4344886243343353 - trainLoss: 0.42732247710227966\n",
      "cnt: 0 - valLoss: 0.4344731569290161 - trainLoss: 0.42730796337127686\n",
      "cnt: 0 - valLoss: 0.43445777893066406 - trainLoss: 0.4272933900356293\n",
      "cnt: 0 - valLoss: 0.43444278836250305 - trainLoss: 0.4272787570953369\n",
      "cnt: 0 - valLoss: 0.43442749977111816 - trainLoss: 0.42726418375968933\n",
      "cnt: 0 - valLoss: 0.43441247940063477 - trainLoss: 0.4272496700286865\n",
      "cnt: 0 - valLoss: 0.4343971312046051 - trainLoss: 0.42723509669303894\n",
      "cnt: 0 - valLoss: 0.43438205122947693 - trainLoss: 0.42722055315971375\n",
      "cnt: 0 - valLoss: 0.43436673283576965 - trainLoss: 0.42720597982406616\n",
      "cnt: 0 - valLoss: 0.4343513250350952 - trainLoss: 0.4271914064884186\n",
      "cnt: 0 - valLoss: 0.43433627486228943 - trainLoss: 0.42717689275741577\n",
      "cnt: 0 - valLoss: 0.43432101607322693 - trainLoss: 0.42716237902641296\n",
      "cnt: 0 - valLoss: 0.4343060255050659 - trainLoss: 0.4271478056907654\n",
      "cnt: 0 - valLoss: 0.43429070711135864 - trainLoss: 0.42713332176208496\n",
      "cnt: 0 - valLoss: 0.43427541851997375 - trainLoss: 0.42711883783340454\n",
      "cnt: 0 - valLoss: 0.4342604875564575 - trainLoss: 0.4271043539047241\n",
      "cnt: 0 - valLoss: 0.43424519896507263 - trainLoss: 0.4270898401737213\n",
      "cnt: 0 - valLoss: 0.43423035740852356 - trainLoss: 0.4270753264427185\n",
      "cnt: 0 - valLoss: 0.43421512842178345 - trainLoss: 0.4270608425140381\n",
      "cnt: 0 - valLoss: 0.4341999590396881 - trainLoss: 0.42704635858535767\n",
      "cnt: 0 - valLoss: 0.43418505787849426 - trainLoss: 0.42703187465667725\n",
      "cnt: 0 - valLoss: 0.43416959047317505 - trainLoss: 0.4270174205303192\n",
      "cnt: 0 - valLoss: 0.43415459990501404 - trainLoss: 0.4270029664039612\n",
      "cnt: 0 - valLoss: 0.43413957953453064 - trainLoss: 0.42698854207992554\n",
      "cnt: 0 - valLoss: 0.43412452936172485 - trainLoss: 0.4269740879535675\n",
      "cnt: 0 - valLoss: 0.4341094195842743 - trainLoss: 0.42695966362953186\n",
      "cnt: 0 - valLoss: 0.4340943992137909 - trainLoss: 0.42694520950317383\n",
      "cnt: 0 - valLoss: 0.4340794086456299 - trainLoss: 0.4269307851791382\n",
      "cnt: 0 - valLoss: 0.4340648353099823 - trainLoss: 0.42691636085510254\n",
      "cnt: 0 - valLoss: 0.43404942750930786 - trainLoss: 0.4269019365310669\n",
      "cnt: 0 - valLoss: 0.43403470516204834 - trainLoss: 0.426887571811676\n",
      "cnt: 0 - valLoss: 0.43401920795440674 - trainLoss: 0.4268731474876404\n",
      "cnt: 0 - valLoss: 0.434004545211792 - trainLoss: 0.42685866355895996\n",
      "cnt: 0 - valLoss: 0.4339892864227295 - trainLoss: 0.42684417963027954\n",
      "cnt: 0 - valLoss: 0.4339745342731476 - trainLoss: 0.4268296957015991\n",
      "cnt: 0 - valLoss: 0.43395906686782837 - trainLoss: 0.4268152415752411\n",
      "cnt: 0 - valLoss: 0.4339444935321808 - trainLoss: 0.42680075764656067\n",
      "cnt: 0 - valLoss: 0.4339289665222168 - trainLoss: 0.426786333322525\n",
      "cnt: 0 - valLoss: 0.4339142441749573 - trainLoss: 0.4267719089984894\n",
      "cnt: 0 - valLoss: 0.4338989853858948 - trainLoss: 0.42675748467445374\n",
      "cnt: 0 - valLoss: 0.4338841736316681 - trainLoss: 0.4267430305480957\n",
      "cnt: 0 - valLoss: 0.4338687062263489 - trainLoss: 0.42672866582870483\n",
      "cnt: 0 - valLoss: 0.4338541328907013 - trainLoss: 0.4267142713069916\n",
      "cnt: 0 - valLoss: 0.4338386356830597 - trainLoss: 0.4266999661922455\n",
      "cnt: 0 - valLoss: 0.4338238835334778 - trainLoss: 0.426685631275177\n",
      "cnt: 0 - valLoss: 0.4338083863258362 - trainLoss: 0.4266712963581085\n",
      "cnt: 0 - valLoss: 0.433793842792511 - trainLoss: 0.42665696144104004\n",
      "cnt: 0 - valLoss: 0.4337783455848694 - trainLoss: 0.42664268612861633\n",
      "cnt: 0 - valLoss: 0.4337635934352875 - trainLoss: 0.42662835121154785\n",
      "cnt: 0 - valLoss: 0.433748334646225 - trainLoss: 0.42661401629447937\n",
      "cnt: 0 - valLoss: 0.43373385071754456 - trainLoss: 0.42659974098205566\n",
      "cnt: 0 - valLoss: 0.43371862173080444 - trainLoss: 0.4265853762626648\n",
      "cnt: 0 - valLoss: 0.4337041676044464 - trainLoss: 0.42657098174095154\n",
      "cnt: 0 - valLoss: 0.4336891770362854 - trainLoss: 0.42655670642852783\n",
      "cnt: 0 - valLoss: 0.43367457389831543 - trainLoss: 0.42654234170913696\n",
      "cnt: 0 - valLoss: 0.4336593747138977 - trainLoss: 0.4265279471874237\n",
      "cnt: 0 - valLoss: 0.4336450695991516 - trainLoss: 0.42651358246803284\n",
      "cnt: 0 - valLoss: 0.4336298406124115 - trainLoss: 0.4264991879463196\n",
      "cnt: 0 - valLoss: 0.43361538648605347 - trainLoss: 0.4264848530292511\n",
      "cnt: 0 - valLoss: 0.43360018730163574 - trainLoss: 0.426470547914505\n",
      "cnt: 0 - valLoss: 0.4335859417915344 - trainLoss: 0.42645615339279175\n",
      "cnt: 0 - valLoss: 0.4335707724094391 - trainLoss: 0.42644181847572327\n",
      "cnt: 0 - valLoss: 0.4335560202598572 - trainLoss: 0.4264275133609772\n",
      "cnt: 0 - valLoss: 0.4335413873195648 - trainLoss: 0.4264131188392639\n",
      "cnt: 0 - valLoss: 0.4335261285305023 - trainLoss: 0.4263988435268402\n",
      "cnt: 0 - valLoss: 0.4335116744041443 - trainLoss: 0.42638444900512695\n",
      "cnt: 0 - valLoss: 0.4334964454174042 - trainLoss: 0.4263700842857361\n",
      "cnt: 0 - valLoss: 0.43348219990730286 - trainLoss: 0.4263557493686676\n",
      "cnt: 0 - valLoss: 0.43346700072288513 - trainLoss: 0.42634132504463196\n",
      "cnt: 0 - valLoss: 0.4334526062011719 - trainLoss: 0.4263269603252411\n",
      "cnt: 0 - valLoss: 0.43343743681907654 - trainLoss: 0.42631256580352783\n",
      "cnt: 0 - valLoss: 0.43342286348342896 - trainLoss: 0.4262980818748474\n",
      "cnt: 0 - valLoss: 0.4334080219268799 - trainLoss: 0.42628371715545654\n",
      "cnt: 0 - valLoss: 0.433393269777298 - trainLoss: 0.4262692928314209\n",
      "cnt: 0 - valLoss: 0.43337830901145935 - trainLoss: 0.4262549579143524\n",
      "cnt: 0 - valLoss: 0.4333639442920685 - trainLoss: 0.42624059319496155\n",
      "cnt: 0 - valLoss: 0.4333488643169403 - trainLoss: 0.42622628808021545\n",
      "cnt: 0 - valLoss: 0.433334618806839 - trainLoss: 0.4262118339538574\n",
      "cnt: 0 - valLoss: 0.43331974744796753 - trainLoss: 0.42619743943214417\n",
      "cnt: 0 - valLoss: 0.4333050549030304 - trainLoss: 0.4261830151081085\n",
      "cnt: 0 - valLoss: 0.4332904517650604 - trainLoss: 0.4261685907840729\n",
      "cnt: 0 - valLoss: 0.433275431394577 - trainLoss: 0.4261541962623596\n",
      "cnt: 0 - valLoss: 0.43326136469841003 - trainLoss: 0.42613983154296875\n",
      "cnt: 0 - valLoss: 0.4332464337348938 - trainLoss: 0.4261254668235779\n",
      "cnt: 0 - valLoss: 0.43323221802711487 - trainLoss: 0.42611101269721985\n",
      "cnt: 0 - valLoss: 0.433217316865921 - trainLoss: 0.426096647977829\n",
      "cnt: 0 - valLoss: 0.43320292234420776 - trainLoss: 0.4260822832584381\n",
      "cnt: 0 - valLoss: 0.43318837881088257 - trainLoss: 0.42606788873672485\n",
      "cnt: 0 - valLoss: 0.4331735372543335 - trainLoss: 0.42605355381965637\n",
      "cnt: 0 - valLoss: 0.4331594407558441 - trainLoss: 0.4260391891002655\n",
      "cnt: 0 - valLoss: 0.43314483761787415 - trainLoss: 0.426024854183197\n",
      "cnt: 0 - valLoss: 0.43313080072402954 - trainLoss: 0.42601045966148376\n",
      "cnt: 0 - valLoss: 0.4331158995628357 - trainLoss: 0.42599615454673767\n",
      "cnt: 0 - valLoss: 0.43310147523880005 - trainLoss: 0.4259818494319916\n",
      "cnt: 0 - valLoss: 0.4330871105194092 - trainLoss: 0.42596757411956787\n",
      "cnt: 0 - valLoss: 0.4330727159976959 - trainLoss: 0.42595329880714417\n",
      "cnt: 0 - valLoss: 0.43305861949920654 - trainLoss: 0.42593902349472046\n",
      "cnt: 0 - valLoss: 0.43304380774497986 - trainLoss: 0.42592474818229675\n",
      "cnt: 0 - valLoss: 0.4330299198627472 - trainLoss: 0.42591050267219543\n",
      "cnt: 0 - valLoss: 0.4330151081085205 - trainLoss: 0.42589622735977173\n",
      "cnt: 0 - valLoss: 0.433000773191452 - trainLoss: 0.425881952047348\n",
      "cnt: 0 - valLoss: 0.43298670649528503 - trainLoss: 0.4258677065372467\n",
      "cnt: 0 - valLoss: 0.43297213315963745 - trainLoss: 0.4258534908294678\n",
      "cnt: 0 - valLoss: 0.43295809626579285 - trainLoss: 0.42583924531936646\n",
      "cnt: 0 - valLoss: 0.4329433739185333 - trainLoss: 0.42582497000694275\n",
      "cnt: 0 - valLoss: 0.43292903900146484 - trainLoss: 0.4258107841014862\n",
      "cnt: 0 - valLoss: 0.43291524052619934 - trainLoss: 0.42579659819602966\n",
      "cnt: 0 - valLoss: 0.43290066719055176 - trainLoss: 0.42578238248825073\n",
      "cnt: 0 - valLoss: 0.43288686871528625 - trainLoss: 0.42576804757118225\n",
      "cnt: 0 - valLoss: 0.43287229537963867 - trainLoss: 0.42575377225875854\n",
      "cnt: 0 - valLoss: 0.4328581988811493 - trainLoss: 0.42573949694633484\n",
      "cnt: 0 - valLoss: 0.4328445792198181 - trainLoss: 0.42572519183158875\n",
      "cnt: 0 - valLoss: 0.4328300654888153 - trainLoss: 0.4257109761238098\n",
      "cnt: 0 - valLoss: 0.4328159987926483 - trainLoss: 0.4256967306137085\n",
      "cnt: 0 - valLoss: 0.4328019618988037 - trainLoss: 0.42568251490592957\n",
      "cnt: 0 - valLoss: 0.4327881336212158 - trainLoss: 0.42566826939582825\n",
      "cnt: 0 - valLoss: 0.43277406692504883 - trainLoss: 0.4256540834903717\n",
      "cnt: 0 - valLoss: 0.4327600598335266 - trainLoss: 0.42563989758491516\n",
      "cnt: 0 - valLoss: 0.432746022939682 - trainLoss: 0.425625741481781\n",
      "cnt: 0 - valLoss: 0.43273210525512695 - trainLoss: 0.42561155557632446\n",
      "cnt: 0 - valLoss: 0.4327181577682495 - trainLoss: 0.4255973696708679\n",
      "cnt: 0 - valLoss: 0.43270376324653625 - trainLoss: 0.4255831837654114\n",
      "cnt: 0 - valLoss: 0.4326903223991394 - trainLoss: 0.42556899785995483\n",
      "cnt: 0 - valLoss: 0.4326761066913605 - trainLoss: 0.42555472254753113\n",
      "cnt: 0 - valLoss: 0.43266281485557556 - trainLoss: 0.4255404472351074\n",
      "cnt: 0 - valLoss: 0.432648628950119 - trainLoss: 0.4255262017250061\n",
      "cnt: 0 - valLoss: 0.43263524770736694 - trainLoss: 0.4255119860172272\n",
      "cnt: 0 - valLoss: 0.4326210021972656 - trainLoss: 0.4254976809024811\n",
      "cnt: 0 - valLoss: 0.4326072335243225 - trainLoss: 0.42548349499702454\n",
      "cnt: 0 - valLoss: 0.4325937032699585 - trainLoss: 0.4254693388938904\n",
      "cnt: 0 - valLoss: 0.43258026242256165 - trainLoss: 0.42545509338378906\n",
      "cnt: 0 - valLoss: 0.43256625533103943 - trainLoss: 0.4254409074783325\n",
      "cnt: 0 - valLoss: 0.4325525760650635 - trainLoss: 0.4254266917705536\n",
      "cnt: 0 - valLoss: 0.4325390160083771 - trainLoss: 0.42541250586509705\n",
      "cnt: 0 - valLoss: 0.4325253367424011 - trainLoss: 0.4253983199596405\n",
      "cnt: 0 - valLoss: 0.4325112998485565 - trainLoss: 0.4253840744495392\n",
      "cnt: 0 - valLoss: 0.4324980080127716 - trainLoss: 0.42536988854408264\n",
      "cnt: 0 - valLoss: 0.4324840009212494 - trainLoss: 0.4253556728363037\n",
      "cnt: 0 - valLoss: 0.4324709475040436 - trainLoss: 0.42534157633781433\n",
      "cnt: 0 - valLoss: 0.432456910610199 - trainLoss: 0.4253273904323578\n",
      "cnt: 0 - valLoss: 0.4324433505535126 - trainLoss: 0.42531320452690125\n",
      "cnt: 0 - valLoss: 0.43243011832237244 - trainLoss: 0.4252990782260895\n",
      "cnt: 0 - valLoss: 0.43241631984710693 - trainLoss: 0.4252849519252777\n",
      "cnt: 0 - valLoss: 0.43240272998809814 - trainLoss: 0.42527076601982117\n",
      "cnt: 0 - valLoss: 0.4323886036872864 - trainLoss: 0.4252566695213318\n",
      "cnt: 0 - valLoss: 0.4323752820491791 - trainLoss: 0.42524254322052\n",
      "cnt: 0 - valLoss: 0.4323611557483673 - trainLoss: 0.42522841691970825\n",
      "cnt: 0 - valLoss: 0.43234801292419434 - trainLoss: 0.4252142906188965\n",
      "cnt: 0 - valLoss: 0.4323338568210602 - trainLoss: 0.42520013451576233\n",
      "cnt: 0 - valLoss: 0.43232011795043945 - trainLoss: 0.4251860976219177\n",
      "cnt: 0 - valLoss: 0.4323064386844635 - trainLoss: 0.42517203092575073\n",
      "cnt: 0 - valLoss: 0.4322926998138428 - trainLoss: 0.42515799403190613\n",
      "cnt: 0 - valLoss: 0.43227916955947876 - trainLoss: 0.4251439571380615\n",
      "cnt: 0 - valLoss: 0.4322650730609894 - trainLoss: 0.4251299202442169\n",
      "cnt: 0 - valLoss: 0.4322516620159149 - trainLoss: 0.4251158833503723\n",
      "cnt: 0 - valLoss: 0.4322376549243927 - trainLoss: 0.4251018464565277\n",
      "cnt: 0 - valLoss: 0.43222424387931824 - trainLoss: 0.42508774995803833\n",
      "cnt: 0 - valLoss: 0.4322105348110199 - trainLoss: 0.42507368326187134\n",
      "cnt: 0 - valLoss: 0.4321969449520111 - trainLoss: 0.42505958676338196\n",
      "cnt: 0 - valLoss: 0.4321829080581665 - trainLoss: 0.42504560947418213\n",
      "cnt: 0 - valLoss: 0.4321696162223816 - trainLoss: 0.4250315725803375\n",
      "cnt: 0 - valLoss: 0.4321557283401489 - trainLoss: 0.4250176250934601\n",
      "cnt: 0 - valLoss: 0.4321419894695282 - trainLoss: 0.4250035881996155\n",
      "cnt: 0 - valLoss: 0.43212851881980896 - trainLoss: 0.4249895513057709\n",
      "cnt: 0 - valLoss: 0.43211421370506287 - trainLoss: 0.42497557401657104\n",
      "cnt: 0 - valLoss: 0.4321003258228302 - trainLoss: 0.42496156692504883\n",
      "cnt: 0 - valLoss: 0.43208664655685425 - trainLoss: 0.4249476492404938\n",
      "cnt: 0 - valLoss: 0.43207263946533203 - trainLoss: 0.42493370175361633\n",
      "cnt: 0 - valLoss: 0.43205857276916504 - trainLoss: 0.42491981387138367\n",
      "cnt: 0 - valLoss: 0.4320448338985443 - trainLoss: 0.4249058663845062\n",
      "cnt: 0 - valLoss: 0.43203017115592957 - trainLoss: 0.42489203810691833\n",
      "cnt: 0 - valLoss: 0.4320160150527954 - trainLoss: 0.42487823963165283\n",
      "cnt: 0 - valLoss: 0.43200233578681946 - trainLoss: 0.42486444115638733\n",
      "cnt: 0 - valLoss: 0.43198785185813904 - trainLoss: 0.4248507022857666\n",
      "cnt: 0 - valLoss: 0.43197375535964966 - trainLoss: 0.4248369038105011\n",
      "cnt: 0 - valLoss: 0.43196001648902893 - trainLoss: 0.424823135137558\n",
      "cnt: 0 - valLoss: 0.43194565176963806 - trainLoss: 0.42480939626693726\n",
      "cnt: 0 - valLoss: 0.43193161487579346 - trainLoss: 0.42479559779167175\n",
      "cnt: 0 - valLoss: 0.4319179952144623 - trainLoss: 0.424781858921051\n",
      "cnt: 0 - valLoss: 0.43190357089042664 - trainLoss: 0.4247681200504303\n",
      "cnt: 0 - valLoss: 0.4318896234035492 - trainLoss: 0.4247543513774872\n",
      "cnt: 0 - valLoss: 0.43187591433525085 - trainLoss: 0.42474061250686646\n",
      "cnt: 0 - valLoss: 0.4318621754646301 - trainLoss: 0.42472681403160095\n",
      "cnt: 0 - valLoss: 0.43184778094291687 - trainLoss: 0.4247130751609802\n",
      "cnt: 0 - valLoss: 0.431833952665329 - trainLoss: 0.4246993362903595\n",
      "cnt: 0 - valLoss: 0.43182042241096497 - trainLoss: 0.424685537815094\n",
      "cnt: 0 - valLoss: 0.4318060576915741 - trainLoss: 0.42467179894447327\n",
      "cnt: 0 - valLoss: 0.43179234862327576 - trainLoss: 0.4246581196784973\n",
      "cnt: 0 - valLoss: 0.4317788779735565 - trainLoss: 0.424644410610199\n",
      "cnt: 0 - valLoss: 0.43176448345184326 - trainLoss: 0.4246307909488678\n",
      "cnt: 0 - valLoss: 0.43175065517425537 - trainLoss: 0.42461708188056946\n",
      "cnt: 0 - valLoss: 0.4317367970943451 - trainLoss: 0.42460349202156067\n",
      "cnt: 0 - valLoss: 0.43172338604927063 - trainLoss: 0.42458978295326233\n",
      "cnt: 0 - valLoss: 0.43170928955078125 - trainLoss: 0.42457619309425354\n",
      "cnt: 0 - valLoss: 0.43169552087783813 - trainLoss: 0.4245625436306\n",
      "cnt: 0 - valLoss: 0.4316817820072174 - trainLoss: 0.4245488941669464\n",
      "cnt: 0 - valLoss: 0.43166789412498474 - trainLoss: 0.42453527450561523\n",
      "cnt: 0 - valLoss: 0.43165409564971924 - trainLoss: 0.42452162504196167\n",
      "cnt: 0 - valLoss: 0.4316403269767761 - trainLoss: 0.4245080351829529\n",
      "cnt: 0 - valLoss: 0.4316263794898987 - trainLoss: 0.4244944155216217\n",
      "cnt: 0 - valLoss: 0.431613028049469 - trainLoss: 0.4244808256626129\n",
      "cnt: 0 - valLoss: 0.43159884214401245 - trainLoss: 0.42446714639663696\n",
      "cnt: 0 - valLoss: 0.43158552050590515 - trainLoss: 0.42445358633995056\n",
      "cnt: 0 - valLoss: 0.4315717816352844 - trainLoss: 0.4244399964809418\n",
      "cnt: 0 - valLoss: 0.4315575659275055 - trainLoss: 0.42442643642425537\n",
      "cnt: 0 - valLoss: 0.4315439760684967 - trainLoss: 0.4244128465652466\n",
      "cnt: 0 - valLoss: 0.43153029680252075 - trainLoss: 0.42439931631088257\n",
      "cnt: 0 - valLoss: 0.4315168559551239 - trainLoss: 0.4243856966495514\n",
      "cnt: 0 - valLoss: 0.43150272965431213 - trainLoss: 0.4243721067905426\n",
      "cnt: 0 - valLoss: 0.431488960981369 - trainLoss: 0.42435845732688904\n",
      "cnt: 0 - valLoss: 0.4314747750759125 - trainLoss: 0.42434486746788025\n",
      "cnt: 0 - valLoss: 0.4314614534378052 - trainLoss: 0.42433127760887146\n",
      "cnt: 0 - valLoss: 0.43144798278808594 - trainLoss: 0.4243176281452179\n",
      "cnt: 0 - valLoss: 0.43143394589424133 - trainLoss: 0.4243040382862091\n",
      "cnt: 0 - valLoss: 0.43142053484916687 - trainLoss: 0.4242904782295227\n",
      "cnt: 0 - valLoss: 0.43140649795532227 - trainLoss: 0.4242769479751587\n",
      "cnt: 0 - valLoss: 0.431393027305603 - trainLoss: 0.4242633879184723\n",
      "cnt: 0 - valLoss: 0.43137896060943604 - trainLoss: 0.4242498576641083\n",
      "cnt: 0 - valLoss: 0.4313659071922302 - trainLoss: 0.4242362976074219\n",
      "cnt: 0 - valLoss: 0.4313519597053528 - trainLoss: 0.4242227375507355\n",
      "cnt: 0 - valLoss: 0.4313385784626007 - trainLoss: 0.42420926690101624\n",
      "cnt: 0 - valLoss: 0.43132469058036804 - trainLoss: 0.42419570684432983\n",
      "cnt: 0 - valLoss: 0.4313112497329712 - trainLoss: 0.42418211698532104\n",
      "cnt: 0 - valLoss: 0.43129751086235046 - trainLoss: 0.42416852712631226\n",
      "cnt: 0 - valLoss: 0.4312841296195984 - trainLoss: 0.4241548180580139\n",
      "cnt: 0 - valLoss: 0.43127021193504333 - trainLoss: 0.42414116859436035\n",
      "cnt: 0 - valLoss: 0.43125730752944946 - trainLoss: 0.4241275191307068\n",
      "cnt: 0 - valLoss: 0.4312435984611511 - trainLoss: 0.42411383986473083\n",
      "cnt: 0 - valLoss: 0.43123024702072144 - trainLoss: 0.42410019040107727\n",
      "cnt: 0 - valLoss: 0.4312165379524231 - trainLoss: 0.4240865409374237\n",
      "cnt: 0 - valLoss: 0.43120288848876953 - trainLoss: 0.42407289147377014\n",
      "cnt: 0 - valLoss: 0.43119004368782043 - trainLoss: 0.4240592420101166\n",
      "cnt: 0 - valLoss: 0.4311761260032654 - trainLoss: 0.4240456521511078\n",
      "cnt: 0 - valLoss: 0.43116289377212524 - trainLoss: 0.4240320026874542\n",
      "cnt: 0 - valLoss: 0.43114927411079407 - trainLoss: 0.42401841282844543\n",
      "cnt: 0 - valLoss: 0.4311358332633972 - trainLoss: 0.42400482296943665\n",
      "cnt: 0 - valLoss: 0.43112269043922424 - trainLoss: 0.42399126291275024\n",
      "cnt: 0 - valLoss: 0.4311089813709259 - trainLoss: 0.42397773265838623\n",
      "cnt: 0 - valLoss: 0.43109622597694397 - trainLoss: 0.42396414279937744\n",
      "cnt: 0 - valLoss: 0.43108242750167847 - trainLoss: 0.42395058274269104\n",
      "cnt: 0 - valLoss: 0.43106886744499207 - trainLoss: 0.4239370822906494\n",
      "cnt: 0 - valLoss: 0.43105563521385193 - trainLoss: 0.4239235520362854\n",
      "cnt: 0 - valLoss: 0.4310421347618103 - trainLoss: 0.4239100515842438\n",
      "cnt: 0 - valLoss: 0.4310285449028015 - trainLoss: 0.42389655113220215\n",
      "cnt: 0 - valLoss: 0.43101558089256287 - trainLoss: 0.4238831102848053\n",
      "cnt: 0 - valLoss: 0.4310019314289093 - trainLoss: 0.42386963963508606\n",
      "cnt: 0 - valLoss: 0.43098920583724976 - trainLoss: 0.42385613918304443\n",
      "cnt: 0 - valLoss: 0.4309754967689514 - trainLoss: 0.4238426983356476\n",
      "cnt: 0 - valLoss: 0.4309619069099426 - trainLoss: 0.4238292872905731\n",
      "cnt: 0 - valLoss: 0.43094876408576965 - trainLoss: 0.42381584644317627\n",
      "cnt: 0 - valLoss: 0.43093523383140564 - trainLoss: 0.4238024055957794\n",
      "cnt: 0 - valLoss: 0.4309217035770416 - trainLoss: 0.42378899455070496\n",
      "cnt: 0 - valLoss: 0.43090856075286865 - trainLoss: 0.4237756133079529\n",
      "cnt: 0 - valLoss: 0.43089523911476135 - trainLoss: 0.4237622618675232\n",
      "cnt: 0 - valLoss: 0.4308817386627197 - trainLoss: 0.42374885082244873\n",
      "cnt: 0 - valLoss: 0.4308689534664154 - trainLoss: 0.42373549938201904\n",
      "cnt: 0 - valLoss: 0.4308553636074066 - trainLoss: 0.42372220754623413\n",
      "cnt: 0 - valLoss: 0.4308418333530426 - trainLoss: 0.42370885610580444\n",
      "cnt: 0 - valLoss: 0.4308289885520935 - trainLoss: 0.42369553446769714\n",
      "cnt: 0 - valLoss: 0.43081560730934143 - trainLoss: 0.42368218302726746\n",
      "cnt: 0 - valLoss: 0.4308023452758789 - trainLoss: 0.42366883158683777\n",
      "cnt: 0 - valLoss: 0.4307894706726074 - trainLoss: 0.4236554801464081\n",
      "cnt: 0 - valLoss: 0.43077608942985535 - trainLoss: 0.4236421287059784\n",
      "cnt: 0 - valLoss: 0.4307628273963928 - trainLoss: 0.4236288368701935\n",
      "cnt: 0 - valLoss: 0.43075016140937805 - trainLoss: 0.4236154854297638\n",
      "cnt: 0 - valLoss: 0.43073683977127075 - trainLoss: 0.4236021637916565\n",
      "cnt: 0 - valLoss: 0.43072348833084106 - trainLoss: 0.4235888719558716\n",
      "cnt: 0 - valLoss: 0.43071097135543823 - trainLoss: 0.4235755205154419\n",
      "cnt: 0 - valLoss: 0.430697500705719 - trainLoss: 0.42356225848197937\n",
      "cnt: 0 - valLoss: 0.4306841194629669 - trainLoss: 0.4235489070415497\n",
      "cnt: 0 - valLoss: 0.430670827627182 - trainLoss: 0.42353567481040955\n",
      "cnt: 0 - valLoss: 0.43065810203552246 - trainLoss: 0.42352235317230225\n",
      "cnt: 0 - valLoss: 0.43064484000205994 - trainLoss: 0.4235091507434845\n",
      "cnt: 0 - valLoss: 0.43063169717788696 - trainLoss: 0.42349591851234436\n",
      "cnt: 0 - valLoss: 0.43061891198158264 - trainLoss: 0.4234826862812042\n",
      "cnt: 0 - valLoss: 0.4306056499481201 - trainLoss: 0.4234694838523865\n",
      "cnt: 0 - valLoss: 0.4305926263332367 - trainLoss: 0.4234562814235687\n",
      "cnt: 0 - valLoss: 0.43057993054389954 - trainLoss: 0.4234430491924286\n",
      "cnt: 0 - valLoss: 0.43056684732437134 - trainLoss: 0.42342981696128845\n",
      "cnt: 0 - valLoss: 0.4305537939071655 - trainLoss: 0.42341652512550354\n",
      "cnt: 0 - valLoss: 0.43054160475730896 - trainLoss: 0.42340323328971863\n",
      "cnt: 0 - valLoss: 0.4305284917354584 - trainLoss: 0.42338991165161133\n",
      "cnt: 0 - valLoss: 0.4305154085159302 - trainLoss: 0.4233767092227936\n",
      "cnt: 0 - valLoss: 0.43050238490104675 - trainLoss: 0.4233633577823639\n",
      "cnt: 0 - valLoss: 0.4304894208908081 - trainLoss: 0.42335012555122375\n",
      "cnt: 0 - valLoss: 0.43047672510147095 - trainLoss: 0.42333686351776123\n",
      "cnt: 0 - valLoss: 0.43046364188194275 - trainLoss: 0.4233236014842987\n",
      "cnt: 0 - valLoss: 0.43045055866241455 - trainLoss: 0.42331039905548096\n",
      "cnt: 0 - valLoss: 0.4304378926753998 - trainLoss: 0.42329713702201843\n",
      "cnt: 0 - valLoss: 0.43042483925819397 - trainLoss: 0.42328399419784546\n",
      "cnt: 0 - valLoss: 0.4304118752479553 - trainLoss: 0.4232707917690277\n",
      "cnt: 0 - valLoss: 0.4303990304470062 - trainLoss: 0.4232575297355652\n",
      "cnt: 0 - valLoss: 0.4303865134716034 - trainLoss: 0.4232443869113922\n",
      "cnt: 0 - valLoss: 0.43037357926368713 - trainLoss: 0.4232311248779297\n",
      "cnt: 0 - valLoss: 0.43036049604415894 - trainLoss: 0.42321786284446716\n",
      "cnt: 0 - valLoss: 0.43034788966178894 - trainLoss: 0.4232046604156494\n",
      "cnt: 0 - valLoss: 0.4303348958492279 - trainLoss: 0.42319151759147644\n",
      "cnt: 0 - valLoss: 0.4303218424320221 - trainLoss: 0.4231783151626587\n",
      "cnt: 0 - valLoss: 0.4303089380264282 - trainLoss: 0.42316511273384094\n",
      "cnt: 0 - valLoss: 0.430296391248703 - trainLoss: 0.4231519401073456\n",
      "cnt: 0 - valLoss: 0.43028342723846436 - trainLoss: 0.42313873767852783\n",
      "cnt: 0 - valLoss: 0.4302704632282257 - trainLoss: 0.4231255352497101\n",
      "cnt: 0 - valLoss: 0.430258184671402 - trainLoss: 0.4231124222278595\n",
      "cnt: 0 - valLoss: 0.43024516105651855 - trainLoss: 0.42309921979904175\n",
      "cnt: 0 - valLoss: 0.4302321970462799 - trainLoss: 0.423086017370224\n",
      "cnt: 0 - valLoss: 0.43021923303604126 - trainLoss: 0.42307281494140625\n",
      "cnt: 0 - valLoss: 0.4302067160606384 - trainLoss: 0.4230596125125885\n",
      "cnt: 0 - valLoss: 0.4301939904689789 - trainLoss: 0.42304641008377075\n",
      "cnt: 0 - valLoss: 0.4301811456680298 - trainLoss: 0.423033207654953\n",
      "cnt: 0 - valLoss: 0.43016815185546875 - trainLoss: 0.42302000522613525\n",
      "cnt: 0 - valLoss: 0.43015578389167786 - trainLoss: 0.4230068027973175\n",
      "cnt: 0 - valLoss: 0.4301428496837616 - trainLoss: 0.42299360036849976\n",
      "cnt: 0 - valLoss: 0.4301302433013916 - trainLoss: 0.4229804277420044\n",
      "cnt: 0 - valLoss: 0.4301173985004425 - trainLoss: 0.4229672849178314\n",
      "cnt: 0 - valLoss: 0.43010497093200684 - trainLoss: 0.42295411229133606\n",
      "cnt: 0 - valLoss: 0.4300922155380249 - trainLoss: 0.4229409694671631\n",
      "cnt: 0 - valLoss: 0.4300793707370758 - trainLoss: 0.4229278564453125\n",
      "cnt: 0 - valLoss: 0.4300665557384491 - trainLoss: 0.4229147136211395\n",
      "cnt: 0 - valLoss: 0.4300541281700134 - trainLoss: 0.42290160059928894\n",
      "cnt: 0 - valLoss: 0.43004119396209717 - trainLoss: 0.42288854718208313\n",
      "cnt: 0 - valLoss: 0.4300280511379242 - trainLoss: 0.42287543416023254\n",
      "cnt: 0 - valLoss: 0.4300151467323303 - trainLoss: 0.42286238074302673\n",
      "cnt: 0 - valLoss: 0.43000227212905884 - trainLoss: 0.42284926772117615\n",
      "cnt: 0 - valLoss: 0.42998993396759033 - trainLoss: 0.4228362441062927\n",
      "cnt: 0 - valLoss: 0.42997705936431885 - trainLoss: 0.4228231906890869\n",
      "cnt: 0 - valLoss: 0.429964154958725 - trainLoss: 0.4228101372718811\n",
      "cnt: 0 - valLoss: 0.429951012134552 - trainLoss: 0.4227970838546753\n",
      "cnt: 0 - valLoss: 0.4299386143684387 - trainLoss: 0.42278411984443665\n",
      "cnt: 0 - valLoss: 0.4299258887767792 - trainLoss: 0.4227711260318756\n",
      "cnt: 0 - valLoss: 0.42991310358047485 - trainLoss: 0.4227581024169922\n",
      "cnt: 0 - valLoss: 0.42990028858184814 - trainLoss: 0.42274513840675354\n",
      "cnt: 0 - valLoss: 0.4298875033855438 - trainLoss: 0.4227321445941925\n",
      "cnt: 0 - valLoss: 0.42987510561943054 - trainLoss: 0.4227190315723419\n",
      "cnt: 0 - valLoss: 0.42986243963241577 - trainLoss: 0.4227059781551361\n",
      "cnt: 0 - valLoss: 0.42984986305236816 - trainLoss: 0.4226929247379303\n",
      "cnt: 0 - valLoss: 0.4298371970653534 - trainLoss: 0.4226798415184021\n",
      "cnt: 0 - valLoss: 0.42982447147369385 - trainLoss: 0.4226667881011963\n",
      "cnt: 0 - valLoss: 0.42981231212615967 - trainLoss: 0.42265379428863525\n",
      "cnt: 0 - valLoss: 0.4297996163368225 - trainLoss: 0.42264068126678467\n",
      "cnt: 0 - valLoss: 0.4297873079776764 - trainLoss: 0.4226275682449341\n",
      "cnt: 0 - valLoss: 0.4297749996185303 - trainLoss: 0.42261457443237305\n",
      "cnt: 0 - valLoss: 0.4297625720500946 - trainLoss: 0.42260146141052246\n",
      "cnt: 0 - valLoss: 0.4297507107257843 - trainLoss: 0.42258843779563904\n",
      "cnt: 0 - valLoss: 0.4297381639480591 - trainLoss: 0.4225753843784332\n",
      "cnt: 0 - valLoss: 0.42972567677497864 - trainLoss: 0.4225623905658722\n",
      "cnt: 0 - valLoss: 0.42971348762512207 - trainLoss: 0.42254942655563354\n",
      "cnt: 0 - valLoss: 0.4297010898590088 - trainLoss: 0.4225364625453949\n",
      "cnt: 0 - valLoss: 0.42968910932540894 - trainLoss: 0.42252349853515625\n",
      "cnt: 0 - valLoss: 0.42967671155929565 - trainLoss: 0.4225105941295624\n",
      "cnt: 0 - valLoss: 0.4296640157699585 - trainLoss: 0.4224976897239685\n",
      "cnt: 0 - valLoss: 0.42965167760849 - trainLoss: 0.42248475551605225\n",
      "cnt: 0 - valLoss: 0.42964017391204834 - trainLoss: 0.4224718511104584\n",
      "cnt: 0 - valLoss: 0.4296284317970276 - trainLoss: 0.4224587082862854\n",
      "cnt: 0 - valLoss: 0.4296165108680725 - trainLoss: 0.4224455952644348\n",
      "cnt: 0 - valLoss: 0.42960458993911743 - trainLoss: 0.42243248224258423\n",
      "cnt: 0 - valLoss: 0.42959246039390564 - trainLoss: 0.4224194288253784\n",
      "cnt: 0 - valLoss: 0.42958056926727295 - trainLoss: 0.42240631580352783\n",
      "cnt: 0 - valLoss: 0.4295687973499298 - trainLoss: 0.42239320278167725\n",
      "cnt: 0 - valLoss: 0.4295569360256195 - trainLoss: 0.42238011956214905\n",
      "cnt: 0 - valLoss: 0.42954501509666443 - trainLoss: 0.42236703634262085\n",
      "cnt: 0 - valLoss: 0.429532915353775 - trainLoss: 0.42235395312309265\n",
      "cnt: 0 - valLoss: 0.42952102422714233 - trainLoss: 0.42234086990356445\n",
      "cnt: 0 - valLoss: 0.42950916290283203 - trainLoss: 0.42232778668403625\n",
      "cnt: 0 - valLoss: 0.42949753999710083 - trainLoss: 0.42231470346450806\n",
      "cnt: 0 - valLoss: 0.4294857084751129 - trainLoss: 0.42230165004730225\n",
      "cnt: 0 - valLoss: 0.42947351932525635 - trainLoss: 0.42228859663009644\n",
      "cnt: 0 - valLoss: 0.42946118116378784 - trainLoss: 0.4222755432128906\n",
      "cnt: 0 - valLoss: 0.42944908142089844 - trainLoss: 0.4222625195980072\n",
      "cnt: 0 - valLoss: 0.4294368624687195 - trainLoss: 0.422249436378479\n",
      "cnt: 0 - valLoss: 0.4294249713420868 - trainLoss: 0.4222364127635956\n",
      "cnt: 0 - valLoss: 0.4294127821922302 - trainLoss: 0.42222335934638977\n",
      "cnt: 0 - valLoss: 0.4294007122516632 - trainLoss: 0.42221030592918396\n",
      "cnt: 0 - valLoss: 0.42938822507858276 - trainLoss: 0.4221973419189453\n",
      "cnt: 0 - valLoss: 0.42937585711479187 - trainLoss: 0.4221843183040619\n",
      "cnt: 0 - valLoss: 0.4293636381626129 - trainLoss: 0.42217138409614563\n",
      "cnt: 0 - valLoss: 0.4293515086174011 - trainLoss: 0.422158420085907\n",
      "cnt: 0 - valLoss: 0.42933934926986694 - trainLoss: 0.42214539647102356\n",
      "cnt: 0 - valLoss: 0.4293268918991089 - trainLoss: 0.4221324324607849\n",
      "cnt: 0 - valLoss: 0.4293145537376404 - trainLoss: 0.42211946845054626\n",
      "cnt: 0 - valLoss: 0.42930248379707336 - trainLoss: 0.42210653424263\n",
      "cnt: 0 - valLoss: 0.42929020524024963 - trainLoss: 0.4220935106277466\n",
      "cnt: 0 - valLoss: 0.4292779266834259 - trainLoss: 0.42208054661750793\n",
      "cnt: 0 - valLoss: 0.42926567792892456 - trainLoss: 0.42206764221191406\n",
      "cnt: 0 - valLoss: 0.4292532205581665 - trainLoss: 0.4220547378063202\n",
      "cnt: 0 - valLoss: 0.4292408227920532 - trainLoss: 0.4220418334007263\n",
      "cnt: 0 - valLoss: 0.42922845482826233 - trainLoss: 0.42202889919281006\n",
      "cnt: 0 - valLoss: 0.4292159974575043 - trainLoss: 0.42201605439186096\n",
      "cnt: 0 - valLoss: 0.42920368909835815 - trainLoss: 0.4220031201839447\n",
      "cnt: 0 - valLoss: 0.42919132113456726 - trainLoss: 0.4219902753829956\n",
      "cnt: 0 - valLoss: 0.42917919158935547 - trainLoss: 0.4219774007797241\n",
      "cnt: 0 - valLoss: 0.4291670024394989 - trainLoss: 0.4219645857810974\n",
      "cnt: 0 - valLoss: 0.42915478348731995 - trainLoss: 0.4219517409801483\n",
      "cnt: 0 - valLoss: 0.4291425347328186 - trainLoss: 0.4219389259815216\n",
      "cnt: 0 - valLoss: 0.42913034558296204 - trainLoss: 0.4219260513782501\n",
      "cnt: 0 - valLoss: 0.4291180670261383 - trainLoss: 0.4219132363796234\n",
      "cnt: 0 - valLoss: 0.4291059374809265 - trainLoss: 0.4219004213809967\n",
      "cnt: 0 - valLoss: 0.42909398674964905 - trainLoss: 0.42188760638237\n",
      "cnt: 0 - valLoss: 0.42908182740211487 - trainLoss: 0.4218747615814209\n",
      "cnt: 0 - valLoss: 0.4290696978569031 - trainLoss: 0.4218619465827942\n",
      "cnt: 0 - valLoss: 0.4290573298931122 - trainLoss: 0.42184919118881226\n",
      "cnt: 0 - valLoss: 0.4290452003479004 - trainLoss: 0.42183637619018555\n",
      "cnt: 0 - valLoss: 0.42903298139572144 - trainLoss: 0.42182356119155884\n",
      "cnt: 0 - valLoss: 0.42902061343193054 - trainLoss: 0.42181074619293213\n",
      "cnt: 0 - valLoss: 0.42900821566581726 - trainLoss: 0.4217979311943054\n",
      "cnt: 0 - valLoss: 0.42899593710899353 - trainLoss: 0.4217851161956787\n",
      "cnt: 0 - valLoss: 0.42898353934288025 - trainLoss: 0.4217723309993744\n",
      "cnt: 0 - valLoss: 0.42897114157676697 - trainLoss: 0.42175954580307007\n",
      "cnt: 0 - valLoss: 0.42895883321762085 - trainLoss: 0.42174676060676575\n",
      "cnt: 0 - valLoss: 0.42894643545150757 - trainLoss: 0.42173394560813904\n",
      "cnt: 0 - valLoss: 0.42893391847610474 - trainLoss: 0.42172113060951233\n",
      "cnt: 0 - valLoss: 0.4289214313030243 - trainLoss: 0.4217083752155304\n",
      "cnt: 0 - valLoss: 0.4289085566997528 - trainLoss: 0.4216955602169037\n",
      "cnt: 0 - valLoss: 0.42889654636383057 - trainLoss: 0.42168280482292175\n",
      "cnt: 0 - valLoss: 0.4288844168186188 - trainLoss: 0.42166998982429504\n",
      "cnt: 0 - valLoss: 0.4288720488548279 - trainLoss: 0.42165711522102356\n",
      "cnt: 0 - valLoss: 0.4288598597049713 - trainLoss: 0.42164430022239685\n",
      "cnt: 0 - valLoss: 0.42884790897369385 - trainLoss: 0.42163148522377014\n",
      "cnt: 0 - valLoss: 0.4288359582424164 - trainLoss: 0.42161864042282104\n",
      "cnt: 0 - valLoss: 0.428823858499527 - trainLoss: 0.42160576581954956\n",
      "cnt: 0 - valLoss: 0.42881128191947937 - trainLoss: 0.42159292101860046\n",
      "cnt: 0 - valLoss: 0.4287993609905243 - trainLoss: 0.4215799868106842\n",
      "cnt: 0 - valLoss: 0.42878714203834534 - trainLoss: 0.42156708240509033\n",
      "cnt: 0 - valLoss: 0.42877495288848877 - trainLoss: 0.42155423760414124\n",
      "cnt: 0 - valLoss: 0.4287627339363098 - trainLoss: 0.42154136300086975\n",
      "cnt: 0 - valLoss: 0.42875033617019653 - trainLoss: 0.4215284585952759\n",
      "cnt: 0 - valLoss: 0.4287380874156952 - trainLoss: 0.4215155243873596\n",
      "cnt: 0 - valLoss: 0.42872560024261475 - trainLoss: 0.42150261998176575\n",
      "cnt: 0 - valLoss: 0.4287133812904358 - trainLoss: 0.4214896559715271\n",
      "cnt: 0 - valLoss: 0.42870092391967773 - trainLoss: 0.421476811170578\n",
      "cnt: 0 - valLoss: 0.42868855595588684 - trainLoss: 0.42146390676498413\n",
      "cnt: 0 - valLoss: 0.4286765456199646 - trainLoss: 0.42145097255706787\n",
      "cnt: 0 - valLoss: 0.42866432666778564 - trainLoss: 0.4214381277561188\n",
      "cnt: 0 - valLoss: 0.42865195870399475 - trainLoss: 0.4214251935482025\n",
      "cnt: 0 - valLoss: 0.42863959074020386 - trainLoss: 0.4214123487472534\n",
      "cnt: 0 - valLoss: 0.4286273717880249 - trainLoss: 0.42139944434165955\n",
      "cnt: 0 - valLoss: 0.4286149740219116 - trainLoss: 0.42138656973838806\n",
      "cnt: 0 - valLoss: 0.4286026656627655 - trainLoss: 0.4213736951351166\n",
      "cnt: 0 - valLoss: 0.42858999967575073 - trainLoss: 0.4213608503341675\n",
      "cnt: 0 - valLoss: 0.428577721118927 - trainLoss: 0.421347975730896\n",
      "cnt: 0 - valLoss: 0.42856547236442566 - trainLoss: 0.4213351607322693\n",
      "cnt: 0 - valLoss: 0.42855313420295715 - trainLoss: 0.4213223457336426\n",
      "cnt: 0 - valLoss: 0.42854082584381104 - trainLoss: 0.42130953073501587\n",
      "cnt: 0 - valLoss: 0.42852815985679626 - trainLoss: 0.42129674553871155\n",
      "cnt: 0 - valLoss: 0.42851588129997253 - trainLoss: 0.42128393054008484\n",
      "cnt: 0 - valLoss: 0.4285036325454712 - trainLoss: 0.42127111554145813\n",
      "cnt: 0 - valLoss: 0.4284912347793579 - trainLoss: 0.4212583005428314\n",
      "cnt: 0 - valLoss: 0.42847880721092224 - trainLoss: 0.4212454855442047\n",
      "cnt: 0 - valLoss: 0.42846643924713135 - trainLoss: 0.4212327003479004\n",
      "cnt: 0 - valLoss: 0.42845436930656433 - trainLoss: 0.42121994495391846\n",
      "cnt: 0 - valLoss: 0.42844194173812866 - trainLoss: 0.42120712995529175\n",
      "cnt: 0 - valLoss: 0.4284295439720154 - trainLoss: 0.42119431495666504\n",
      "cnt: 0 - valLoss: 0.4284173548221588 - trainLoss: 0.42118147015571594\n",
      "cnt: 0 - valLoss: 0.4284054636955261 - trainLoss: 0.42116865515708923\n",
      "cnt: 0 - valLoss: 0.42839333415031433 - trainLoss: 0.4211558401584625\n",
      "cnt: 0 - valLoss: 0.4283810257911682 - trainLoss: 0.42114296555519104\n",
      "cnt: 0 - valLoss: 0.42836886644363403 - trainLoss: 0.42113015055656433\n",
      "cnt: 0 - valLoss: 0.4283568859100342 - trainLoss: 0.4211173951625824\n",
      "cnt: 0 - valLoss: 0.4283446669578552 - trainLoss: 0.4211045801639557\n",
      "cnt: 0 - valLoss: 0.4283325970172882 - trainLoss: 0.421091765165329\n",
      "cnt: 0 - valLoss: 0.4283202588558197 - trainLoss: 0.42107900977134705\n",
      "cnt: 0 - valLoss: 0.42830824851989746 - trainLoss: 0.42106619477272034\n",
      "cnt: 0 - valLoss: 0.4282962679862976 - trainLoss: 0.421053409576416\n",
      "cnt: 0 - valLoss: 0.42828425765037537 - trainLoss: 0.4210405945777893\n",
      "cnt: 0 - valLoss: 0.42827194929122925 - trainLoss: 0.4210278391838074\n",
      "cnt: 0 - valLoss: 0.4282599091529846 - trainLoss: 0.42101502418518066\n",
      "cnt: 0 - valLoss: 0.42824795842170715 - trainLoss: 0.42100226879119873\n",
      "cnt: 0 - valLoss: 0.42823612689971924 - trainLoss: 0.4209895133972168\n",
      "cnt: 0 - valLoss: 0.4282238483428955 - trainLoss: 0.4209767282009125\n",
      "cnt: 0 - valLoss: 0.42821162939071655 - trainLoss: 0.42096397280693054\n",
      "cnt: 0 - valLoss: 0.4281996190547943 - trainLoss: 0.4209512174129486\n",
      "cnt: 0 - valLoss: 0.4281875789165497 - trainLoss: 0.4209384322166443\n",
      "cnt: 0 - valLoss: 0.42817533016204834 - trainLoss: 0.42092567682266235\n",
      "cnt: 0 - valLoss: 0.42816317081451416 - trainLoss: 0.4209129512310028\n",
      "cnt: 0 - valLoss: 0.4281511902809143 - trainLoss: 0.4209001958370209\n",
      "cnt: 0 - valLoss: 0.42813923954963684 - trainLoss: 0.42088747024536133\n",
      "cnt: 0 - valLoss: 0.42812690138816833 - trainLoss: 0.42087477445602417\n",
      "cnt: 0 - valLoss: 0.42811495065689087 - trainLoss: 0.42086198925971985\n",
      "cnt: 0 - valLoss: 0.4281029999256134 - trainLoss: 0.4208492934703827\n",
      "cnt: 0 - valLoss: 0.4280910789966583 - trainLoss: 0.42083656787872314\n",
      "cnt: 0 - valLoss: 0.4280787408351898 - trainLoss: 0.4208238422870636\n",
      "cnt: 0 - valLoss: 0.42806771397590637 - trainLoss: 0.42081108689308167\n",
      "cnt: 0 - valLoss: 0.4280565679073334 - trainLoss: 0.420798122882843\n",
      "cnt: 0 - valLoss: 0.42804527282714844 - trainLoss: 0.42078515887260437\n",
      "cnt: 0 - valLoss: 0.4280341863632202 - trainLoss: 0.42077216506004333\n",
      "cnt: 0 - valLoss: 0.42802315950393677 - trainLoss: 0.42075929045677185\n",
      "cnt: 0 - valLoss: 0.4280119240283966 - trainLoss: 0.42074644565582275\n",
      "cnt: 0 - valLoss: 0.4280012249946594 - trainLoss: 0.42073342204093933\n",
      "cnt: 0 - valLoss: 0.4279903769493103 - trainLoss: 0.42072027921676636\n",
      "cnt: 0 - valLoss: 0.4279794991016388 - trainLoss: 0.420707106590271\n",
      "cnt: 0 - valLoss: 0.42796868085861206 - trainLoss: 0.4206940233707428\n",
      "cnt: 0 - valLoss: 0.4279578924179077 - trainLoss: 0.4206809103488922\n",
      "cnt: 0 - valLoss: 0.42794671654701233 - trainLoss: 0.4206678569316864\n",
      "cnt: 0 - valLoss: 0.4279358983039856 - trainLoss: 0.4206547439098358\n",
      "cnt: 0 - valLoss: 0.42792510986328125 - trainLoss: 0.42064169049263\n",
      "cnt: 0 - valLoss: 0.4279140532016754 - trainLoss: 0.4206285774707794\n",
      "cnt: 0 - valLoss: 0.42790326476097107 - trainLoss: 0.42061546444892883\n",
      "cnt: 0 - valLoss: 0.4278925657272339 - trainLoss: 0.4206022620201111\n",
      "cnt: 0 - valLoss: 0.42788165807724 - trainLoss: 0.42058905959129333\n",
      "cnt: 0 - valLoss: 0.4278709292411804 - trainLoss: 0.4205758571624756\n",
      "cnt: 0 - valLoss: 0.42785999178886414 - trainLoss: 0.4205626845359802\n",
      "cnt: 0 - valLoss: 0.4278493821620941 - trainLoss: 0.4205494523048401\n",
      "cnt: 0 - valLoss: 0.4278385639190674 - trainLoss: 0.4205363690853119\n",
      "cnt: 0 - valLoss: 0.42782777547836304 - trainLoss: 0.4205233156681061\n",
      "cnt: 0 - valLoss: 0.42781656980514526 - trainLoss: 0.42051026225090027\n",
      "cnt: 0 - valLoss: 0.42780545353889465 - trainLoss: 0.42049726843833923\n",
      "cnt: 0 - valLoss: 0.42779451608657837 - trainLoss: 0.4204843044281006\n",
      "cnt: 0 - valLoss: 0.4277835190296173 - trainLoss: 0.42047134041786194\n",
      "cnt: 0 - valLoss: 0.42777329683303833 - trainLoss: 0.4204583168029785\n",
      "cnt: 0 - valLoss: 0.4277627170085907 - trainLoss: 0.4204450845718384\n",
      "cnt: 0 - valLoss: 0.4277522563934326 - trainLoss: 0.42043188214302063\n",
      "cnt: 0 - valLoss: 0.4277419447898865 - trainLoss: 0.42041870951652527\n",
      "cnt: 0 - valLoss: 0.42773178219795227 - trainLoss: 0.42040565609931946\n",
      "cnt: 0 - valLoss: 0.427721232175827 - trainLoss: 0.4203925132751465\n",
      "cnt: 0 - valLoss: 0.42770999670028687 - trainLoss: 0.42037948966026306\n",
      "cnt: 0 - valLoss: 0.42769917845726013 - trainLoss: 0.4203665256500244\n",
      "cnt: 0 - valLoss: 0.4276881515979767 - trainLoss: 0.4203535318374634\n",
      "cnt: 0 - valLoss: 0.42767688632011414 - trainLoss: 0.4203406572341919\n",
      "cnt: 0 - valLoss: 0.42766544222831726 - trainLoss: 0.4203278124332428\n",
      "cnt: 0 - valLoss: 0.427653968334198 - trainLoss: 0.42031508684158325\n",
      "cnt: 0 - valLoss: 0.42764270305633545 - trainLoss: 0.42030227184295654\n",
      "cnt: 0 - valLoss: 0.4276314377784729 - trainLoss: 0.4202895164489746\n",
      "cnt: 0 - valLoss: 0.4276202321052551 - trainLoss: 0.4202767312526703\n",
      "cnt: 0 - valLoss: 0.42760851979255676 - trainLoss: 0.42026403546333313\n",
      "cnt: 0 - valLoss: 0.4275970458984375 - trainLoss: 0.4202513098716736\n",
      "cnt: 0 - valLoss: 0.4275856018066406 - trainLoss: 0.4202386438846588\n",
      "cnt: 0 - valLoss: 0.4275740087032318 - trainLoss: 0.42022597789764404\n",
      "cnt: 0 - valLoss: 0.4275624454021454 - trainLoss: 0.42021337151527405\n",
      "cnt: 0 - valLoss: 0.42755064368247986 - trainLoss: 0.42020073533058167\n",
      "cnt: 0 - valLoss: 0.4275394082069397 - trainLoss: 0.42018812894821167\n",
      "cnt: 0 - valLoss: 0.4275279641151428 - trainLoss: 0.4201754927635193\n",
      "cnt: 0 - valLoss: 0.42751652002334595 - trainLoss: 0.42016294598579407\n",
      "cnt: 0 - valLoss: 0.4275050759315491 - trainLoss: 0.42015036940574646\n",
      "cnt: 0 - valLoss: 0.42749330401420593 - trainLoss: 0.4201377332210541\n",
      "cnt: 0 - valLoss: 0.4274817705154419 - trainLoss: 0.42012521624565125\n",
      "cnt: 0 - valLoss: 0.42747044563293457 - trainLoss: 0.4201126992702484\n",
      "cnt: 0 - valLoss: 0.42745909094810486 - trainLoss: 0.4201001524925232\n",
      "cnt: 0 - valLoss: 0.42744770646095276 - trainLoss: 0.420087605714798\n",
      "cnt: 0 - valLoss: 0.42743614315986633 - trainLoss: 0.42007508873939514\n",
      "cnt: 0 - valLoss: 0.4274247884750366 - trainLoss: 0.4200625717639923\n",
      "cnt: 0 - valLoss: 0.4274134635925293 - trainLoss: 0.4200499951839447\n",
      "cnt: 0 - valLoss: 0.4274021089076996 - trainLoss: 0.42003747820854187\n",
      "cnt: 0 - valLoss: 0.42739057540893555 - trainLoss: 0.42002496123313904\n",
      "cnt: 0 - valLoss: 0.42737874388694763 - trainLoss: 0.4200124442577362\n",
      "cnt: 0 - valLoss: 0.42736712098121643 - trainLoss: 0.41999998688697815\n",
      "cnt: 0 - valLoss: 0.42735546827316284 - trainLoss: 0.4199874997138977\n",
      "cnt: 0 - valLoss: 0.4273441731929779 - trainLoss: 0.41997507214546204\n",
      "cnt: 0 - valLoss: 0.42733240127563477 - trainLoss: 0.419962614774704\n",
      "cnt: 0 - valLoss: 0.42732101678848267 - trainLoss: 0.4199501574039459\n",
      "cnt: 0 - valLoss: 0.42730966210365295 - trainLoss: 0.41993772983551025\n",
      "cnt: 0 - valLoss: 0.42729833722114563 - trainLoss: 0.4199252426624298\n",
      "cnt: 0 - valLoss: 0.4272870421409607 - trainLoss: 0.41991281509399414\n",
      "cnt: 0 - valLoss: 0.42727547883987427 - trainLoss: 0.41990041732788086\n",
      "cnt: 0 - valLoss: 0.4272640645503998 - trainLoss: 0.4198879897594452\n",
      "cnt: 0 - valLoss: 0.4272526204586029 - trainLoss: 0.4198756217956543\n",
      "cnt: 0 - valLoss: 0.4272412061691284 - trainLoss: 0.41986319422721863\n",
      "cnt: 0 - valLoss: 0.42722952365875244 - trainLoss: 0.41985082626342773\n",
      "cnt: 0 - valLoss: 0.4272177517414093 - trainLoss: 0.41983839869499207\n",
      "cnt: 0 - valLoss: 0.42720621824264526 - trainLoss: 0.41982603073120117\n",
      "cnt: 0 - valLoss: 0.42719465494155884 - trainLoss: 0.4198136031627655\n",
      "cnt: 0 - valLoss: 0.42718306183815 - trainLoss: 0.419801265001297\n",
      "cnt: 0 - valLoss: 0.4271712899208069 - trainLoss: 0.41978880763053894\n",
      "cnt: 0 - valLoss: 0.42715975642204285 - trainLoss: 0.41977643966674805\n",
      "cnt: 0 - valLoss: 0.42714810371398926 - trainLoss: 0.4197640120983124\n",
      "cnt: 0 - valLoss: 0.4271364212036133 - trainLoss: 0.4197515845298767\n",
      "cnt: 0 - valLoss: 0.4271245300769806 - trainLoss: 0.4197392165660858\n",
      "cnt: 0 - valLoss: 0.4271129369735718 - trainLoss: 0.41972672939300537\n",
      "cnt: 0 - valLoss: 0.42710134387016296 - trainLoss: 0.41971442103385925\n",
      "cnt: 0 - valLoss: 0.42708978056907654 - trainLoss: 0.4197019934654236\n",
      "cnt: 0 - valLoss: 0.42707836627960205 - trainLoss: 0.4196896553039551\n",
      "cnt: 0 - valLoss: 0.42706650495529175 - trainLoss: 0.41967734694480896\n",
      "cnt: 0 - valLoss: 0.4270552098751068 - trainLoss: 0.41966497898101807\n",
      "cnt: 0 - valLoss: 0.42704376578330994 - trainLoss: 0.4196526110172272\n",
      "cnt: 0 - valLoss: 0.4270322322845459 - trainLoss: 0.41964027285575867\n",
      "cnt: 0 - valLoss: 0.4270205497741699 - trainLoss: 0.41962799429893494\n",
      "cnt: 0 - valLoss: 0.42700883746147156 - trainLoss: 0.4196157157421112\n",
      "cnt: 0 - valLoss: 0.4269973337650299 - trainLoss: 0.4196034371852875\n",
      "cnt: 0 - valLoss: 0.4269857704639435 - trainLoss: 0.41959115862846375\n",
      "cnt: 0 - valLoss: 0.42697402834892273 - trainLoss: 0.4195789396762848\n",
      "cnt: 0 - valLoss: 0.42696231603622437 - trainLoss: 0.41956663131713867\n",
      "cnt: 0 - valLoss: 0.42695075273513794 - trainLoss: 0.41955438256263733\n",
      "cnt: 0 - valLoss: 0.4269391894340515 - trainLoss: 0.4195421636104584\n",
      "cnt: 0 - valLoss: 0.42692747712135315 - trainLoss: 0.41952985525131226\n",
      "cnt: 0 - valLoss: 0.4269157648086548 - trainLoss: 0.4195176661014557\n",
      "cnt: 0 - valLoss: 0.4269042909145355 - trainLoss: 0.41950538754463196\n",
      "cnt: 0 - valLoss: 0.42689278721809387 - trainLoss: 0.419493168592453\n",
      "cnt: 0 - valLoss: 0.4268810749053955 - trainLoss: 0.41948094964027405\n",
      "cnt: 0 - valLoss: 0.42686939239501953 - trainLoss: 0.4194687008857727\n",
      "cnt: 0 - valLoss: 0.42685800790786743 - trainLoss: 0.41945648193359375\n",
      "cnt: 0 - valLoss: 0.4268465042114258 - trainLoss: 0.4194442629814148\n",
      "cnt: 0 - valLoss: 0.4268346130847931 - trainLoss: 0.41943201422691345\n",
      "cnt: 0 - valLoss: 0.42682313919067383 - trainLoss: 0.4194197952747345\n",
      "cnt: 0 - valLoss: 0.4268116354942322 - trainLoss: 0.4194076657295227\n",
      "cnt: 0 - valLoss: 0.4267999827861786 - trainLoss: 0.41939544677734375\n",
      "cnt: 0 - valLoss: 0.42678868770599365 - trainLoss: 0.4193832278251648\n",
      "cnt: 0 - valLoss: 0.42677709460258484 - trainLoss: 0.4193710386753082\n",
      "cnt: 0 - valLoss: 0.42676541209220886 - trainLoss: 0.41935887932777405\n",
      "cnt: 0 - valLoss: 0.42675408720970154 - trainLoss: 0.4193466305732727\n",
      "cnt: 0 - valLoss: 0.42674240469932556 - trainLoss: 0.41933441162109375\n",
      "cnt: 0 - valLoss: 0.4267307221889496 - trainLoss: 0.4193221926689148\n",
      "cnt: 0 - valLoss: 0.4267193675041199 - trainLoss: 0.41930994391441345\n",
      "cnt: 0 - valLoss: 0.4267077147960663 - trainLoss: 0.4192977845668793\n",
      "cnt: 0 - valLoss: 0.4266960620880127 - trainLoss: 0.4192855656147003\n",
      "cnt: 0 - valLoss: 0.4266848564147949 - trainLoss: 0.41927337646484375\n",
      "cnt: 0 - valLoss: 0.42667320370674133 - trainLoss: 0.4192611873149872\n",
      "cnt: 0 - valLoss: 0.42666158080101013 - trainLoss: 0.419249027967453\n",
      "cnt: 0 - valLoss: 0.4266502261161804 - trainLoss: 0.41923680901527405\n",
      "cnt: 0 - valLoss: 0.4266384243965149 - trainLoss: 0.41922467947006226\n",
      "cnt: 0 - valLoss: 0.4266270697116852 - trainLoss: 0.4192124307155609\n",
      "cnt: 0 - valLoss: 0.42661550641059875 - trainLoss: 0.41920021176338196\n",
      "cnt: 0 - valLoss: 0.42660391330718994 - trainLoss: 0.41918808221817017\n",
      "cnt: 0 - valLoss: 0.4265926480293274 - trainLoss: 0.4191758930683136\n",
      "cnt: 0 - valLoss: 0.42658108472824097 - trainLoss: 0.4191637635231018\n",
      "cnt: 0 - valLoss: 0.42656996846199036 - trainLoss: 0.41915163397789\n",
      "cnt: 0 - valLoss: 0.4265584647655487 - trainLoss: 0.41913947463035583\n",
      "cnt: 0 - valLoss: 0.4265468716621399 - trainLoss: 0.41912728548049927\n",
      "cnt: 0 - valLoss: 0.42653530836105347 - trainLoss: 0.4191151261329651\n",
      "cnt: 0 - valLoss: 0.4265240728855133 - trainLoss: 0.4191029369831085\n",
      "cnt: 0 - valLoss: 0.42651262879371643 - trainLoss: 0.41909074783325195\n",
      "cnt: 0 - valLoss: 0.42650106549263 - trainLoss: 0.41907861828804016\n",
      "cnt: 0 - valLoss: 0.42648962140083313 - trainLoss: 0.41906648874282837\n",
      "cnt: 0 - valLoss: 0.42647790908813477 - trainLoss: 0.4190543293952942\n",
      "cnt: 0 - valLoss: 0.4264664649963379 - trainLoss: 0.4190421402454376\n",
      "cnt: 0 - valLoss: 0.42645469307899475 - trainLoss: 0.41902992129325867\n",
      "cnt: 0 - valLoss: 0.42644309997558594 - trainLoss: 0.4190177917480469\n",
      "cnt: 0 - valLoss: 0.4264313876628876 - trainLoss: 0.4190056622028351\n",
      "cnt: 0 - valLoss: 0.42642003297805786 - trainLoss: 0.4189935326576233\n",
      "cnt: 0 - valLoss: 0.4264083504676819 - trainLoss: 0.4189814031124115\n",
      "cnt: 0 - valLoss: 0.4263972043991089 - trainLoss: 0.41896921396255493\n",
      "cnt: 0 - valLoss: 0.4263860285282135 - trainLoss: 0.41895708441734314\n",
      "cnt: 0 - valLoss: 0.42637479305267334 - trainLoss: 0.41894492506980896\n",
      "cnt: 0 - valLoss: 0.4263634979724884 - trainLoss: 0.41893279552459717\n",
      "cnt: 0 - valLoss: 0.4263525903224945 - trainLoss: 0.4189206063747406\n",
      "cnt: 0 - valLoss: 0.4263412356376648 - trainLoss: 0.4189084768295288\n",
      "cnt: 0 - valLoss: 0.42633000016212463 - trainLoss: 0.418896347284317\n",
      "cnt: 0 - valLoss: 0.4263187348842621 - trainLoss: 0.41888418793678284\n",
      "cnt: 0 - valLoss: 0.4263075888156891 - trainLoss: 0.41887205839157104\n",
      "cnt: 0 - valLoss: 0.4262961745262146 - trainLoss: 0.41885992884635925\n",
      "cnt: 0 - valLoss: 0.4262849688529968 - trainLoss: 0.41884782910346985\n",
      "cnt: 0 - valLoss: 0.42627376317977905 - trainLoss: 0.41883566975593567\n",
      "cnt: 0 - valLoss: 0.42626267671585083 - trainLoss: 0.41882357001304626\n",
      "cnt: 0 - valLoss: 0.42625129222869873 - trainLoss: 0.41881150007247925\n",
      "cnt: 0 - valLoss: 0.42624014616012573 - trainLoss: 0.41879937052726746\n",
      "cnt: 0 - valLoss: 0.42622897028923035 - trainLoss: 0.41878724098205566\n",
      "cnt: 0 - valLoss: 0.4262177050113678 - trainLoss: 0.41877517104148865\n",
      "cnt: 0 - valLoss: 0.4262067675590515 - trainLoss: 0.41876304149627686\n",
      "cnt: 0 - valLoss: 0.4261954128742218 - trainLoss: 0.41875097155570984\n",
      "cnt: 0 - valLoss: 0.4261842668056488 - trainLoss: 0.41873884201049805\n",
      "cnt: 0 - valLoss: 0.4261729419231415 - trainLoss: 0.41872674226760864\n",
      "cnt: 0 - valLoss: 0.42616209387779236 - trainLoss: 0.4187147319316864\n",
      "cnt: 0 - valLoss: 0.42615076899528503 - trainLoss: 0.4187026023864746\n",
      "cnt: 0 - valLoss: 0.4261394739151001 - trainLoss: 0.4186905026435852\n",
      "cnt: 0 - valLoss: 0.4261285960674286 - trainLoss: 0.41867849230766296\n",
      "cnt: 0 - valLoss: 0.42611709237098694 - trainLoss: 0.41866636276245117\n",
      "cnt: 0 - valLoss: 0.4261062443256378 - trainLoss: 0.41865426301956177\n",
      "cnt: 0 - valLoss: 0.42609480023384094 - trainLoss: 0.41864219307899475\n",
      "cnt: 0 - valLoss: 0.4260835349559784 - trainLoss: 0.41863006353378296\n",
      "cnt: 0 - valLoss: 0.4260726571083069 - trainLoss: 0.41861799359321594\n",
      "cnt: 0 - valLoss: 0.42606106400489807 - trainLoss: 0.4186058044433594\n",
      "cnt: 0 - valLoss: 0.4260501265525818 - trainLoss: 0.4185936748981476\n",
      "cnt: 0 - valLoss: 0.4260387420654297 - trainLoss: 0.418581485748291\n",
      "cnt: 0 - valLoss: 0.42602741718292236 - trainLoss: 0.4185693562030792\n",
      "cnt: 0 - valLoss: 0.4260164499282837 - trainLoss: 0.4185572862625122\n",
      "cnt: 0 - valLoss: 0.4260050654411316 - trainLoss: 0.4185451567173004\n",
      "cnt: 0 - valLoss: 0.42599374055862427 - trainLoss: 0.4185330271720886\n",
      "cnt: 0 - valLoss: 0.4259827733039856 - trainLoss: 0.41852089762687683\n",
      "cnt: 0 - valLoss: 0.4259711503982544 - trainLoss: 0.41850876808166504\n",
      "cnt: 0 - valLoss: 0.42596033215522766 - trainLoss: 0.418496698141098\n",
      "cnt: 0 - valLoss: 0.4259490668773651 - trainLoss: 0.41848456859588623\n",
      "cnt: 0 - valLoss: 0.42593780159950256 - trainLoss: 0.4184724986553192\n",
      "cnt: 0 - valLoss: 0.42592698335647583 - trainLoss: 0.4184604585170746\n",
      "cnt: 0 - valLoss: 0.4259154796600342 - trainLoss: 0.41844838857650757\n",
      "cnt: 0 - valLoss: 0.42590445280075073 - trainLoss: 0.41843628883361816\n",
      "cnt: 0 - valLoss: 0.4258934259414673 - trainLoss: 0.41842421889305115\n",
      "cnt: 0 - valLoss: 0.4258822202682495 - trainLoss: 0.4184121787548065\n",
      "cnt: 0 - valLoss: 0.42587125301361084 - trainLoss: 0.4184001088142395\n",
      "cnt: 0 - valLoss: 0.4258600175380707 - trainLoss: 0.4183880686759949\n",
      "cnt: 0 - valLoss: 0.42584899067878723 - trainLoss: 0.41837599873542786\n",
      "cnt: 0 - valLoss: 0.4258378744125366 - trainLoss: 0.4183639585971832\n",
      "cnt: 0 - valLoss: 0.4258269667625427 - trainLoss: 0.4183518886566162\n",
      "cnt: 0 - valLoss: 0.42581605911254883 - trainLoss: 0.4183398485183716\n",
      "cnt: 0 - valLoss: 0.42580491304397583 - trainLoss: 0.41832780838012695\n",
      "cnt: 0 - valLoss: 0.4257940351963043 - trainLoss: 0.4183157980442047\n",
      "cnt: 0 - valLoss: 0.4257829785346985 - trainLoss: 0.4183037579059601\n",
      "cnt: 0 - valLoss: 0.425772100687027 - trainLoss: 0.41829177737236023\n",
      "cnt: 0 - valLoss: 0.42576074600219727 - trainLoss: 0.4182797074317932\n",
      "cnt: 0 - valLoss: 0.42575010657310486 - trainLoss: 0.41826775670051575\n",
      "cnt: 0 - valLoss: 0.42573902010917664 - trainLoss: 0.4182557463645935\n",
      "cnt: 0 - valLoss: 0.4257279336452484 - trainLoss: 0.4182437062263489\n",
      "cnt: 0 - valLoss: 0.4257173240184784 - trainLoss: 0.418231725692749\n",
      "cnt: 0 - valLoss: 0.42570602893829346 - trainLoss: 0.41821974515914917\n",
      "cnt: 0 - valLoss: 0.4256952404975891 - trainLoss: 0.41820770502090454\n",
      "cnt: 0 - valLoss: 0.4256843328475952 - trainLoss: 0.4181957244873047\n",
      "cnt: 0 - valLoss: 0.42567330598831177 - trainLoss: 0.41818374395370483\n",
      "cnt: 0 - valLoss: 0.42566242814064026 - trainLoss: 0.4181717038154602\n",
      "cnt: 0 - valLoss: 0.4256514310836792 - trainLoss: 0.4181597828865051\n",
      "cnt: 0 - valLoss: 0.4256405830383301 - trainLoss: 0.4181478023529053\n",
      "cnt: 0 - valLoss: 0.42562955617904663 - trainLoss: 0.4181358218193054\n",
      "cnt: 0 - valLoss: 0.42561864852905273 - trainLoss: 0.41812387108802795\n",
      "cnt: 0 - valLoss: 0.42560723423957825 - trainLoss: 0.4181118905544281\n",
      "cnt: 0 - valLoss: 0.4255964159965515 - trainLoss: 0.41809991002082825\n",
      "cnt: 0 - valLoss: 0.4255850911140442 - trainLoss: 0.41808798909187317\n",
      "cnt: 0 - valLoss: 0.4255737364292145 - trainLoss: 0.4180760383605957\n",
      "cnt: 0 - valLoss: 0.4255625605583191 - trainLoss: 0.41806405782699585\n",
      "cnt: 0 - valLoss: 0.4255514442920685 - trainLoss: 0.41805213689804077\n",
      "cnt: 0 - valLoss: 0.42554035782814026 - trainLoss: 0.4180401563644409\n",
      "cnt: 0 - valLoss: 0.4255290627479553 - trainLoss: 0.41802820563316345\n",
      "cnt: 0 - valLoss: 0.4255180358886719 - trainLoss: 0.4180162847042084\n",
      "cnt: 0 - valLoss: 0.42550694942474365 - trainLoss: 0.4180043637752533\n",
      "cnt: 0 - valLoss: 0.42549583315849304 - trainLoss: 0.41799241304397583\n",
      "cnt: 0 - valLoss: 0.4254845976829529 - trainLoss: 0.41798052191734314\n",
      "cnt: 0 - valLoss: 0.42547330260276794 - trainLoss: 0.41796860098838806\n",
      "cnt: 0 - valLoss: 0.42546242475509644 - trainLoss: 0.417956680059433\n",
      "cnt: 0 - valLoss: 0.42545121908187866 - trainLoss: 0.4179447889328003\n",
      "cnt: 0 - valLoss: 0.4254399836063385 - trainLoss: 0.4179328382015228\n",
      "cnt: 0 - valLoss: 0.42542874813079834 - trainLoss: 0.4179209768772125\n",
      "cnt: 0 - valLoss: 0.4254177212715149 - trainLoss: 0.41790908575057983\n",
      "cnt: 0 - valLoss: 0.42540648579597473 - trainLoss: 0.41789716482162476\n",
      "cnt: 0 - valLoss: 0.425395667552948 - trainLoss: 0.41788530349731445\n",
      "cnt: 0 - valLoss: 0.425384521484375 - trainLoss: 0.41787344217300415\n",
      "cnt: 0 - valLoss: 0.42537373304367065 - trainLoss: 0.41786155104637146\n",
      "cnt: 0 - valLoss: 0.4253626763820648 - trainLoss: 0.41784965991973877\n",
      "cnt: 0 - valLoss: 0.42535167932510376 - trainLoss: 0.41783782839775085\n",
      "cnt: 0 - valLoss: 0.4253406822681427 - trainLoss: 0.41782593727111816\n",
      "cnt: 0 - valLoss: 0.42532965540885925 - trainLoss: 0.41781410574913025\n",
      "cnt: 0 - valLoss: 0.4253186881542206 - trainLoss: 0.41780227422714233\n",
      "cnt: 0 - valLoss: 0.42530784010887146 - trainLoss: 0.4177904427051544\n",
      "cnt: 0 - valLoss: 0.4252967834472656 - trainLoss: 0.4177786111831665\n",
      "cnt: 0 - valLoss: 0.4252857267856598 - trainLoss: 0.4177667796611786\n",
      "cnt: 0 - valLoss: 0.42527469992637634 - trainLoss: 0.41775497794151306\n",
      "cnt: 0 - valLoss: 0.4252636134624481 - trainLoss: 0.41774314641952515\n",
      "cnt: 0 - valLoss: 0.42525261640548706 - trainLoss: 0.4177313446998596\n",
      "cnt: 0 - valLoss: 0.425241619348526 - trainLoss: 0.4177195131778717\n",
      "cnt: 0 - valLoss: 0.42523038387298584 - trainLoss: 0.41770774126052856\n",
      "cnt: 0 - valLoss: 0.4252192974090576 - trainLoss: 0.41769593954086304\n",
      "cnt: 0 - valLoss: 0.42520803213119507 - trainLoss: 0.4176841676235199\n",
      "cnt: 0 - valLoss: 0.4251970052719116 - trainLoss: 0.41767236590385437\n",
      "cnt: 0 - valLoss: 0.42518600821495056 - trainLoss: 0.41766059398651123\n",
      "cnt: 0 - valLoss: 0.42517468333244324 - trainLoss: 0.4176487922668457\n",
      "cnt: 0 - valLoss: 0.4251633286476135 - trainLoss: 0.41763702034950256\n",
      "cnt: 0 - valLoss: 0.42515209317207336 - trainLoss: 0.4176252782344818\n",
      "cnt: 0 - valLoss: 0.42514094710350037 - trainLoss: 0.41761350631713867\n",
      "cnt: 0 - valLoss: 0.42512941360473633 - trainLoss: 0.4176017642021179\n",
      "cnt: 0 - valLoss: 0.42511844635009766 - trainLoss: 0.4175899624824524\n",
      "cnt: 0 - valLoss: 0.42510712146759033 - trainLoss: 0.41757822036743164\n",
      "cnt: 0 - valLoss: 0.42509615421295166 - trainLoss: 0.4175664484500885\n",
      "cnt: 0 - valLoss: 0.42508479952812195 - trainLoss: 0.41755470633506775\n",
      "cnt: 0 - valLoss: 0.42507344484329224 - trainLoss: 0.417542964220047\n",
      "cnt: 0 - valLoss: 0.4250628352165222 - trainLoss: 0.41753122210502625\n",
      "cnt: 0 - valLoss: 0.42505165934562683 - trainLoss: 0.41751953959465027\n",
      "cnt: 0 - valLoss: 0.4250408709049225 - trainLoss: 0.4175078570842743\n",
      "cnt: 0 - valLoss: 0.42502984404563904 - trainLoss: 0.4174961745738983\n",
      "cnt: 0 - valLoss: 0.42501887679100037 - trainLoss: 0.4174845218658447\n",
      "cnt: 0 - valLoss: 0.4250076711177826 - trainLoss: 0.4174728989601135\n",
      "cnt: 0 - valLoss: 0.4249965250492096 - trainLoss: 0.41746118664741516\n",
      "cnt: 0 - valLoss: 0.4249846637248993 - trainLoss: 0.41744956374168396\n",
      "cnt: 0 - valLoss: 0.4249725043773651 - trainLoss: 0.4174378216266632\n",
      "cnt: 0 - valLoss: 0.42496100068092346 - trainLoss: 0.41742607951164246\n",
      "cnt: 0 - valLoss: 0.42494967579841614 - trainLoss: 0.4174143970012665\n",
      "cnt: 0 - valLoss: 0.4249384105205536 - trainLoss: 0.4174027442932129\n",
      "cnt: 0 - valLoss: 0.42492738366127014 - trainLoss: 0.4173911213874817\n",
      "cnt: 0 - valLoss: 0.4249160885810852 - trainLoss: 0.4173794388771057\n",
      "cnt: 0 - valLoss: 0.42490527033805847 - trainLoss: 0.4173677861690521\n",
      "cnt: 0 - valLoss: 0.4248940348625183 - trainLoss: 0.4173562526702881\n",
      "cnt: 0 - valLoss: 0.4248828589916229 - trainLoss: 0.4173445999622345\n",
      "cnt: 0 - valLoss: 0.4248718321323395 - trainLoss: 0.4173330068588257\n",
      "cnt: 0 - valLoss: 0.42486050724983215 - trainLoss: 0.41732147336006165\n",
      "cnt: 0 - valLoss: 0.4248495101928711 - trainLoss: 0.41730988025665283\n",
      "cnt: 0 - valLoss: 0.4248383343219757 - trainLoss: 0.417298287153244\n",
      "cnt: 0 - valLoss: 0.42482733726501465 - trainLoss: 0.4172866940498352\n",
      "cnt: 0 - valLoss: 0.42481595277786255 - trainLoss: 0.41727516055107117\n",
      "cnt: 0 - valLoss: 0.4248049855232239 - trainLoss: 0.41726362705230713\n",
      "cnt: 0 - valLoss: 0.42479366064071655 - trainLoss: 0.4172520637512207\n",
      "cnt: 0 - valLoss: 0.42478275299072266 - trainLoss: 0.4172404706478119\n",
      "cnt: 0 - valLoss: 0.4247715473175049 - trainLoss: 0.41722893714904785\n",
      "cnt: 0 - valLoss: 0.4247606098651886 - trainLoss: 0.4172174632549286\n",
      "cnt: 0 - valLoss: 0.4247495234012604 - trainLoss: 0.41720589995384216\n",
      "cnt: 0 - valLoss: 0.4247385859489441 - trainLoss: 0.4171943664550781\n",
      "cnt: 0 - valLoss: 0.4247276186943054 - trainLoss: 0.4171828329563141\n",
      "cnt: 0 - valLoss: 0.42471644282341003 - trainLoss: 0.41717126965522766\n",
      "cnt: 0 - valLoss: 0.42470502853393555 - trainLoss: 0.4171597957611084\n",
      "cnt: 0 - valLoss: 0.42469438910484314 - trainLoss: 0.41714829206466675\n",
      "cnt: 0 - valLoss: 0.42468297481536865 - trainLoss: 0.4171367585659027\n",
      "cnt: 0 - valLoss: 0.42467209696769714 - trainLoss: 0.41712531447410583\n",
      "cnt: 0 - valLoss: 0.4246610999107361 - trainLoss: 0.4171137809753418\n",
      "cnt: 0 - valLoss: 0.4246499240398407 - trainLoss: 0.41710227727890015\n",
      "cnt: 0 - valLoss: 0.4246388375759125 - trainLoss: 0.41709083318710327\n",
      "cnt: 0 - valLoss: 0.4246276617050171 - trainLoss: 0.4170793890953064\n",
      "cnt: 0 - valLoss: 0.42461663484573364 - trainLoss: 0.41706788539886475\n",
      "cnt: 0 - valLoss: 0.4246056377887726 - trainLoss: 0.41705644130706787\n",
      "cnt: 0 - valLoss: 0.42459437251091003 - trainLoss: 0.417044997215271\n",
      "cnt: 0 - valLoss: 0.4245835542678833 - trainLoss: 0.4170336127281189\n",
      "cnt: 0 - valLoss: 0.4245724081993103 - trainLoss: 0.41702210903167725\n",
      "cnt: 0 - valLoss: 0.42456141114234924 - trainLoss: 0.41701072454452515\n",
      "cnt: 0 - valLoss: 0.4245504140853882 - trainLoss: 0.41699928045272827\n",
      "cnt: 0 - valLoss: 0.4245392084121704 - trainLoss: 0.4169878363609314\n",
      "cnt: 0 - valLoss: 0.4245282709598541 - trainLoss: 0.4169764518737793\n",
      "cnt: 0 - valLoss: 0.42451712489128113 - trainLoss: 0.4169650077819824\n",
      "cnt: 0 - valLoss: 0.42450612783432007 - trainLoss: 0.41695359349250793\n",
      "cnt: 0 - valLoss: 0.42449522018432617 - trainLoss: 0.41694220900535583\n",
      "cnt: 0 - valLoss: 0.4244840741157532 - trainLoss: 0.41693076491355896\n",
      "cnt: 0 - valLoss: 0.4244731366634369 - trainLoss: 0.41691938042640686\n",
      "cnt: 0 - valLoss: 0.4244620203971863 - trainLoss: 0.4169079661369324\n",
      "cnt: 0 - valLoss: 0.4244513213634491 - trainLoss: 0.4168965220451355\n",
      "cnt: 0 - valLoss: 0.42444026470184326 - trainLoss: 0.4168851673603058\n",
      "cnt: 0 - valLoss: 0.4244289994239807 - trainLoss: 0.4168737232685089\n",
      "cnt: 0 - valLoss: 0.42441844940185547 - trainLoss: 0.4168623983860016\n",
      "cnt: 0 - valLoss: 0.424407035112381 - trainLoss: 0.4168509840965271\n",
      "cnt: 0 - valLoss: 0.4243963062763214 - trainLoss: 0.416839599609375\n",
      "cnt: 0 - valLoss: 0.4243854284286499 - trainLoss: 0.4168282449245453\n",
      "cnt: 0 - valLoss: 0.4243740141391754 - trainLoss: 0.4168168604373932\n",
      "cnt: 0 - valLoss: 0.4243631660938263 - trainLoss: 0.4168054759502411\n",
      "cnt: 0 - valLoss: 0.42435216903686523 - trainLoss: 0.4167941212654114\n",
      "cnt: 0 - valLoss: 0.4243413805961609 - trainLoss: 0.4167827069759369\n",
      "cnt: 0 - valLoss: 0.42433011531829834 - trainLoss: 0.4167713224887848\n",
      "cnt: 0 - valLoss: 0.4243193566799164 - trainLoss: 0.4167599678039551\n",
      "cnt: 0 - valLoss: 0.42430880665779114 - trainLoss: 0.41674864292144775\n",
      "cnt: 0 - valLoss: 0.4242976903915405 - trainLoss: 0.4167371988296509\n",
      "cnt: 0 - valLoss: 0.42428696155548096 - trainLoss: 0.4167257845401764\n",
      "cnt: 0 - valLoss: 0.42427557706832886 - trainLoss: 0.4167144000530243\n",
      "cnt: 0 - valLoss: 0.4242652356624603 - trainLoss: 0.4167030155658722\n",
      "cnt: 0 - valLoss: 0.42425402998924255 - trainLoss: 0.4166916012763977\n",
      "cnt: 0 - valLoss: 0.42424318194389343 - trainLoss: 0.41668015718460083\n",
      "cnt: 0 - valLoss: 0.42423251271247864 - trainLoss: 0.4166688323020935\n",
      "cnt: 0 - valLoss: 0.42422136664390564 - trainLoss: 0.416657418012619\n",
      "cnt: 0 - valLoss: 0.424210786819458 - trainLoss: 0.4166460335254669\n",
      "cnt: 0 - valLoss: 0.42420095205307007 - trainLoss: 0.4166344404220581\n",
      "cnt: 0 - valLoss: 0.4241902530193329 - trainLoss: 0.4166228473186493\n",
      "cnt: 0 - valLoss: 0.4241800904273987 - trainLoss: 0.4166112542152405\n",
      "cnt: 0 - valLoss: 0.4241691827774048 - trainLoss: 0.41659966111183167\n",
      "cnt: 0 - valLoss: 0.42415904998779297 - trainLoss: 0.41658806800842285\n",
      "cnt: 0 - valLoss: 0.4241487383842468 - trainLoss: 0.4165765345096588\n",
      "cnt: 0 - valLoss: 0.4241381585597992 - trainLoss: 0.41656494140625\n",
      "cnt: 0 - valLoss: 0.4241277873516083 - trainLoss: 0.41655340790748596\n",
      "cnt: 0 - valLoss: 0.42411744594573975 - trainLoss: 0.41654181480407715\n",
      "cnt: 0 - valLoss: 0.42410728335380554 - trainLoss: 0.41653022170066833\n",
      "cnt: 0 - valLoss: 0.42409655451774597 - trainLoss: 0.4165186882019043\n",
      "cnt: 0 - valLoss: 0.42408645153045654 - trainLoss: 0.41650712490081787\n",
      "cnt: 0 - valLoss: 0.4240760803222656 - trainLoss: 0.41649559140205383\n",
      "cnt: 0 - valLoss: 0.4240655303001404 - trainLoss: 0.4164840579032898\n",
      "cnt: 0 - valLoss: 0.4240550994873047 - trainLoss: 0.41647249460220337\n",
      "cnt: 0 - valLoss: 0.4240449368953705 - trainLoss: 0.41646093130111694\n",
      "cnt: 0 - valLoss: 0.4240342080593109 - trainLoss: 0.41644933819770813\n",
      "cnt: 0 - valLoss: 0.42402422428131104 - trainLoss: 0.4164377748966217\n",
      "cnt: 0 - valLoss: 0.42401352524757385 - trainLoss: 0.41642624139785767\n",
      "cnt: 0 - valLoss: 0.42400339245796204 - trainLoss: 0.41641464829444885\n",
      "cnt: 0 - valLoss: 0.42399290204048157 - trainLoss: 0.4164031147956848\n",
      "cnt: 0 - valLoss: 0.42398226261138916 - trainLoss: 0.416391521692276\n",
      "cnt: 0 - valLoss: 0.4239722490310669 - trainLoss: 0.41637998819351196\n",
      "cnt: 0 - valLoss: 0.42396146059036255 - trainLoss: 0.4163683354854584\n",
      "cnt: 0 - valLoss: 0.42395099997520447 - trainLoss: 0.41635680198669434\n",
      "cnt: 0 - valLoss: 0.4239409863948822 - trainLoss: 0.4163452088832855\n",
      "cnt: 0 - valLoss: 0.42393016815185547 - trainLoss: 0.4163336753845215\n",
      "cnt: 0 - valLoss: 0.42392003536224365 - trainLoss: 0.41632208228111267\n",
      "cnt: 0 - valLoss: 0.42390933632850647 - trainLoss: 0.41631051898002625\n",
      "cnt: 0 - valLoss: 0.4238992929458618 - trainLoss: 0.4162989854812622\n",
      "cnt: 0 - valLoss: 0.4238889813423157 - trainLoss: 0.41628745198249817\n",
      "cnt: 0 - valLoss: 0.42387837171554565 - trainLoss: 0.41627591848373413\n",
      "cnt: 0 - valLoss: 0.423868328332901 - trainLoss: 0.4162643551826477\n",
      "cnt: 0 - valLoss: 0.42385759949684143 - trainLoss: 0.41625291109085083\n",
      "cnt: 0 - valLoss: 0.42384713888168335 - trainLoss: 0.41624143719673157\n",
      "cnt: 0 - valLoss: 0.4238366484642029 - trainLoss: 0.4162299335002899\n",
      "cnt: 0 - valLoss: 0.4238261580467224 - trainLoss: 0.41621848940849304\n",
      "cnt: 0 - valLoss: 0.423816055059433 - trainLoss: 0.41620704531669617\n",
      "cnt: 0 - valLoss: 0.42380520701408386 - trainLoss: 0.4161956012248993\n",
      "cnt: 0 - valLoss: 0.42379501461982727 - trainLoss: 0.4161841571331024\n",
      "cnt: 0 - valLoss: 0.4237845540046692 - trainLoss: 0.41617271304130554\n",
      "cnt: 0 - valLoss: 0.4237740635871887 - trainLoss: 0.41616126894950867\n",
      "cnt: 0 - valLoss: 0.4237636923789978 - trainLoss: 0.4161498248577118\n",
      "cnt: 0 - valLoss: 0.42375320196151733 - trainLoss: 0.4161383807659149\n",
      "cnt: 0 - valLoss: 0.4237428605556488 - trainLoss: 0.41612693667411804\n",
      "cnt: 0 - valLoss: 0.42373257875442505 - trainLoss: 0.41611549258232117\n",
      "cnt: 0 - valLoss: 0.4237217605113983 - trainLoss: 0.41610410809516907\n",
      "cnt: 0 - valLoss: 0.4237118363380432 - trainLoss: 0.4160926938056946\n",
      "cnt: 0 - valLoss: 0.4237010180950165 - trainLoss: 0.4160812497138977\n",
      "cnt: 0 - valLoss: 0.4236909747123718 - trainLoss: 0.41606980562210083\n",
      "cnt: 0 - valLoss: 0.4236803352832794 - trainLoss: 0.41605842113494873\n",
      "cnt: 0 - valLoss: 0.4236701428890228 - trainLoss: 0.41604703664779663\n",
      "cnt: 0 - valLoss: 0.4236598312854767 - trainLoss: 0.41603559255599976\n",
      "cnt: 0 - valLoss: 0.42364925146102905 - trainLoss: 0.41602423787117004\n",
      "cnt: 0 - valLoss: 0.4236396849155426 - trainLoss: 0.41601279377937317\n",
      "cnt: 0 - valLoss: 0.42362990975379944 - trainLoss: 0.4160011112689972\n",
      "cnt: 0 - valLoss: 0.4236201047897339 - trainLoss: 0.41598939895629883\n",
      "cnt: 0 - valLoss: 0.4236106276512146 - trainLoss: 0.41597768664360046\n",
      "cnt: 0 - valLoss: 0.4236006736755371 - trainLoss: 0.4159660339355469\n",
      "cnt: 0 - valLoss: 0.42359113693237305 - trainLoss: 0.4159543514251709\n",
      "cnt: 0 - valLoss: 0.42358091473579407 - trainLoss: 0.4159426987171173\n",
      "cnt: 0 - valLoss: 0.4235714077949524 - trainLoss: 0.41593101620674133\n",
      "cnt: 0 - valLoss: 0.4235619604587555 - trainLoss: 0.41591936349868774\n",
      "cnt: 0 - valLoss: 0.4235519766807556 - trainLoss: 0.4159078001976013\n",
      "cnt: 0 - valLoss: 0.4235418736934662 - trainLoss: 0.41589608788490295\n",
      "cnt: 0 - valLoss: 0.4235324263572693 - trainLoss: 0.4158845543861389\n",
      "cnt: 0 - valLoss: 0.4235225021839142 - trainLoss: 0.4158729016780853\n",
      "cnt: 0 - valLoss: 0.42351290583610535 - trainLoss: 0.4158613383769989\n",
      "cnt: 0 - valLoss: 0.42350253462791443 - trainLoss: 0.4158497750759125\n",
      "cnt: 0 - valLoss: 0.42349326610565186 - trainLoss: 0.4158383011817932\n",
      "cnt: 0 - valLoss: 0.4234829246997833 - trainLoss: 0.4158267378807068\n",
      "cnt: 0 - valLoss: 0.42347317934036255 - trainLoss: 0.41581520438194275\n",
      "cnt: 0 - valLoss: 0.42346346378326416 - trainLoss: 0.4158037006855011\n",
      "cnt: 0 - valLoss: 0.4234533905982971 - trainLoss: 0.41579222679138184\n",
      "cnt: 0 - valLoss: 0.4234435260295868 - trainLoss: 0.4157807230949402\n",
      "cnt: 0 - valLoss: 0.4234335720539093 - trainLoss: 0.4157692492008209\n",
      "cnt: 0 - valLoss: 0.42342373728752136 - trainLoss: 0.4157576858997345\n",
      "cnt: 0 - valLoss: 0.42341384291648865 - trainLoss: 0.4157462418079376\n",
      "cnt: 0 - valLoss: 0.42340388894081116 - trainLoss: 0.41573476791381836\n",
      "cnt: 0 - valLoss: 0.4233939051628113 - trainLoss: 0.4157232642173767\n",
      "cnt: 0 - valLoss: 0.42338407039642334 - trainLoss: 0.41571182012557983\n",
      "cnt: 0 - valLoss: 0.42337438464164734 - trainLoss: 0.4157003164291382\n",
      "cnt: 0 - valLoss: 0.42336413264274597 - trainLoss: 0.4156888425350189\n",
      "cnt: 0 - valLoss: 0.42335444688796997 - trainLoss: 0.41567733883857727\n",
      "cnt: 0 - valLoss: 0.423344224691391 - trainLoss: 0.4156658947467804\n",
      "cnt: 0 - valLoss: 0.4233345091342926 - trainLoss: 0.41565439105033875\n",
      "cnt: 0 - valLoss: 0.4233242869377136 - trainLoss: 0.41564294695854187\n",
      "cnt: 0 - valLoss: 0.42331480979919434 - trainLoss: 0.415631502866745\n",
      "cnt: 0 - valLoss: 0.4233044683933258 - trainLoss: 0.4156201183795929\n",
      "cnt: 0 - valLoss: 0.4232944846153259 - trainLoss: 0.4156087338924408\n",
      "cnt: 0 - valLoss: 0.4232843816280365 - trainLoss: 0.4155973792076111\n",
      "cnt: 0 - valLoss: 0.4232742190361023 - trainLoss: 0.4155859351158142\n",
      "cnt: 0 - valLoss: 0.42326444387435913 - trainLoss: 0.4155745804309845\n",
      "cnt: 0 - valLoss: 0.4232542812824249 - trainLoss: 0.4155632555484772\n",
      "cnt: 0 - valLoss: 0.42324456572532654 - trainLoss: 0.4155518114566803\n",
      "cnt: 0 - valLoss: 0.4232342541217804 - trainLoss: 0.4155404567718506\n",
      "cnt: 0 - valLoss: 0.4232243597507477 - trainLoss: 0.4155290424823761\n",
      "cnt: 0 - valLoss: 0.42321404814720154 - trainLoss: 0.4155177175998688\n",
      "cnt: 0 - valLoss: 0.42320430278778076 - trainLoss: 0.41550636291503906\n",
      "cnt: 0 - valLoss: 0.42319372296333313 - trainLoss: 0.4154950678348541\n",
      "cnt: 0 - valLoss: 0.4231836795806885 - trainLoss: 0.4154838025569916\n",
      "cnt: 0 - valLoss: 0.4231736361980438 - trainLoss: 0.4154726564884186\n",
      "cnt: 0 - valLoss: 0.4231632947921753 - trainLoss: 0.4154614806175232\n",
      "cnt: 0 - valLoss: 0.4231525957584381 - trainLoss: 0.4154503047466278\n",
      "cnt: 0 - valLoss: 0.42314252257347107 - trainLoss: 0.4154391288757324\n",
      "cnt: 0 - valLoss: 0.4231320023536682 - trainLoss: 0.41542795300483704\n",
      "cnt: 0 - valLoss: 0.42312169075012207 - trainLoss: 0.41541680693626404\n",
      "cnt: 0 - valLoss: 0.4231112003326416 - trainLoss: 0.41540560126304626\n",
      "cnt: 0 - valLoss: 0.4231008291244507 - trainLoss: 0.4153944253921509\n",
      "cnt: 0 - valLoss: 0.4230901002883911 - trainLoss: 0.4153832793235779\n",
      "cnt: 0 - valLoss: 0.4230796992778778 - trainLoss: 0.4153721034526825\n",
      "cnt: 0 - valLoss: 0.42306947708129883 - trainLoss: 0.4153609573841095\n",
      "cnt: 0 - valLoss: 0.42305848002433777 - trainLoss: 0.4153498113155365\n",
      "cnt: 0 - valLoss: 0.42304831743240356 - trainLoss: 0.4153387248516083\n",
      "cnt: 0 - valLoss: 0.42303770780563354 - trainLoss: 0.41532760858535767\n",
      "cnt: 0 - valLoss: 0.4230267405509949 - trainLoss: 0.41531646251678467\n",
      "cnt: 0 - valLoss: 0.42301681637763977 - trainLoss: 0.41530531644821167\n",
      "cnt: 0 - valLoss: 0.42300617694854736 - trainLoss: 0.41529420018196106\n",
      "cnt: 0 - valLoss: 0.4229959547519684 - trainLoss: 0.41528305411338806\n",
      "cnt: 0 - valLoss: 0.4229851961135864 - trainLoss: 0.41527190804481506\n",
      "cnt: 0 - valLoss: 0.42297494411468506 - trainLoss: 0.4152607023715973\n",
      "cnt: 0 - valLoss: 0.4229646623134613 - trainLoss: 0.41524961590766907\n",
      "cnt: 0 - valLoss: 0.4229544699192047 - trainLoss: 0.4152384102344513\n",
      "cnt: 0 - valLoss: 0.42294400930404663 - trainLoss: 0.4152272641658783\n",
      "cnt: 0 - valLoss: 0.4229332208633423 - trainLoss: 0.4152160584926605\n",
      "cnt: 0 - valLoss: 0.42292362451553345 - trainLoss: 0.4152049422264099\n",
      "cnt: 0 - valLoss: 0.4229128360748291 - trainLoss: 0.41519370675086975\n",
      "cnt: 0 - valLoss: 0.42290303111076355 - trainLoss: 0.41518259048461914\n",
      "cnt: 0 - valLoss: 0.4228920042514801 - trainLoss: 0.4151715040206909\n",
      "cnt: 0 - valLoss: 0.4228825271129608 - trainLoss: 0.4151603877544403\n",
      "cnt: 0 - valLoss: 0.42287155985832214 - trainLoss: 0.4151493012905121\n",
      "cnt: 0 - valLoss: 0.422861784696579 - trainLoss: 0.4151381552219391\n",
      "cnt: 0 - valLoss: 0.42285075783729553 - trainLoss: 0.41512709856033325\n",
      "cnt: 0 - valLoss: 0.42284122109413147 - trainLoss: 0.41511598229408264\n",
      "cnt: 0 - valLoss: 0.4228304326534271 - trainLoss: 0.4151048958301544\n",
      "cnt: 0 - valLoss: 0.42282071709632874 - trainLoss: 0.4150937795639038\n",
      "cnt: 0 - valLoss: 0.4228101670742035 - trainLoss: 0.4150826930999756\n",
      "cnt: 0 - valLoss: 0.4228001534938812 - trainLoss: 0.415071576833725\n",
      "cnt: 0 - valLoss: 0.42278966307640076 - trainLoss: 0.41506049036979675\n",
      "cnt: 0 - valLoss: 0.42277950048446655 - trainLoss: 0.4150494337081909\n",
      "cnt: 0 - valLoss: 0.422769159078598 - trainLoss: 0.4150383770465851\n",
      "cnt: 0 - valLoss: 0.4227592647075653 - trainLoss: 0.41502732038497925\n",
      "cnt: 0 - valLoss: 0.4227486848831177 - trainLoss: 0.415016233921051\n",
      "cnt: 0 - valLoss: 0.42273861169815063 - trainLoss: 0.4150051772594452\n",
      "cnt: 0 - valLoss: 0.42272818088531494 - trainLoss: 0.41499412059783936\n",
      "cnt: 0 - valLoss: 0.4227180778980255 - trainLoss: 0.4149830639362335\n",
      "cnt: 0 - valLoss: 0.42270776629447937 - trainLoss: 0.4149720072746277\n",
      "cnt: 0 - valLoss: 0.4226977825164795 - trainLoss: 0.41496095061302185\n",
      "cnt: 0 - valLoss: 0.42268744111061096 - trainLoss: 0.414949893951416\n",
      "cnt: 0 - valLoss: 0.4226774275302887 - trainLoss: 0.41493889689445496\n",
      "cnt: 0 - valLoss: 0.4226669371128082 - trainLoss: 0.41492781043052673\n",
      "cnt: 0 - valLoss: 0.4226570427417755 - trainLoss: 0.4149167835712433\n",
      "cnt: 0 - valLoss: 0.4226464033126831 - trainLoss: 0.41490575671195984\n",
      "cnt: 0 - valLoss: 0.42263686656951904 - trainLoss: 0.4148947298526764\n",
      "cnt: 0 - valLoss: 0.42262622714042664 - trainLoss: 0.41488373279571533\n",
      "cnt: 0 - valLoss: 0.422616571187973 - trainLoss: 0.4148726761341095\n",
      "cnt: 0 - valLoss: 0.4226057231426239 - trainLoss: 0.41486167907714844\n",
      "cnt: 0 - valLoss: 0.4225960671901703 - trainLoss: 0.4148506820201874\n",
      "cnt: 0 - valLoss: 0.42258548736572266 - trainLoss: 0.41483962535858154\n",
      "cnt: 0 - valLoss: 0.422575980424881 - trainLoss: 0.41482865810394287\n",
      "cnt: 0 - valLoss: 0.42256513237953186 - trainLoss: 0.41481760144233704\n",
      "cnt: 0 - valLoss: 0.4225555658340454 - trainLoss: 0.414806604385376\n",
      "cnt: 0 - valLoss: 0.422544002532959 - trainLoss: 0.4147956073284149\n",
      "cnt: 0 - valLoss: 0.4225348234176636 - trainLoss: 0.41478464007377625\n",
      "cnt: 0 - valLoss: 0.42252543568611145 - trainLoss: 0.4147736430168152\n",
      "cnt: 0 - valLoss: 0.4225136339664459 - trainLoss: 0.4147626459598541\n",
      "cnt: 0 - valLoss: 0.4225040078163147 - trainLoss: 0.414751797914505\n",
      "cnt: 0 - valLoss: 0.4224947690963745 - trainLoss: 0.41474077105522156\n",
      "cnt: 0 - valLoss: 0.422482967376709 - trainLoss: 0.41472986340522766\n",
      "cnt: 0 - valLoss: 0.4224739968776703 - trainLoss: 0.4147189259529114\n",
      "cnt: 0 - valLoss: 0.422463983297348 - trainLoss: 0.4147079586982727\n",
      "cnt: 0 - valLoss: 0.4224536716938019 - trainLoss: 0.4146971106529236\n",
      "cnt: 0 - valLoss: 0.4224439561367035 - trainLoss: 0.4146861433982849\n",
      "cnt: 0 - valLoss: 0.4224335253238678 - trainLoss: 0.4146752953529358\n",
      "cnt: 0 - valLoss: 0.42242351174354553 - trainLoss: 0.4146643280982971\n",
      "cnt: 0 - valLoss: 0.4224134683609009 - trainLoss: 0.414653480052948\n",
      "cnt: 0 - valLoss: 0.4224031865596771 - trainLoss: 0.4146426320075989\n",
      "cnt: 0 - valLoss: 0.4223938584327698 - trainLoss: 0.414631724357605\n",
      "cnt: 0 - valLoss: 0.4223833382129669 - trainLoss: 0.41462087631225586\n",
      "cnt: 0 - valLoss: 0.4223729074001312 - trainLoss: 0.41460996866226196\n",
      "cnt: 0 - valLoss: 0.4223622977733612 - trainLoss: 0.41459909081459045\n",
      "cnt: 0 - valLoss: 0.4223526418209076 - trainLoss: 0.41458824276924133\n",
      "cnt: 0 - valLoss: 0.4223419427871704 - trainLoss: 0.4145773947238922\n",
      "cnt: 0 - valLoss: 0.42233315110206604 - trainLoss: 0.4145665168762207\n",
      "cnt: 0 - valLoss: 0.42232292890548706 - trainLoss: 0.4145556688308716\n",
      "cnt: 0 - valLoss: 0.4223124086856842 - trainLoss: 0.41454482078552246\n",
      "cnt: 0 - valLoss: 0.4223019480705261 - trainLoss: 0.41453394293785095\n",
      "cnt: 0 - valLoss: 0.42229172587394714 - trainLoss: 0.41452309489250183\n",
      "cnt: 0 - valLoss: 0.4222814440727234 - trainLoss: 0.4145122468471527\n",
      "cnt: 0 - valLoss: 0.4222714304924011 - trainLoss: 0.4145013988018036\n",
      "cnt: 0 - valLoss: 0.422261506319046 - trainLoss: 0.4144905209541321\n",
      "cnt: 0 - valLoss: 0.4222511649131775 - trainLoss: 0.41447973251342773\n",
      "cnt: 0 - valLoss: 0.42224135994911194 - trainLoss: 0.414468914270401\n",
      "cnt: 0 - valLoss: 0.4222312569618225 - trainLoss: 0.4144580662250519\n",
      "cnt: 0 - valLoss: 0.4222211241722107 - trainLoss: 0.41444724798202515\n",
      "cnt: 0 - valLoss: 0.4222114086151123 - trainLoss: 0.4144364297389984\n",
      "cnt: 0 - valLoss: 0.4222012460231781 - trainLoss: 0.4144255816936493\n",
      "cnt: 0 - valLoss: 0.42219144105911255 - trainLoss: 0.41441473364830017\n",
      "cnt: 0 - valLoss: 0.4221811890602112 - trainLoss: 0.41440391540527344\n",
      "cnt: 0 - valLoss: 0.4221712052822113 - trainLoss: 0.4143930971622467\n",
      "cnt: 0 - valLoss: 0.42216160893440247 - trainLoss: 0.41438230872154236\n",
      "cnt: 0 - valLoss: 0.42215123772621155 - trainLoss: 0.4143714904785156\n",
      "cnt: 0 - valLoss: 0.4221414029598236 - trainLoss: 0.41436073184013367\n",
      "cnt: 0 - valLoss: 0.42213141918182373 - trainLoss: 0.41434991359710693\n",
      "cnt: 0 - valLoss: 0.4221212565898895 - trainLoss: 0.414339154958725\n",
      "cnt: 0 - valLoss: 0.4221114218235016 - trainLoss: 0.414328396320343\n",
      "cnt: 0 - valLoss: 0.4221017062664032 - trainLoss: 0.41431760787963867\n",
      "cnt: 0 - valLoss: 0.422091543674469 - trainLoss: 0.4143068492412567\n",
      "cnt: 0 - valLoss: 0.42208167910575867 - trainLoss: 0.41429609060287476\n",
      "cnt: 0 - valLoss: 0.4220719039440155 - trainLoss: 0.4142853319644928\n",
      "cnt: 0 - valLoss: 0.422061949968338 - trainLoss: 0.41427454352378845\n",
      "cnt: 0 - valLoss: 0.4220520555973053 - trainLoss: 0.4142637848854065\n",
      "cnt: 0 - valLoss: 0.42204228043556213 - trainLoss: 0.41425302624702454\n",
      "cnt: 0 - valLoss: 0.4220322072505951 - trainLoss: 0.4142422676086426\n",
      "cnt: 0 - valLoss: 0.42202228307724 - trainLoss: 0.4142315089702606\n",
      "cnt: 0 - valLoss: 0.4220125675201416 - trainLoss: 0.41422075033187866\n",
      "cnt: 0 - valLoss: 0.4220026433467865 - trainLoss: 0.41421011090278625\n",
      "cnt: 0 - valLoss: 0.4219927191734314 - trainLoss: 0.4141993522644043\n",
      "cnt: 0 - valLoss: 0.42198270559310913 - trainLoss: 0.41418859362602234\n",
      "cnt: 0 - valLoss: 0.4219726622104645 - trainLoss: 0.41417792439460754\n",
      "cnt: 0 - valLoss: 0.4219629466533661 - trainLoss: 0.4141671657562256\n",
      "cnt: 0 - valLoss: 0.42195290327072144 - trainLoss: 0.414156436920166\n",
      "cnt: 0 - valLoss: 0.42194414138793945 - trainLoss: 0.41414573788642883\n",
      "cnt: 0 - valLoss: 0.42193517088890076 - trainLoss: 0.4141348898410797\n",
      "cnt: 0 - valLoss: 0.4219259023666382 - trainLoss: 0.41412413120269775\n",
      "cnt: 0 - valLoss: 0.4219168424606323 - trainLoss: 0.414113312959671\n",
      "cnt: 0 - valLoss: 0.4219079315662384 - trainLoss: 0.41410255432128906\n",
      "cnt: 0 - valLoss: 0.421898752450943 - trainLoss: 0.4140917956829071\n",
      "cnt: 0 - valLoss: 0.42188945412635803 - trainLoss: 0.41408097743988037\n",
      "cnt: 0 - valLoss: 0.42188045382499695 - trainLoss: 0.4140702188014984\n",
      "cnt: 0 - valLoss: 0.421870619058609 - trainLoss: 0.41405946016311646\n",
      "cnt: 0 - valLoss: 0.4218611717224121 - trainLoss: 0.4140487015247345\n",
      "cnt: 0 - valLoss: 0.4218512773513794 - trainLoss: 0.4140380322933197\n",
      "cnt: 0 - valLoss: 0.4218413531780243 - trainLoss: 0.41402727365493774\n",
      "cnt: 0 - valLoss: 0.42183151841163635 - trainLoss: 0.41401657462120056\n",
      "cnt: 0 - valLoss: 0.42182183265686035 - trainLoss: 0.4140058755874634\n",
      "cnt: 0 - valLoss: 0.42181164026260376 - trainLoss: 0.4139951467514038\n",
      "cnt: 0 - valLoss: 0.4218020439147949 - trainLoss: 0.4139844477176666\n",
      "cnt: 0 - valLoss: 0.42179208993911743 - trainLoss: 0.41397374868392944\n",
      "cnt: 0 - valLoss: 0.4217820465564728 - trainLoss: 0.41396307945251465\n",
      "cnt: 0 - valLoss: 0.4217723309993744 - trainLoss: 0.4139523506164551\n",
      "cnt: 0 - valLoss: 0.4217625856399536 - trainLoss: 0.41394171118736267\n",
      "cnt: 0 - valLoss: 0.42175257205963135 - trainLoss: 0.4139310419559479\n",
      "cnt: 0 - valLoss: 0.42174288630485535 - trainLoss: 0.4139203727245331\n",
      "cnt: 0 - valLoss: 0.42173299193382263 - trainLoss: 0.4139096140861511\n",
      "cnt: 0 - valLoss: 0.4217230975627899 - trainLoss: 0.41389894485473633\n",
      "cnt: 0 - valLoss: 0.4217131733894348 - trainLoss: 0.4138883054256439\n",
      "cnt: 0 - valLoss: 0.4217035472393036 - trainLoss: 0.4138776361942291\n",
      "cnt: 0 - valLoss: 0.4216936528682709 - trainLoss: 0.41386696696281433\n",
      "cnt: 0 - valLoss: 0.42168405652046204 - trainLoss: 0.41385629773139954\n",
      "cnt: 0 - valLoss: 0.4216739535331726 - trainLoss: 0.41384565830230713\n",
      "cnt: 0 - valLoss: 0.4216642379760742 - trainLoss: 0.41383498907089233\n",
      "cnt: 0 - valLoss: 0.4216545820236206 - trainLoss: 0.41382431983947754\n",
      "cnt: 0 - valLoss: 0.42164474725723267 - trainLoss: 0.41381365060806274\n",
      "cnt: 0 - valLoss: 0.42163506150245667 - trainLoss: 0.4138030409812927\n",
      "cnt: 0 - valLoss: 0.4216254651546478 - trainLoss: 0.4137924015522003\n",
      "cnt: 0 - valLoss: 0.4216155409812927 - trainLoss: 0.4137817919254303\n",
      "cnt: 0 - valLoss: 0.4216057062149048 - trainLoss: 0.4137710630893707\n",
      "cnt: 0 - valLoss: 0.4215964078903198 - trainLoss: 0.4137604534626007\n",
      "cnt: 0 - valLoss: 0.4215866029262543 - trainLoss: 0.4137498438358307\n",
      "cnt: 0 - valLoss: 0.42157652974128723 - trainLoss: 0.4137391746044159\n",
      "cnt: 0 - valLoss: 0.42156726121902466 - trainLoss: 0.4137285649776459\n",
      "cnt: 0 - valLoss: 0.42155706882476807 - trainLoss: 0.4137178957462311\n",
      "cnt: 0 - valLoss: 0.4215475618839264 - trainLoss: 0.41370734572410583\n",
      "cnt: 0 - valLoss: 0.42153826355934143 - trainLoss: 0.41369667649269104\n",
      "cnt: 0 - valLoss: 0.42152854800224304 - trainLoss: 0.413686066865921\n",
      "cnt: 0 - valLoss: 0.42151862382888794 - trainLoss: 0.413675457239151\n",
      "cnt: 0 - valLoss: 0.4215090870857239 - trainLoss: 0.413664847612381\n",
      "cnt: 0 - valLoss: 0.4214994013309479 - trainLoss: 0.41365423798561096\n",
      "cnt: 0 - valLoss: 0.42148980498313904 - trainLoss: 0.41364362835884094\n",
      "cnt: 0 - valLoss: 0.4214803874492645 - trainLoss: 0.4136330187320709\n",
      "cnt: 0 - valLoss: 0.42147096991539 - trainLoss: 0.4136224389076233\n",
      "cnt: 0 - valLoss: 0.42146116495132446 - trainLoss: 0.41361182928085327\n",
      "cnt: 0 - valLoss: 0.4214516282081604 - trainLoss: 0.4136013090610504\n",
      "cnt: 0 - valLoss: 0.4214417040348053 - trainLoss: 0.4135906994342804\n",
      "cnt: 0 - valLoss: 0.4214320480823517 - trainLoss: 0.41358017921447754\n",
      "cnt: 0 - valLoss: 0.42142295837402344 - trainLoss: 0.4135696291923523\n",
      "cnt: 0 - valLoss: 0.42141303420066833 - trainLoss: 0.41355904936790466\n",
      "cnt: 0 - valLoss: 0.42140331864356995 - trainLoss: 0.4135484993457794\n",
      "cnt: 0 - valLoss: 0.4213937520980835 - trainLoss: 0.41353797912597656\n",
      "cnt: 0 - valLoss: 0.4213840663433075 - trainLoss: 0.4135274291038513\n",
      "cnt: 0 - valLoss: 0.4213743209838867 - trainLoss: 0.41351690888404846\n",
      "cnt: 0 - valLoss: 0.42136484384536743 - trainLoss: 0.41350632905960083\n",
      "cnt: 0 - valLoss: 0.42135533690452576 - trainLoss: 0.41349586844444275\n",
      "cnt: 0 - valLoss: 0.4213457405567169 - trainLoss: 0.4134853184223175\n",
      "cnt: 0 - valLoss: 0.42133602499961853 - trainLoss: 0.4134747385978699\n",
      "cnt: 0 - valLoss: 0.42132678627967834 - trainLoss: 0.4134642779827118\n",
      "cnt: 0 - valLoss: 0.42131689190864563 - trainLoss: 0.41345372796058655\n",
      "cnt: 0 - valLoss: 0.42130744457244873 - trainLoss: 0.4134432375431061\n",
      "cnt: 0 - valLoss: 0.42129820585250854 - trainLoss: 0.41343268752098083\n",
      "cnt: 0 - valLoss: 0.4212886393070221 - trainLoss: 0.41342222690582275\n",
      "cnt: 0 - valLoss: 0.42127901315689087 - trainLoss: 0.4134116470813751\n",
      "cnt: 0 - valLoss: 0.4212697446346283 - trainLoss: 0.41340118646621704\n",
      "cnt: 0 - valLoss: 0.4212600588798523 - trainLoss: 0.4133906662464142\n",
      "cnt: 0 - valLoss: 0.4212506413459778 - trainLoss: 0.41338014602661133\n",
      "cnt: 0 - valLoss: 0.42124143242836 - trainLoss: 0.41336968541145325\n",
      "cnt: 0 - valLoss: 0.42123183608055115 - trainLoss: 0.41335922479629517\n",
      "cnt: 0 - valLoss: 0.4212222099304199 - trainLoss: 0.4133487045764923\n",
      "cnt: 0 - valLoss: 0.4212128818035126 - trainLoss: 0.41333818435668945\n",
      "cnt: 0 - valLoss: 0.42120349407196045 - trainLoss: 0.41332772374153137\n",
      "cnt: 0 - valLoss: 0.4211939573287964 - trainLoss: 0.4133172631263733\n",
      "cnt: 0 - valLoss: 0.4211845099925995 - trainLoss: 0.4133068025112152\n",
      "cnt: 0 - valLoss: 0.42117559909820557 - trainLoss: 0.41329634189605713\n",
      "cnt: 0 - valLoss: 0.42116579413414 - trainLoss: 0.41328588128089905\n",
      "cnt: 0 - valLoss: 0.4211564362049103 - trainLoss: 0.4132753908634186\n",
      "cnt: 0 - valLoss: 0.42114707827568054 - trainLoss: 0.4132649302482605\n",
      "cnt: 0 - valLoss: 0.4211374819278717 - trainLoss: 0.4132544696331024\n",
      "cnt: 0 - valLoss: 0.4211280047893524 - trainLoss: 0.41324400901794434\n",
      "cnt: 0 - valLoss: 0.42111867666244507 - trainLoss: 0.41323357820510864\n",
      "cnt: 0 - valLoss: 0.4211094379425049 - trainLoss: 0.41322311758995056\n",
      "cnt: 0 - valLoss: 0.4210999011993408 - trainLoss: 0.41321271657943726\n",
      "cnt: 0 - valLoss: 0.42109039425849915 - trainLoss: 0.4132022261619568\n",
      "cnt: 0 - valLoss: 0.42108118534088135 - trainLoss: 0.4131917655467987\n",
      "cnt: 0 - valLoss: 0.4210715591907501 - trainLoss: 0.4131813645362854\n",
      "cnt: 0 - valLoss: 0.4210624694824219 - trainLoss: 0.4131709337234497\n",
      "cnt: 0 - valLoss: 0.4210532009601593 - trainLoss: 0.4131605327129364\n",
      "cnt: 0 - valLoss: 0.4210437536239624 - trainLoss: 0.4131501019001007\n",
      "cnt: 0 - valLoss: 0.42103463411331177 - trainLoss: 0.41313955187797546\n",
      "cnt: 0 - valLoss: 0.42102569341659546 - trainLoss: 0.413129061460495\n",
      "cnt: 0 - valLoss: 0.4210163950920105 - trainLoss: 0.4131186008453369\n",
      "cnt: 0 - valLoss: 0.4210068881511688 - trainLoss: 0.41310808062553406\n",
      "cnt: 0 - valLoss: 0.4209977090358734 - trainLoss: 0.4130975902080536\n",
      "cnt: 0 - valLoss: 0.4209887981414795 - trainLoss: 0.4130870997905731\n",
      "cnt: 0 - valLoss: 0.42097973823547363 - trainLoss: 0.41307663917541504\n",
      "cnt: 0 - valLoss: 0.42097023129463196 - trainLoss: 0.41306617856025696\n",
      "cnt: 0 - valLoss: 0.4209613502025604 - trainLoss: 0.4130557179450989\n",
      "cnt: 0 - valLoss: 0.42095211148262024 - trainLoss: 0.413045197725296\n",
      "cnt: 0 - valLoss: 0.4209426939487457 - trainLoss: 0.41303473711013794\n",
      "cnt: 0 - valLoss: 0.42093342542648315 - trainLoss: 0.41302430629730225\n",
      "cnt: 0 - valLoss: 0.4209243357181549 - trainLoss: 0.41301384568214417\n",
      "cnt: 0 - valLoss: 0.42091530561447144 - trainLoss: 0.4130033850669861\n",
      "cnt: 0 - valLoss: 0.4209059476852417 - trainLoss: 0.412992924451828\n",
      "cnt: 0 - valLoss: 0.42089664936065674 - trainLoss: 0.4129824638366699\n",
      "cnt: 0 - valLoss: 0.42088761925697327 - trainLoss: 0.412972092628479\n",
      "cnt: 0 - valLoss: 0.42087817192077637 - trainLoss: 0.4129616320133209\n",
      "cnt: 0 - valLoss: 0.4208689332008362 - trainLoss: 0.41295126080513\n",
      "cnt: 0 - valLoss: 0.4208599328994751 - trainLoss: 0.4129408299922943\n",
      "cnt: 0 - valLoss: 0.4208506643772125 - trainLoss: 0.412930428981781\n",
      "cnt: 0 - valLoss: 0.42084136605262756 - trainLoss: 0.4129200577735901\n",
      "cnt: 0 - valLoss: 0.4208325743675232 - trainLoss: 0.41290968656539917\n",
      "cnt: 0 - valLoss: 0.4208230674266815 - trainLoss: 0.41289931535720825\n",
      "cnt: 0 - valLoss: 0.42081403732299805 - trainLoss: 0.41288885474205017\n",
      "cnt: 0 - valLoss: 0.42080479860305786 - trainLoss: 0.41287851333618164\n",
      "cnt: 0 - valLoss: 0.4207956790924072 - trainLoss: 0.4128681719303131\n",
      "cnt: 0 - valLoss: 0.4207865297794342 - trainLoss: 0.4128577411174774\n",
      "cnt: 0 - valLoss: 0.42077741026878357 - trainLoss: 0.4128473699092865\n",
      "cnt: 0 - valLoss: 0.42076852917671204 - trainLoss: 0.4128369987010956\n",
      "cnt: 0 - valLoss: 0.4207593500614166 - trainLoss: 0.41282662749290466\n",
      "cnt: 0 - valLoss: 0.42075031995773315 - trainLoss: 0.41281625628471375\n",
      "cnt: 0 - valLoss: 0.42074137926101685 - trainLoss: 0.41280579566955566\n",
      "cnt: 0 - valLoss: 0.420732319355011 - trainLoss: 0.41279542446136475\n",
      "cnt: 0 - valLoss: 0.4207233786582947 - trainLoss: 0.41278502345085144\n",
      "cnt: 0 - valLoss: 0.420714408159256 - trainLoss: 0.41277459263801575\n",
      "cnt: 0 - valLoss: 0.42070555686950684 - trainLoss: 0.41276422142982483\n",
      "cnt: 0 - valLoss: 0.42069634795188904 - trainLoss: 0.4127538204193115\n",
      "cnt: 0 - valLoss: 0.4206869304180145 - trainLoss: 0.41274338960647583\n",
      "cnt: 0 - valLoss: 0.4206780195236206 - trainLoss: 0.4127330183982849\n",
      "cnt: 0 - valLoss: 0.4206690490245819 - trainLoss: 0.41272270679473877\n",
      "cnt: 0 - valLoss: 0.4206596910953522 - trainLoss: 0.4127123951911926\n",
      "cnt: 0 - valLoss: 0.420650452375412 - trainLoss: 0.4127020239830017\n",
      "cnt: 0 - valLoss: 0.4206415116786957 - trainLoss: 0.41269171237945557\n",
      "cnt: 0 - valLoss: 0.42063212394714355 - trainLoss: 0.41268137097358704\n",
      "cnt: 0 - valLoss: 0.42062315344810486 - trainLoss: 0.4126710593700409\n",
      "cnt: 0 - valLoss: 0.4206138253211975 - trainLoss: 0.41266074776649475\n",
      "cnt: 0 - valLoss: 0.4206048548221588 - trainLoss: 0.4126504361629486\n",
      "cnt: 0 - valLoss: 0.4205954670906067 - trainLoss: 0.4126400947570801\n",
      "cnt: 0 - valLoss: 0.42058658599853516 - trainLoss: 0.4126298427581787\n",
      "cnt: 0 - valLoss: 0.4205775856971741 - trainLoss: 0.4126194715499878\n",
      "cnt: 0 - valLoss: 0.42056816816329956 - trainLoss: 0.41260918974876404\n",
      "cnt: 0 - valLoss: 0.42055919766426086 - trainLoss: 0.4125988781452179\n",
      "cnt: 0 - valLoss: 0.420549601316452 - trainLoss: 0.41258856654167175\n",
      "cnt: 0 - valLoss: 0.420540988445282 - trainLoss: 0.41257819533348083\n",
      "cnt: 0 - valLoss: 0.4205317795276642 - trainLoss: 0.4125679135322571\n",
      "cnt: 0 - valLoss: 0.4205227494239807 - trainLoss: 0.41255760192871094\n",
      "cnt: 0 - valLoss: 0.4205133020877838 - trainLoss: 0.4125472903251648\n",
      "cnt: 0 - valLoss: 0.42050451040267944 - trainLoss: 0.41253700852394104\n",
      "cnt: 0 - valLoss: 0.4204953610897064 - trainLoss: 0.4125266969203949\n",
      "cnt: 0 - valLoss: 0.420486181974411 - trainLoss: 0.41251638531684875\n",
      "cnt: 0 - valLoss: 0.4204772412776947 - trainLoss: 0.412506103515625\n",
      "cnt: 0 - valLoss: 0.42046821117401123 - trainLoss: 0.41249579191207886\n",
      "cnt: 0 - valLoss: 0.4204585552215576 - trainLoss: 0.4124855101108551\n",
      "cnt: 0 - valLoss: 0.42044946551322937 - trainLoss: 0.41247519850730896\n",
      "cnt: 0 - valLoss: 0.42044058442115784 - trainLoss: 0.4124649167060852\n",
      "cnt: 0 - valLoss: 0.4204312562942505 - trainLoss: 0.41245466470718384\n",
      "cnt: 0 - valLoss: 0.4204217791557312 - trainLoss: 0.41244444251060486\n",
      "cnt: 0 - valLoss: 0.4204128384590149 - trainLoss: 0.4124342203140259\n",
      "cnt: 0 - valLoss: 0.42040392756462097 - trainLoss: 0.4124239385128021\n",
      "cnt: 0 - valLoss: 0.4203948974609375 - trainLoss: 0.4124137759208679\n",
      "cnt: 0 - valLoss: 0.4203856587409973 - trainLoss: 0.41240355372428894\n",
      "cnt: 0 - valLoss: 0.42037662863731384 - trainLoss: 0.41239333152770996\n",
      "cnt: 0 - valLoss: 0.420367568731308 - trainLoss: 0.41238313913345337\n",
      "cnt: 0 - valLoss: 0.42035794258117676 - trainLoss: 0.41237297654151917\n",
      "cnt: 0 - valLoss: 0.42034855484962463 - trainLoss: 0.41236281394958496\n",
      "cnt: 0 - valLoss: 0.42033910751342773 - trainLoss: 0.41235268115997314\n",
      "cnt: 0 - valLoss: 0.42032942175865173 - trainLoss: 0.41234254837036133\n",
      "cnt: 0 - valLoss: 0.4203200042247772 - trainLoss: 0.4123324453830719\n",
      "cnt: 0 - valLoss: 0.42031043767929077 - trainLoss: 0.4123223125934601\n",
      "cnt: 0 - valLoss: 0.4203011393547058 - trainLoss: 0.4123121500015259\n",
      "cnt: 0 - valLoss: 0.4202916920185089 - trainLoss: 0.41230207681655884\n",
      "cnt: 0 - valLoss: 0.4202824831008911 - trainLoss: 0.41229188442230225\n",
      "cnt: 0 - valLoss: 0.42027318477630615 - trainLoss: 0.41228172183036804\n",
      "cnt: 0 - valLoss: 0.42026379704475403 - trainLoss: 0.4122715890407562\n",
      "cnt: 0 - valLoss: 0.4202544689178467 - trainLoss: 0.4122614860534668\n",
      "cnt: 0 - valLoss: 0.4202451705932617 - trainLoss: 0.412251353263855\n",
      "cnt: 0 - valLoss: 0.4202357828617096 - trainLoss: 0.41224122047424316\n",
      "cnt: 0 - valLoss: 0.42022645473480225 - trainLoss: 0.41223105788230896\n",
      "cnt: 0 - valLoss: 0.42021751403808594 - trainLoss: 0.41222089529037476\n",
      "cnt: 0 - valLoss: 0.42020800709724426 - trainLoss: 0.41221076250076294\n",
      "cnt: 0 - valLoss: 0.4201990067958832 - trainLoss: 0.4122006297111511\n",
      "cnt: 0 - valLoss: 0.4201895296573639 - trainLoss: 0.4121905565261841\n",
      "cnt: 0 - valLoss: 0.420180082321167 - trainLoss: 0.41218042373657227\n",
      "cnt: 0 - valLoss: 0.42017102241516113 - trainLoss: 0.41217032074928284\n",
      "cnt: 0 - valLoss: 0.42016157507896423 - trainLoss: 0.4121602475643158\n",
      "cnt: 0 - valLoss: 0.4201524555683136 - trainLoss: 0.41215020418167114\n",
      "cnt: 0 - valLoss: 0.42014291882514954 - trainLoss: 0.4121401906013489\n",
      "cnt: 0 - valLoss: 0.4201337695121765 - trainLoss: 0.41213011741638184\n",
      "cnt: 0 - valLoss: 0.4201245903968811 - trainLoss: 0.4121200740337372\n",
      "cnt: 0 - valLoss: 0.4201156198978424 - trainLoss: 0.4121100604534149\n",
      "cnt: 0 - valLoss: 0.42010655999183655 - trainLoss: 0.41210007667541504\n",
      "cnt: 0 - valLoss: 0.4200974106788635 - trainLoss: 0.4120900630950928\n",
      "cnt: 0 - valLoss: 0.42008814215660095 - trainLoss: 0.4120800793170929\n",
      "cnt: 0 - valLoss: 0.4200791120529175 - trainLoss: 0.41207006573677063\n",
      "cnt: 0 - valLoss: 0.42007017135620117 - trainLoss: 0.41206008195877075\n",
      "cnt: 0 - valLoss: 0.420060932636261 - trainLoss: 0.4120500385761261\n",
      "cnt: 0 - valLoss: 0.4200519919395447 - trainLoss: 0.4120400846004486\n",
      "cnt: 0 - valLoss: 0.4200429320335388 - trainLoss: 0.41203004121780396\n",
      "cnt: 0 - valLoss: 0.4200337827205658 - trainLoss: 0.4120200574398041\n",
      "cnt: 0 - valLoss: 0.4200247526168823 - trainLoss: 0.4120101034641266\n",
      "cnt: 0 - valLoss: 0.42001551389694214 - trainLoss: 0.4120001196861267\n",
      "cnt: 0 - valLoss: 0.42000627517700195 - trainLoss: 0.41199013590812683\n",
      "cnt: 0 - valLoss: 0.41999712586402893 - trainLoss: 0.41198015213012695\n",
      "cnt: 0 - valLoss: 0.41998788714408875 - trainLoss: 0.41197019815444946\n",
      "cnt: 0 - valLoss: 0.4199787378311157 - trainLoss: 0.41196027398109436\n",
      "cnt: 0 - valLoss: 0.4199694097042084 - trainLoss: 0.41195032000541687\n",
      "cnt: 0 - valLoss: 0.4199603199958801 - trainLoss: 0.41194039583206177\n",
      "cnt: 0 - valLoss: 0.41995078325271606 - trainLoss: 0.41193053126335144\n",
      "cnt: 0 - valLoss: 0.4199417531490326 - trainLoss: 0.41192060708999634\n",
      "cnt: 0 - valLoss: 0.4199327826499939 - trainLoss: 0.41191062331199646\n",
      "cnt: 0 - valLoss: 0.4199235439300537 - trainLoss: 0.41190072894096375\n",
      "cnt: 0 - valLoss: 0.4199145436286926 - trainLoss: 0.41189077496528625\n",
      "cnt: 0 - valLoss: 0.41990554332733154 - trainLoss: 0.41188088059425354\n",
      "cnt: 0 - valLoss: 0.41989627480506897 - trainLoss: 0.4118710160255432\n",
      "cnt: 0 - valLoss: 0.4198874235153198 - trainLoss: 0.4118610918521881\n",
      "cnt: 0 - valLoss: 0.41987839341163635 - trainLoss: 0.4118511378765106\n",
      "cnt: 0 - valLoss: 0.4198695421218872 - trainLoss: 0.41184118390083313\n",
      "cnt: 0 - valLoss: 0.4198603928089142 - trainLoss: 0.4118312895298004\n",
      "cnt: 0 - valLoss: 0.41985151171684265 - trainLoss: 0.41182130575180054\n",
      "cnt: 0 - valLoss: 0.4198426604270935 - trainLoss: 0.4118114411830902\n",
      "cnt: 0 - valLoss: 0.4198334515094757 - trainLoss: 0.4118015170097351\n",
      "cnt: 0 - valLoss: 0.41982460021972656 - trainLoss: 0.41179159283638\n",
      "cnt: 0 - valLoss: 0.4198157787322998 - trainLoss: 0.4117816388607025\n",
      "cnt: 0 - valLoss: 0.4198068380355835 - trainLoss: 0.4117717146873474\n",
      "cnt: 0 - valLoss: 0.41979798674583435 - trainLoss: 0.4117618501186371\n",
      "cnt: 0 - valLoss: 0.4197888970375061 - trainLoss: 0.411751925945282\n",
      "cnt: 0 - valLoss: 0.41978007555007935 - trainLoss: 0.4117420017719269\n",
      "cnt: 0 - valLoss: 0.41977113485336304 - trainLoss: 0.41173210740089417\n",
      "cnt: 0 - valLoss: 0.4197620749473572 - trainLoss: 0.41172224283218384\n",
      "cnt: 0 - valLoss: 0.4197533130645752 - trainLoss: 0.4117123484611511\n",
      "cnt: 0 - valLoss: 0.4197445809841156 - trainLoss: 0.411702424287796\n",
      "cnt: 0 - valLoss: 0.41973546147346497 - trainLoss: 0.4116925895214081\n",
      "cnt: 0 - valLoss: 0.419726699590683 - trainLoss: 0.4116826057434082\n",
      "cnt: 0 - valLoss: 0.41971784830093384 - trainLoss: 0.4116727411746979\n",
      "cnt: 0 - valLoss: 0.41970881819725037 - trainLoss: 0.41166284680366516\n",
      "cnt: 0 - valLoss: 0.41969984769821167 - trainLoss: 0.4116530418395996\n",
      "cnt: 0 - valLoss: 0.41969117522239685 - trainLoss: 0.4116431474685669\n",
      "cnt: 0 - valLoss: 0.41968223452568054 - trainLoss: 0.41163331270217896\n",
      "cnt: 0 - valLoss: 0.41967353224754333 - trainLoss: 0.41162338852882385\n",
      "cnt: 0 - valLoss: 0.4196646213531494 - trainLoss: 0.4116135239601135\n",
      "cnt: 0 - valLoss: 0.4196555018424988 - trainLoss: 0.4116036891937256\n",
      "cnt: 0 - valLoss: 0.41964682936668396 - trainLoss: 0.41159379482269287\n",
      "cnt: 0 - valLoss: 0.4196380376815796 - trainLoss: 0.41158393025398254\n",
      "cnt: 0 - valLoss: 0.41962918639183044 - trainLoss: 0.4115740954875946\n",
      "cnt: 0 - valLoss: 0.41962042450904846 - trainLoss: 0.41156426072120667\n",
      "cnt: 0 - valLoss: 0.41961145401000977 - trainLoss: 0.41155439615249634\n",
      "cnt: 0 - valLoss: 0.41960248351097107 - trainLoss: 0.4115445613861084\n",
      "cnt: 0 - valLoss: 0.419593870639801 - trainLoss: 0.41153472661972046\n",
      "cnt: 0 - valLoss: 0.41958513855934143 - trainLoss: 0.4115248918533325\n",
      "cnt: 0 - valLoss: 0.4195762276649475 - trainLoss: 0.4115150570869446\n",
      "cnt: 0 - valLoss: 0.41956740617752075 - trainLoss: 0.41150522232055664\n",
      "cnt: 0 - valLoss: 0.41955870389938354 - trainLoss: 0.4114954173564911\n",
      "cnt: 0 - valLoss: 0.4195497930049896 - trainLoss: 0.41148561239242554\n",
      "cnt: 0 - valLoss: 0.41954120993614197 - trainLoss: 0.4114757478237152\n",
      "cnt: 0 - valLoss: 0.4195324182510376 - trainLoss: 0.41146597266197205\n",
      "cnt: 0 - valLoss: 0.4195234775543213 - trainLoss: 0.41145607829093933\n",
      "cnt: 0 - valLoss: 0.419514924287796 - trainLoss: 0.41144630312919617\n",
      "cnt: 0 - valLoss: 0.4195060729980469 - trainLoss: 0.4114364683628082\n",
      "cnt: 0 - valLoss: 0.419497549533844 - trainLoss: 0.41142669320106506\n",
      "cnt: 0 - valLoss: 0.4194886386394501 - trainLoss: 0.4114168584346771\n",
      "cnt: 0 - valLoss: 0.4194797873497009 - trainLoss: 0.4114070236682892\n",
      "cnt: 0 - valLoss: 0.41947126388549805 - trainLoss: 0.411397248506546\n",
      "cnt: 0 - valLoss: 0.41946274042129517 - trainLoss: 0.4113874137401581\n",
      "cnt: 0 - valLoss: 0.419453889131546 - trainLoss: 0.4113776385784149\n",
      "cnt: 0 - valLoss: 0.4194452166557312 - trainLoss: 0.41136786341667175\n",
      "cnt: 0 - valLoss: 0.41943633556365967 - trainLoss: 0.4113580286502838\n",
      "cnt: 0 - valLoss: 0.41942736506462097 - trainLoss: 0.4113481938838959\n",
      "cnt: 0 - valLoss: 0.41941869258880615 - trainLoss: 0.4113384187221527\n",
      "cnt: 0 - valLoss: 0.41940996050834656 - trainLoss: 0.41132864356040955\n",
      "cnt: 0 - valLoss: 0.4194009602069855 - trainLoss: 0.41131889820098877\n",
      "cnt: 0 - valLoss: 0.41939228773117065 - trainLoss: 0.4113091826438904\n",
      "cnt: 0 - valLoss: 0.4193834662437439 - trainLoss: 0.4112994372844696\n",
      "cnt: 0 - valLoss: 0.41937437653541565 - trainLoss: 0.41128969192504883\n",
      "cnt: 0 - valLoss: 0.41936561465263367 - trainLoss: 0.4112800061702728\n",
      "cnt: 0 - valLoss: 0.4193568527698517 - trainLoss: 0.4112703502178192\n",
      "cnt: 0 - valLoss: 0.4193478226661682 - trainLoss: 0.4112606644630432\n",
      "cnt: 0 - valLoss: 0.41933906078338623 - trainLoss: 0.4112509787082672\n",
      "cnt: 0 - valLoss: 0.41933026909828186 - trainLoss: 0.4112412929534912\n",
      "cnt: 0 - valLoss: 0.4193209409713745 - trainLoss: 0.4112316071987152\n",
      "cnt: 0 - valLoss: 0.4193119406700134 - trainLoss: 0.4112219214439392\n",
      "cnt: 0 - valLoss: 0.4193030893802643 - trainLoss: 0.4112122356891632\n",
      "cnt: 0 - valLoss: 0.41929396986961365 - trainLoss: 0.4112025499343872\n",
      "cnt: 0 - valLoss: 0.4192851185798645 - trainLoss: 0.411192923784256\n",
      "cnt: 0 - valLoss: 0.41927608847618103 - trainLoss: 0.41118329763412476\n",
      "cnt: 0 - valLoss: 0.41926705837249756 - trainLoss: 0.41117361187934875\n",
      "cnt: 0 - valLoss: 0.419258177280426 - trainLoss: 0.41116395592689514\n",
      "cnt: 0 - valLoss: 0.41924935579299927 - trainLoss: 0.4111543297767639\n",
      "cnt: 0 - valLoss: 0.41924020648002625 - trainLoss: 0.4111446440219879\n",
      "cnt: 0 - valLoss: 0.4192313551902771 - trainLoss: 0.4111350178718567\n",
      "cnt: 0 - valLoss: 0.4192224442958832 - trainLoss: 0.41112539172172546\n",
      "cnt: 0 - valLoss: 0.41921359300613403 - trainLoss: 0.41111573576927185\n",
      "cnt: 0 - valLoss: 0.4192042052745819 - trainLoss: 0.4111061096191406\n",
      "cnt: 0 - valLoss: 0.4191952347755432 - trainLoss: 0.4110965132713318\n",
      "cnt: 0 - valLoss: 0.4191862642765045 - trainLoss: 0.41108694672584534\n",
      "cnt: 0 - valLoss: 0.41917702555656433 - trainLoss: 0.4110772907733917\n",
      "cnt: 0 - valLoss: 0.4191679060459137 - trainLoss: 0.41106775403022766\n",
      "cnt: 0 - valLoss: 0.41915884613990784 - trainLoss: 0.4110582172870636\n",
      "cnt: 0 - valLoss: 0.41914981603622437 - trainLoss: 0.4110487401485443\n",
      "cnt: 0 - valLoss: 0.419140487909317 - trainLoss: 0.41103920340538025\n",
      "cnt: 0 - valLoss: 0.41913139820098877 - trainLoss: 0.41102972626686096\n",
      "cnt: 0 - valLoss: 0.4191223978996277 - trainLoss: 0.4110202193260193\n",
      "cnt: 0 - valLoss: 0.41911330819129944 - trainLoss: 0.4110107421875\n",
      "cnt: 0 - valLoss: 0.4191039502620697 - trainLoss: 0.4110012948513031\n",
      "cnt: 0 - valLoss: 0.41909492015838623 - trainLoss: 0.4109918773174286\n",
      "cnt: 0 - valLoss: 0.41908594965934753 - trainLoss: 0.4109823703765869\n",
      "cnt: 0 - valLoss: 0.41907671093940735 - trainLoss: 0.41097292304039\n",
      "cnt: 0 - valLoss: 0.4190676212310791 - trainLoss: 0.4109635055065155\n",
      "cnt: 0 - valLoss: 0.41905853152275085 - trainLoss: 0.41095399856567383\n",
      "cnt: 0 - valLoss: 0.41904932260513306 - trainLoss: 0.41094452142715454\n",
      "cnt: 0 - valLoss: 0.41904011368751526 - trainLoss: 0.4109351336956024\n",
      "cnt: 0 - valLoss: 0.4190310835838318 - trainLoss: 0.41092565655708313\n",
      "cnt: 0 - valLoss: 0.4190220534801483 - trainLoss: 0.41091620922088623\n",
      "cnt: 0 - valLoss: 0.41901302337646484 - trainLoss: 0.41090676188468933\n",
      "cnt: 0 - valLoss: 0.41900381445884705 - trainLoss: 0.41089728474617004\n",
      "cnt: 0 - valLoss: 0.4189947545528412 - trainLoss: 0.4108878970146179\n",
      "cnt: 0 - valLoss: 0.4189856946468353 - trainLoss: 0.41087839007377625\n",
      "cnt: 0 - valLoss: 0.4189763367176056 - trainLoss: 0.4108690023422241\n",
      "cnt: 0 - valLoss: 0.41896724700927734 - trainLoss: 0.4108595848083496\n",
      "cnt: 0 - valLoss: 0.41895800828933716 - trainLoss: 0.4108501374721527\n",
      "cnt: 0 - valLoss: 0.4189489781856537 - trainLoss: 0.4108407497406006\n",
      "cnt: 0 - valLoss: 0.41894006729125977 - trainLoss: 0.4108313024044037\n",
      "cnt: 0 - valLoss: 0.4189310073852539 - trainLoss: 0.4108218252658844\n",
      "cnt: 0 - valLoss: 0.41892215609550476 - trainLoss: 0.4108123183250427\n",
      "cnt: 0 - valLoss: 0.4189128577709198 - trainLoss: 0.4108029007911682\n",
      "cnt: 0 - valLoss: 0.41890403628349304 - trainLoss: 0.41079339385032654\n",
      "cnt: 0 - valLoss: 0.41889524459838867 - trainLoss: 0.41078391671180725\n",
      "cnt: 0 - valLoss: 0.4188862144947052 - trainLoss: 0.4107745289802551\n",
      "cnt: 0 - valLoss: 0.4188772439956665 - trainLoss: 0.41076502203941345\n",
      "cnt: 0 - valLoss: 0.41886839270591736 - trainLoss: 0.41075554490089417\n",
      "cnt: 0 - valLoss: 0.4188593924045563 - trainLoss: 0.41074609756469727\n",
      "cnt: 0 - valLoss: 0.41885045170783997 - trainLoss: 0.410736620426178\n",
      "cnt: 0 - valLoss: 0.4188413918018341 - trainLoss: 0.4107271730899811\n",
      "cnt: 0 - valLoss: 0.4188327193260193 - trainLoss: 0.41071775555610657\n",
      "cnt: 0 - valLoss: 0.41882386803627014 - trainLoss: 0.41070830821990967\n",
      "cnt: 0 - valLoss: 0.4188150465488434 - trainLoss: 0.4106988310813904\n",
      "cnt: 0 - valLoss: 0.4188063144683838 - trainLoss: 0.41068941354751587\n",
      "cnt: 0 - valLoss: 0.41879740357398987 - trainLoss: 0.4106799364089966\n",
      "cnt: 0 - valLoss: 0.41878849267959595 - trainLoss: 0.41067054867744446\n",
      "cnt: 0 - valLoss: 0.4187796413898468 - trainLoss: 0.41066107153892517\n",
      "cnt: 0 - valLoss: 0.41877102851867676 - trainLoss: 0.41065168380737305\n",
      "cnt: 0 - valLoss: 0.41876208782196045 - trainLoss: 0.41064217686653137\n",
      "cnt: 0 - valLoss: 0.41875335574150085 - trainLoss: 0.41063275933265686\n",
      "cnt: 0 - valLoss: 0.4187448024749756 - trainLoss: 0.41062331199645996\n",
      "cnt: 0 - valLoss: 0.4187360107898712 - trainLoss: 0.41061392426490784\n",
      "cnt: 0 - valLoss: 0.41872698068618774 - trainLoss: 0.4106045365333557\n",
      "cnt: 0 - valLoss: 0.418718159198761 - trainLoss: 0.4105951488018036\n",
      "cnt: 0 - valLoss: 0.4187094569206238 - trainLoss: 0.41058579087257385\n",
      "cnt: 0 - valLoss: 0.4187004864215851 - trainLoss: 0.4105764627456665\n",
      "cnt: 0 - valLoss: 0.4186917543411255 - trainLoss: 0.4105670750141144\n",
      "cnt: 0 - valLoss: 0.41868293285369873 - trainLoss: 0.4105577766895294\n",
      "cnt: 0 - valLoss: 0.41867417097091675 - trainLoss: 0.4105483889579773\n",
      "cnt: 0 - valLoss: 0.41866534948349 - trainLoss: 0.41053903102874756\n",
      "cnt: 0 - valLoss: 0.41865673661231995 - trainLoss: 0.4105297029018402\n",
      "cnt: 0 - valLoss: 0.41864755749702454 - trainLoss: 0.41052040457725525\n",
      "cnt: 0 - valLoss: 0.4186389744281769 - trainLoss: 0.4105110764503479\n",
      "cnt: 0 - valLoss: 0.41863012313842773 - trainLoss: 0.41050171852111816\n",
      "cnt: 0 - valLoss: 0.41862156987190247 - trainLoss: 0.4104924201965332\n",
      "cnt: 0 - valLoss: 0.4186125099658966 - trainLoss: 0.41048315167427063\n",
      "cnt: 0 - valLoss: 0.41860389709472656 - trainLoss: 0.4104737937450409\n",
      "cnt: 0 - valLoss: 0.41859501600265503 - trainLoss: 0.41046446561813354\n",
      "cnt: 0 - valLoss: 0.4185863137245178 - trainLoss: 0.4104551672935486\n",
      "cnt: 0 - valLoss: 0.4185773730278015 - trainLoss: 0.4104458689689636\n",
      "cnt: 0 - valLoss: 0.418568879365921 - trainLoss: 0.41043657064437866\n",
      "cnt: 0 - valLoss: 0.41856011748313904 - trainLoss: 0.4104273021221161\n",
      "cnt: 0 - valLoss: 0.41855114698410034 - trainLoss: 0.41041800379753113\n",
      "cnt: 0 - valLoss: 0.41854241490364075 - trainLoss: 0.41040870547294617\n",
      "cnt: 0 - valLoss: 0.4185338616371155 - trainLoss: 0.4103994071483612\n",
      "cnt: 0 - valLoss: 0.41852492094039917 - trainLoss: 0.41039010882377625\n",
      "cnt: 0 - valLoss: 0.41851601004600525 - trainLoss: 0.41038084030151367\n",
      "cnt: 0 - valLoss: 0.4185073673725128 - trainLoss: 0.4103715419769287\n",
      "cnt: 0 - valLoss: 0.41849833726882935 - trainLoss: 0.41036224365234375\n",
      "cnt: 0 - valLoss: 0.4184896945953369 - trainLoss: 0.4103529155254364\n",
      "cnt: 0 - valLoss: 0.4184807538986206 - trainLoss: 0.41034355759620667\n",
      "cnt: 0 - valLoss: 0.4184721112251282 - trainLoss: 0.4103343188762665\n",
      "cnt: 0 - valLoss: 0.4184631407260895 - trainLoss: 0.41032499074935913\n",
      "cnt: 0 - valLoss: 0.4184543192386627 - trainLoss: 0.41031569242477417\n",
      "cnt: 0 - valLoss: 0.4184456765651703 - trainLoss: 0.4103063941001892\n",
      "cnt: 0 - valLoss: 0.41843658685684204 - trainLoss: 0.4102971851825714\n",
      "cnt: 0 - valLoss: 0.4184280037879944 - trainLoss: 0.41028791666030884\n",
      "cnt: 0 - valLoss: 0.41841915249824524 - trainLoss: 0.4102786183357239\n",
      "cnt: 0 - valLoss: 0.41841021180152893 - trainLoss: 0.4102693796157837\n",
      "cnt: 0 - valLoss: 0.4184015989303589 - trainLoss: 0.4102601408958435\n",
      "cnt: 0 - valLoss: 0.4183928668498993 - trainLoss: 0.41025087237358093\n",
      "cnt: 0 - valLoss: 0.41838401556015015 - trainLoss: 0.41024160385131836\n",
      "cnt: 0 - valLoss: 0.41837525367736816 - trainLoss: 0.41023239493370056\n",
      "cnt: 0 - valLoss: 0.41836652159690857 - trainLoss: 0.410223126411438\n",
      "cnt: 0 - valLoss: 0.4183579385280609 - trainLoss: 0.4102139174938202\n",
      "cnt: 0 - valLoss: 0.4183490574359894 - trainLoss: 0.41020461916923523\n",
      "cnt: 0 - valLoss: 0.4183403551578522 - trainLoss: 0.41019541025161743\n",
      "cnt: 0 - valLoss: 0.4183317720890045 - trainLoss: 0.41018617153167725\n",
      "cnt: 0 - valLoss: 0.418322890996933 - trainLoss: 0.41017693281173706\n",
      "cnt: 0 - valLoss: 0.4183141887187958 - trainLoss: 0.41016775369644165\n",
      "cnt: 0 - valLoss: 0.41830572485923767 - trainLoss: 0.41015851497650146\n",
      "cnt: 0 - valLoss: 0.4182968735694885 - trainLoss: 0.41014930605888367\n",
      "cnt: 0 - valLoss: 0.41828814148902893 - trainLoss: 0.41014009714126587\n",
      "cnt: 0 - valLoss: 0.41827961802482605 - trainLoss: 0.41013094782829285\n",
      "cnt: 0 - valLoss: 0.41827094554901123 - trainLoss: 0.41012173891067505\n",
      "cnt: 0 - valLoss: 0.4182620942592621 - trainLoss: 0.410112589597702\n",
      "cnt: 0 - valLoss: 0.418253630399704 - trainLoss: 0.4101034104824066\n",
      "cnt: 0 - valLoss: 0.4182448387145996 - trainLoss: 0.4100942611694336\n",
      "cnt: 0 - valLoss: 0.41823598742485046 - trainLoss: 0.4100850522518158\n",
      "cnt: 0 - valLoss: 0.4182274639606476 - trainLoss: 0.41007593274116516\n",
      "cnt: 0 - valLoss: 0.41821879148483276 - trainLoss: 0.41006675362586975\n",
      "cnt: 0 - valLoss: 0.4182099997997284 - trainLoss: 0.4100576341152191\n",
      "cnt: 0 - valLoss: 0.41820162534713745 - trainLoss: 0.4100484251976013\n",
      "cnt: 0 - valLoss: 0.41819295287132263 - trainLoss: 0.4100392758846283\n",
      "cnt: 0 - valLoss: 0.4181841015815735 - trainLoss: 0.41003015637397766\n",
      "cnt: 0 - valLoss: 0.41817545890808105 - trainLoss: 0.410021036863327\n",
      "cnt: 0 - valLoss: 0.4181671142578125 - trainLoss: 0.4100119173526764\n",
      "cnt: 0 - valLoss: 0.4181583821773529 - trainLoss: 0.410002738237381\n",
      "cnt: 0 - valLoss: 0.41814976930618286 - trainLoss: 0.40999358892440796\n",
      "cnt: 0 - valLoss: 0.4181411862373352 - trainLoss: 0.4099844694137573\n",
      "cnt: 0 - valLoss: 0.41813260316848755 - trainLoss: 0.4099753499031067\n",
      "cnt: 0 - valLoss: 0.4181240200996399 - trainLoss: 0.40996620059013367\n",
      "cnt: 0 - valLoss: 0.41811543703079224 - trainLoss: 0.40995705127716064\n",
      "cnt: 0 - valLoss: 0.4181070029735565 - trainLoss: 0.40994793176651\n",
      "cnt: 0 - valLoss: 0.41809847950935364 - trainLoss: 0.4099388122558594\n",
      "cnt: 0 - valLoss: 0.4180898368358612 - trainLoss: 0.40992969274520874\n",
      "cnt: 0 - valLoss: 0.4180813431739807 - trainLoss: 0.4099205732345581\n",
      "cnt: 0 - valLoss: 0.4180726706981659 - trainLoss: 0.40991145372390747\n",
      "cnt: 0 - valLoss: 0.4180643558502197 - trainLoss: 0.4099023640155792\n",
      "cnt: 0 - valLoss: 0.41805553436279297 - trainLoss: 0.4098932445049286\n",
      "cnt: 0 - valLoss: 0.4180470407009125 - trainLoss: 0.40988409519195557\n",
      "cnt: 0 - valLoss: 0.4180384576320648 - trainLoss: 0.40987497568130493\n",
      "cnt: 0 - valLoss: 0.41803014278411865 - trainLoss: 0.4098658859729767\n",
      "cnt: 0 - valLoss: 0.418021559715271 - trainLoss: 0.40985676646232605\n",
      "cnt: 0 - valLoss: 0.4180130660533905 - trainLoss: 0.4098476767539978\n",
      "cnt: 0 - valLoss: 0.41800448298454285 - trainLoss: 0.40983861684799194\n",
      "cnt: 0 - valLoss: 0.4179958701133728 - trainLoss: 0.4098294973373413\n",
      "cnt: 0 - valLoss: 0.4179876148700714 - trainLoss: 0.40982040762901306\n",
      "cnt: 0 - valLoss: 0.41797909140586853 - trainLoss: 0.4098113179206848\n",
      "cnt: 0 - valLoss: 0.4179705083370209 - trainLoss: 0.40980222821235657\n",
      "cnt: 0 - valLoss: 0.41796204447746277 - trainLoss: 0.4097931385040283\n",
      "cnt: 0 - valLoss: 0.4179534912109375 - trainLoss: 0.4097840487957001\n",
      "cnt: 0 - valLoss: 0.41794511675834656 - trainLoss: 0.4097749888896942\n",
      "cnt: 0 - valLoss: 0.4179367125034332 - trainLoss: 0.40976592898368835\n",
      "cnt: 0 - valLoss: 0.4179283082485199 - trainLoss: 0.4097568392753601\n",
      "cnt: 0 - valLoss: 0.4179191291332245 - trainLoss: 0.40974777936935425\n",
      "cnt: 0 - valLoss: 0.4179104268550873 - trainLoss: 0.409738689661026\n",
      "cnt: 0 - valLoss: 0.41790181398391724 - trainLoss: 0.409729540348053\n",
      "cnt: 0 - valLoss: 0.4178934097290039 - trainLoss: 0.4097204804420471\n",
      "cnt: 0 - valLoss: 0.4178849458694458 - trainLoss: 0.4097113609313965\n",
      "cnt: 0 - valLoss: 0.41787663102149963 - trainLoss: 0.40970227122306824\n",
      "cnt: 0 - valLoss: 0.41786807775497437 - trainLoss: 0.4096932113170624\n",
      "cnt: 0 - valLoss: 0.4178590178489685 - trainLoss: 0.4096841514110565\n",
      "cnt: 0 - valLoss: 0.4178506135940552 - trainLoss: 0.40967512130737305\n",
      "cnt: 0 - valLoss: 0.41784197092056274 - trainLoss: 0.40966612100601196\n",
      "cnt: 0 - valLoss: 0.417833149433136 - trainLoss: 0.4096570909023285\n",
      "cnt: 0 - valLoss: 0.41782456636428833 - trainLoss: 0.4096481204032898\n",
      "cnt: 0 - valLoss: 0.4178159534931183 - trainLoss: 0.4096391201019287\n",
      "cnt: 0 - valLoss: 0.4178074300289154 - trainLoss: 0.4096301198005676\n",
      "cnt: 0 - valLoss: 0.41779863834381104 - trainLoss: 0.40962114930152893\n",
      "cnt: 0 - valLoss: 0.41779035329818726 - trainLoss: 0.40961217880249023\n",
      "cnt: 0 - valLoss: 0.4177818298339844 - trainLoss: 0.40960320830345154\n",
      "cnt: 0 - valLoss: 0.41777336597442627 - trainLoss: 0.40959420800209045\n",
      "cnt: 0 - valLoss: 0.41776466369628906 - trainLoss: 0.40958526730537415\n",
      "cnt: 0 - valLoss: 0.41775593161582947 - trainLoss: 0.40957632660865784\n",
      "cnt: 0 - valLoss: 0.4177473187446594 - trainLoss: 0.40956735610961914\n",
      "cnt: 0 - valLoss: 0.4177388548851013 - trainLoss: 0.40955841541290283\n",
      "cnt: 0 - valLoss: 0.4177302122116089 - trainLoss: 0.4095494747161865\n",
      "cnt: 0 - valLoss: 0.4177224338054657 - trainLoss: 0.4095405042171478\n",
      "cnt: 0 - valLoss: 0.4177137315273285 - trainLoss: 0.40953153371810913\n",
      "cnt: 0 - valLoss: 0.4177049398422241 - trainLoss: 0.4095226228237152\n",
      "cnt: 0 - valLoss: 0.4176965653896332 - trainLoss: 0.4095136821269989\n",
      "cnt: 0 - valLoss: 0.4176877737045288 - trainLoss: 0.4095047116279602\n",
      "cnt: 0 - valLoss: 0.4176793396472931 - trainLoss: 0.40949583053588867\n",
      "cnt: 0 - valLoss: 0.4176708459854126 - trainLoss: 0.40948686003685\n",
      "cnt: 0 - valLoss: 0.417662113904953 - trainLoss: 0.40947794914245605\n",
      "cnt: 0 - valLoss: 0.41765421628952026 - trainLoss: 0.40946900844573975\n",
      "cnt: 0 - valLoss: 0.41764557361602783 - trainLoss: 0.4094600975513458\n",
      "cnt: 0 - valLoss: 0.4176369309425354 - trainLoss: 0.4094511866569519\n",
      "cnt: 0 - valLoss: 0.41762855648994446 - trainLoss: 0.4094422459602356\n",
      "cnt: 0 - valLoss: 0.4176201820373535 - trainLoss: 0.4094333052635193\n",
      "cnt: 0 - valLoss: 0.41761136054992676 - trainLoss: 0.40942439436912537\n",
      "cnt: 0 - valLoss: 0.41760361194610596 - trainLoss: 0.40941545367240906\n",
      "cnt: 0 - valLoss: 0.417594850063324 - trainLoss: 0.4094066023826599\n",
      "cnt: 0 - valLoss: 0.4175860285758972 - trainLoss: 0.4093976616859436\n",
      "cnt: 0 - valLoss: 0.4175783395767212 - trainLoss: 0.4093887507915497\n",
      "cnt: 0 - valLoss: 0.41756975650787354 - trainLoss: 0.40937986969947815\n",
      "cnt: 0 - valLoss: 0.4175606369972229 - trainLoss: 0.4093709886074066\n",
      "cnt: 0 - valLoss: 0.4175529479980469 - trainLoss: 0.4093620479106903\n",
      "cnt: 0 - valLoss: 0.417544424533844 - trainLoss: 0.40935319662094116\n",
      "cnt: 0 - valLoss: 0.4175357222557068 - trainLoss: 0.40934431552886963\n",
      "cnt: 0 - valLoss: 0.41752806305885315 - trainLoss: 0.4093354344367981\n",
      "cnt: 0 - valLoss: 0.4175189733505249 - trainLoss: 0.4093264937400818\n",
      "cnt: 0 - valLoss: 0.4175104796886444 - trainLoss: 0.40931764245033264\n",
      "cnt: 0 - valLoss: 0.4175017178058624 - trainLoss: 0.4093087315559387\n",
      "cnt: 0 - valLoss: 0.4174940884113312 - trainLoss: 0.4092998802661896\n",
      "cnt: 0 - valLoss: 0.4174852967262268 - trainLoss: 0.40929102897644043\n",
      "cnt: 0 - valLoss: 0.41747647523880005 - trainLoss: 0.4092821478843689\n",
      "cnt: 0 - valLoss: 0.4174688458442688 - trainLoss: 0.40927332639694214\n",
      "cnt: 0 - valLoss: 0.417460173368454 - trainLoss: 0.4092644453048706\n",
      "cnt: 0 - valLoss: 0.4174516797065735 - trainLoss: 0.40925559401512146\n",
      "cnt: 0 - valLoss: 0.4174438416957855 - trainLoss: 0.4092467725276947\n",
      "cnt: 0 - valLoss: 0.41743531823158264 - trainLoss: 0.40923792123794556\n",
      "cnt: 0 - valLoss: 0.41742658615112305 - trainLoss: 0.409229040145874\n",
      "cnt: 0 - valLoss: 0.4174182415008545 - trainLoss: 0.40922021865844727\n",
      "cnt: 0 - valLoss: 0.4174096882343292 - trainLoss: 0.4092113673686981\n",
      "cnt: 0 - valLoss: 0.417400985956192 - trainLoss: 0.409202516078949\n",
      "cnt: 0 - valLoss: 0.41739290952682495 - trainLoss: 0.40919366478919983\n",
      "cnt: 0 - valLoss: 0.41738396883010864 - trainLoss: 0.40918484330177307\n",
      "cnt: 0 - valLoss: 0.4173761308193207 - trainLoss: 0.4091760218143463\n",
      "cnt: 0 - valLoss: 0.4173673093318939 - trainLoss: 0.40916717052459717\n",
      "cnt: 0 - valLoss: 0.4173588156700134 - trainLoss: 0.4091583788394928\n",
      "cnt: 0 - valLoss: 0.4173509180545807 - trainLoss: 0.40914952754974365\n",
      "cnt: 0 - valLoss: 0.41734206676483154 - trainLoss: 0.4091407060623169\n",
      "cnt: 0 - valLoss: 0.4173339009284973 - trainLoss: 0.40913188457489014\n",
      "cnt: 0 - valLoss: 0.41732513904571533 - trainLoss: 0.40912309288978577\n",
      "cnt: 0 - valLoss: 0.41731661558151245 - trainLoss: 0.40911421179771423\n",
      "cnt: 0 - valLoss: 0.4173088073730469 - trainLoss: 0.40910542011260986\n",
      "cnt: 0 - valLoss: 0.41730010509490967 - trainLoss: 0.4090966284275055\n",
      "cnt: 0 - valLoss: 0.41729190945625305 - trainLoss: 0.40908780694007874\n",
      "cnt: 0 - valLoss: 0.41728341579437256 - trainLoss: 0.40907904505729675\n",
      "cnt: 0 - valLoss: 0.4172748327255249 - trainLoss: 0.4090701937675476\n",
      "cnt: 0 - valLoss: 0.4172668159008026 - trainLoss: 0.4090614318847656\n",
      "cnt: 0 - valLoss: 0.41725820302963257 - trainLoss: 0.40905264019966125\n",
      "cnt: 0 - valLoss: 0.41725000739097595 - trainLoss: 0.4090438187122345\n",
      "cnt: 0 - valLoss: 0.41724154353141785 - trainLoss: 0.4090350270271301\n",
      "cnt: 0 - valLoss: 0.4172331392765045 - trainLoss: 0.40902623534202576\n",
      "cnt: 0 - valLoss: 0.41722509264945984 - trainLoss: 0.4090174734592438\n",
      "cnt: 0 - valLoss: 0.4172166883945465 - trainLoss: 0.4090087115764618\n",
      "cnt: 0 - valLoss: 0.4172079265117645 - trainLoss: 0.4089999198913574\n",
      "cnt: 0 - valLoss: 0.41719967126846313 - trainLoss: 0.40899115800857544\n",
      "cnt: 0 - valLoss: 0.4171912372112274 - trainLoss: 0.40898236632347107\n",
      "cnt: 0 - valLoss: 0.41718342900276184 - trainLoss: 0.4089736044406891\n",
      "cnt: 0 - valLoss: 0.4171748459339142 - trainLoss: 0.4089648127555847\n",
      "cnt: 0 - valLoss: 0.4171663522720337 - trainLoss: 0.4089560806751251\n",
      "cnt: 0 - valLoss: 0.4171580374240875 - trainLoss: 0.40894731879234314\n",
      "cnt: 0 - valLoss: 0.4171495735645294 - trainLoss: 0.40893858671188354\n",
      "cnt: 0 - valLoss: 0.4171411097049713 - trainLoss: 0.4089297652244568\n",
      "cnt: 0 - valLoss: 0.41713300347328186 - trainLoss: 0.4089210629463196\n",
      "cnt: 0 - valLoss: 0.41712459921836853 - trainLoss: 0.40891233086586\n",
      "cnt: 0 - valLoss: 0.41711628437042236 - trainLoss: 0.408903568983078\n",
      "cnt: 0 - valLoss: 0.41710785031318665 - trainLoss: 0.408894807100296\n",
      "cnt: 0 - valLoss: 0.4170994162559509 - trainLoss: 0.40888604521751404\n",
      "cnt: 0 - valLoss: 0.417091429233551 - trainLoss: 0.40887731313705444\n",
      "cnt: 0 - valLoss: 0.4170829951763153 - trainLoss: 0.40886858105659485\n",
      "cnt: 0 - valLoss: 0.4170742630958557 - trainLoss: 0.40885984897613525\n",
      "cnt: 0 - valLoss: 0.417066365480423 - trainLoss: 0.40885114669799805\n",
      "cnt: 0 - valLoss: 0.41705796122550964 - trainLoss: 0.40884241461753845\n",
      "cnt: 0 - valLoss: 0.4170490801334381 - trainLoss: 0.40883371233940125\n",
      "cnt: 0 - valLoss: 0.4170416295528412 - trainLoss: 0.4088250398635864\n",
      "cnt: 0 - valLoss: 0.417032927274704 - trainLoss: 0.40881630778312683\n",
      "cnt: 0 - valLoss: 0.4170245826244354 - trainLoss: 0.4088076055049896\n",
      "cnt: 0 - valLoss: 0.41701680421829224 - trainLoss: 0.4087989926338196\n",
      "cnt: 0 - valLoss: 0.4170077443122864 - trainLoss: 0.40879034996032715\n",
      "cnt: 0 - valLoss: 0.41700005531311035 - trainLoss: 0.40878167748451233\n",
      "cnt: 0 - valLoss: 0.41699162125587463 - trainLoss: 0.4087730348110199\n",
      "cnt: 0 - valLoss: 0.4169827997684479 - trainLoss: 0.40876439213752747\n",
      "cnt: 0 - valLoss: 0.416975200176239 - trainLoss: 0.4087557792663574\n",
      "cnt: 0 - valLoss: 0.4169667661190033 - trainLoss: 0.4087471067905426\n",
      "cnt: 0 - valLoss: 0.41695788502693176 - trainLoss: 0.40873846411705017\n",
      "cnt: 0 - valLoss: 0.4169505834579468 - trainLoss: 0.4087298512458801\n",
      "cnt: 0 - valLoss: 0.4169416129589081 - trainLoss: 0.4087212383747101\n",
      "cnt: 0 - valLoss: 0.41693323850631714 - trainLoss: 0.40871259570121765\n",
      "cnt: 0 - valLoss: 0.41692572832107544 - trainLoss: 0.4087039530277252\n",
      "cnt: 0 - valLoss: 0.41691693663597107 - trainLoss: 0.4086953103542328\n",
      "cnt: 0 - valLoss: 0.4169085621833801 - trainLoss: 0.40868672728538513\n",
      "cnt: 0 - valLoss: 0.4169010818004608 - trainLoss: 0.4086781144142151\n",
      "cnt: 0 - valLoss: 0.4168923497200012 - trainLoss: 0.40866947174072266\n",
      "cnt: 0 - valLoss: 0.41688403487205505 - trainLoss: 0.40866076946258545\n",
      "cnt: 0 - valLoss: 0.41687679290771484 - trainLoss: 0.40865200757980347\n",
      "cnt: 0 - valLoss: 0.41686826944351196 - trainLoss: 0.40864327549934387\n",
      "cnt: 0 - valLoss: 0.4168597161769867 - trainLoss: 0.4086345136165619\n",
      "cnt: 0 - valLoss: 0.4168519675731659 - trainLoss: 0.4086258113384247\n",
      "cnt: 0 - valLoss: 0.41684389114379883 - trainLoss: 0.4086171090602875\n",
      "cnt: 0 - valLoss: 0.41683560609817505 - trainLoss: 0.40860840678215027\n",
      "cnt: 0 - valLoss: 0.4168276786804199 - trainLoss: 0.4085996747016907\n",
      "cnt: 0 - valLoss: 0.4168197214603424 - trainLoss: 0.4085909426212311\n",
      "cnt: 0 - valLoss: 0.4168112874031067 - trainLoss: 0.40858224034309387\n",
      "cnt: 0 - valLoss: 0.41680318117141724 - trainLoss: 0.4085734784603119\n",
      "cnt: 0 - valLoss: 0.4167954623699188 - trainLoss: 0.40856483578681946\n",
      "cnt: 0 - valLoss: 0.41678717732429504 - trainLoss: 0.40855610370635986\n",
      "cnt: 0 - valLoss: 0.41677895188331604 - trainLoss: 0.40854737162590027\n",
      "cnt: 0 - valLoss: 0.41677126288414 - trainLoss: 0.40853872895240784\n",
      "cnt: 0 - valLoss: 0.4167628884315491 - trainLoss: 0.40852996706962585\n",
      "cnt: 0 - valLoss: 0.4167548716068268 - trainLoss: 0.40852126479148865\n",
      "cnt: 0 - valLoss: 0.41674700379371643 - trainLoss: 0.40851259231567383\n",
      "cnt: 0 - valLoss: 0.4167386293411255 - trainLoss: 0.4085038900375366\n",
      "cnt: 0 - valLoss: 0.4167306125164032 - trainLoss: 0.4084951877593994\n",
      "cnt: 0 - valLoss: 0.41672274470329285 - trainLoss: 0.4084864854812622\n",
      "cnt: 0 - valLoss: 0.41671448945999146 - trainLoss: 0.4084778428077698\n",
      "cnt: 0 - valLoss: 0.41670647263526917 - trainLoss: 0.40846914052963257\n",
      "cnt: 0 - valLoss: 0.41669780015945435 - trainLoss: 0.40846046805381775\n",
      "cnt: 0 - valLoss: 0.4166904389858246 - trainLoss: 0.40845176577568054\n",
      "cnt: 0 - valLoss: 0.41668206453323364 - trainLoss: 0.4084431231021881\n",
      "cnt: 0 - valLoss: 0.416673481464386 - trainLoss: 0.40843454003334045\n",
      "cnt: 0 - valLoss: 0.41666579246520996 - trainLoss: 0.4084259867668152\n",
      "cnt: 0 - valLoss: 0.41665756702423096 - trainLoss: 0.4084174335002899\n",
      "cnt: 0 - valLoss: 0.4166491627693176 - trainLoss: 0.40840885043144226\n",
      "cnt: 0 - valLoss: 0.4166414737701416 - trainLoss: 0.4084003269672394\n",
      "cnt: 0 - valLoss: 0.41663315892219543 - trainLoss: 0.4083917438983917\n",
      "cnt: 0 - valLoss: 0.41662484407424927 - trainLoss: 0.40838319063186646\n",
      "cnt: 0 - valLoss: 0.41661709547042847 - trainLoss: 0.4083746671676636\n",
      "cnt: 0 - valLoss: 0.4166087508201599 - trainLoss: 0.4083661437034607\n",
      "cnt: 0 - valLoss: 0.4166005849838257 - trainLoss: 0.40835756063461304\n",
      "cnt: 0 - valLoss: 0.4165921211242676 - trainLoss: 0.40834900736808777\n",
      "cnt: 0 - valLoss: 0.41658419370651245 - trainLoss: 0.4083405137062073\n",
      "cnt: 0 - valLoss: 0.41657644510269165 - trainLoss: 0.4083319902420044\n",
      "cnt: 0 - valLoss: 0.41656774282455444 - trainLoss: 0.4083234369754791\n",
      "cnt: 0 - valLoss: 0.41656047105789185 - trainLoss: 0.40831488370895386\n",
      "cnt: 0 - valLoss: 0.41655203700065613 - trainLoss: 0.408306360244751\n",
      "cnt: 0 - valLoss: 0.4165436923503876 - trainLoss: 0.4082978665828705\n",
      "cnt: 0 - valLoss: 0.41653555631637573 - trainLoss: 0.4082893133163452\n",
      "cnt: 0 - valLoss: 0.4165273904800415 - trainLoss: 0.4082808494567871\n",
      "cnt: 0 - valLoss: 0.41651904582977295 - trainLoss: 0.40827253460884094\n",
      "cnt: 0 - valLoss: 0.41651061177253723 - trainLoss: 0.4082641899585724\n",
      "cnt: 0 - valLoss: 0.4165027141571045 - trainLoss: 0.40825584530830383\n",
      "cnt: 0 - valLoss: 0.4164942502975464 - trainLoss: 0.4082474708557129\n",
      "cnt: 0 - valLoss: 0.4164859652519226 - trainLoss: 0.4082391560077667\n",
      "cnt: 0 - valLoss: 0.41647785902023315 - trainLoss: 0.40823081135749817\n",
      "cnt: 0 - valLoss: 0.4164696931838989 - trainLoss: 0.4082224667072296\n",
      "cnt: 0 - valLoss: 0.41646116971969604 - trainLoss: 0.40821415185928345\n",
      "cnt: 0 - valLoss: 0.4164533317089081 - trainLoss: 0.4082058072090149\n",
      "cnt: 0 - valLoss: 0.4164452850818634 - trainLoss: 0.4081974923610687\n",
      "cnt: 0 - valLoss: 0.4164367616176605 - trainLoss: 0.40818920731544495\n",
      "cnt: 0 - valLoss: 0.41642889380455017 - trainLoss: 0.4081808924674988\n",
      "cnt: 0 - valLoss: 0.4164208769798279 - trainLoss: 0.408172607421875\n",
      "cnt: 0 - valLoss: 0.4164130389690399 - trainLoss: 0.40816426277160645\n",
      "cnt: 0 - valLoss: 0.4164045453071594 - trainLoss: 0.40815597772598267\n",
      "cnt: 0 - valLoss: 0.4163966178894043 - trainLoss: 0.4081477224826813\n",
      "cnt: 0 - valLoss: 0.4163888394832611 - trainLoss: 0.4081394076347351\n",
      "cnt: 0 - valLoss: 0.4163801074028015 - trainLoss: 0.40813112258911133\n",
      "cnt: 0 - valLoss: 0.4163723587989807 - trainLoss: 0.40812280774116516\n",
      "cnt: 0 - valLoss: 0.416364461183548 - trainLoss: 0.4081145226955414\n",
      "cnt: 0 - valLoss: 0.41635653376579285 - trainLoss: 0.4081061780452728\n",
      "cnt: 0 - valLoss: 0.41634729504585266 - trainLoss: 0.4080979824066162\n",
      "cnt: 0 - valLoss: 0.41633933782577515 - trainLoss: 0.40808966755867004\n",
      "cnt: 0 - valLoss: 0.41633105278015137 - trainLoss: 0.40808138251304626\n",
      "cnt: 0 - valLoss: 0.4163224697113037 - trainLoss: 0.40807315707206726\n",
      "cnt: 0 - valLoss: 0.41631415486335754 - trainLoss: 0.4080648720264435\n",
      "cnt: 0 - valLoss: 0.4163056015968323 - trainLoss: 0.4080565869808197\n",
      "cnt: 0 - valLoss: 0.4162976145744324 - trainLoss: 0.4080483913421631\n",
      "cnt: 0 - valLoss: 0.416289359331131 - trainLoss: 0.4080401360988617\n",
      "cnt: 0 - valLoss: 0.41628074645996094 - trainLoss: 0.4080318510532379\n",
      "cnt: 0 - valLoss: 0.41627246141433716 - trainLoss: 0.4080235958099365\n",
      "cnt: 0 - valLoss: 0.41626420617103577 - trainLoss: 0.40801534056663513\n",
      "cnt: 0 - valLoss: 0.41625553369522095 - trainLoss: 0.4080071449279785\n",
      "cnt: 0 - valLoss: 0.4162478744983673 - trainLoss: 0.40799885988235474\n",
      "cnt: 0 - valLoss: 0.41623952984809875 - trainLoss: 0.40799063444137573\n",
      "cnt: 0 - valLoss: 0.41623109579086304 - trainLoss: 0.40798237919807434\n",
      "cnt: 0 - valLoss: 0.4162229597568512 - trainLoss: 0.4079741835594177\n",
      "cnt: 0 - valLoss: 0.41621434688568115 - trainLoss: 0.40796589851379395\n",
      "cnt: 0 - valLoss: 0.4162060618400574 - trainLoss: 0.40795770287513733\n",
      "cnt: 0 - valLoss: 0.41619765758514404 - trainLoss: 0.40794944763183594\n",
      "cnt: 0 - valLoss: 0.4161900579929352 - trainLoss: 0.40794122219085693\n",
      "cnt: 0 - valLoss: 0.416181743144989 - trainLoss: 0.4079330265522003\n",
      "cnt: 0 - valLoss: 0.41617342829704285 - trainLoss: 0.4079247713088989\n",
      "cnt: 0 - valLoss: 0.4161653220653534 - trainLoss: 0.4079165458679199\n",
      "cnt: 0 - valLoss: 0.4161567687988281 - trainLoss: 0.4079083502292633\n",
      "cnt: 0 - valLoss: 0.4161486029624939 - trainLoss: 0.4079001247882843\n",
      "cnt: 0 - valLoss: 0.41614049673080444 - trainLoss: 0.4078918993473053\n",
      "cnt: 0 - valLoss: 0.41613277792930603 - trainLoss: 0.4078837037086487\n",
      "cnt: 0 - valLoss: 0.4161246120929718 - trainLoss: 0.40787553787231445\n",
      "cnt: 0 - valLoss: 0.41611626744270325 - trainLoss: 0.40786734223365784\n",
      "cnt: 0 - valLoss: 0.4161081612110138 - trainLoss: 0.407859206199646\n",
      "cnt: 0 - valLoss: 0.4160999059677124 - trainLoss: 0.40785107016563416\n",
      "cnt: 0 - valLoss: 0.41609182953834534 - trainLoss: 0.4078429639339447\n",
      "cnt: 0 - valLoss: 0.41608381271362305 - trainLoss: 0.40783485770225525\n",
      "cnt: 0 - valLoss: 0.4160751700401306 - trainLoss: 0.4078267514705658\n",
      "cnt: 0 - valLoss: 0.41606733202934265 - trainLoss: 0.40781864523887634\n",
      "cnt: 0 - valLoss: 0.4160597622394562 - trainLoss: 0.4078105390071869\n",
      "cnt: 0 - valLoss: 0.4160519242286682 - trainLoss: 0.40780243277549744\n",
      "cnt: 0 - valLoss: 0.41604363918304443 - trainLoss: 0.407794326543808\n",
      "cnt: 0 - valLoss: 0.41603562235832214 - trainLoss: 0.4077862501144409\n",
      "cnt: 0 - valLoss: 0.4160274565219879 - trainLoss: 0.4077781140804291\n",
      "cnt: 0 - valLoss: 0.4160196781158447 - trainLoss: 0.4077700674533844\n",
      "cnt: 0 - valLoss: 0.4160115718841553 - trainLoss: 0.40776196122169495\n",
      "cnt: 0 - valLoss: 0.416003555059433 - trainLoss: 0.4077538847923279\n",
      "cnt: 0 - valLoss: 0.41599565744400024 - trainLoss: 0.4077458083629608\n",
      "cnt: 0 - valLoss: 0.4159877896308899 - trainLoss: 0.407737672328949\n",
      "cnt: 0 - valLoss: 0.41597944498062134 - trainLoss: 0.4077296257019043\n",
      "cnt: 0 - valLoss: 0.4159716069698334 - trainLoss: 0.40772154927253723\n",
      "cnt: 0 - valLoss: 0.41596361994743347 - trainLoss: 0.40771347284317017\n",
      "cnt: 0 - valLoss: 0.4159562289714813 - trainLoss: 0.4077054262161255\n",
      "cnt: 0 - valLoss: 0.41594821214675903 - trainLoss: 0.4076973497867584\n",
      "cnt: 0 - valLoss: 0.41594046354293823 - trainLoss: 0.40768924355506897\n",
      "cnt: 0 - valLoss: 0.4159325659275055 - trainLoss: 0.4076811969280243\n",
      "cnt: 0 - valLoss: 0.4159245193004608 - trainLoss: 0.407673180103302\n",
      "cnt: 0 - valLoss: 0.41591671109199524 - trainLoss: 0.40766507387161255\n",
      "cnt: 0 - valLoss: 0.4159090220928192 - trainLoss: 0.40765702724456787\n",
      "cnt: 0 - valLoss: 0.4159010946750641 - trainLoss: 0.4076489806175232\n",
      "cnt: 0 - valLoss: 0.41589319705963135 - trainLoss: 0.4076409339904785\n",
      "cnt: 0 - valLoss: 0.41588538885116577 - trainLoss: 0.40763288736343384\n",
      "cnt: 0 - valLoss: 0.4158776104450226 - trainLoss: 0.40762487053871155\n",
      "cnt: 0 - valLoss: 0.4158693552017212 - trainLoss: 0.40761685371398926\n",
      "cnt: 0 - valLoss: 0.415861576795578 - trainLoss: 0.40760886669158936\n",
      "cnt: 0 - valLoss: 0.4158535599708557 - trainLoss: 0.40760087966918945\n",
      "cnt: 0 - valLoss: 0.4158458113670349 - trainLoss: 0.40759289264678955\n",
      "cnt: 0 - valLoss: 0.415838360786438 - trainLoss: 0.40758493542671204\n",
      "cnt: 0 - valLoss: 0.41583016514778137 - trainLoss: 0.40757694840431213\n",
      "cnt: 0 - valLoss: 0.4158223867416382 - trainLoss: 0.4075689911842346\n",
      "cnt: 0 - valLoss: 0.41581442952156067 - trainLoss: 0.40756097435951233\n",
      "cnt: 0 - valLoss: 0.41580668091773987 - trainLoss: 0.4075530171394348\n",
      "cnt: 0 - valLoss: 0.4157988131046295 - trainLoss: 0.4075450897216797\n",
      "cnt: 0 - valLoss: 0.4157911241054535 - trainLoss: 0.4075371325016022\n",
      "cnt: 0 - valLoss: 0.41578319668769836 - trainLoss: 0.40752917528152466\n",
      "cnt: 0 - valLoss: 0.4157756567001343 - trainLoss: 0.40752118825912476\n",
      "cnt: 0 - valLoss: 0.415767639875412 - trainLoss: 0.40751323103904724\n",
      "cnt: 0 - valLoss: 0.4157596528530121 - trainLoss: 0.40750524401664734\n",
      "cnt: 0 - valLoss: 0.4157518744468689 - trainLoss: 0.40749725699424744\n",
      "cnt: 0 - valLoss: 0.41574394702911377 - trainLoss: 0.4074892997741699\n",
      "cnt: 0 - valLoss: 0.41573649644851685 - trainLoss: 0.4074813425540924\n",
      "cnt: 0 - valLoss: 0.4157283902168274 - trainLoss: 0.4074733555316925\n",
      "cnt: 0 - valLoss: 0.415720671415329 - trainLoss: 0.407465398311615\n",
      "cnt: 0 - valLoss: 0.415712833404541 - trainLoss: 0.4074574410915375\n",
      "cnt: 0 - valLoss: 0.41570520401000977 - trainLoss: 0.4074494540691376\n",
      "cnt: 0 - valLoss: 0.41569724678993225 - trainLoss: 0.40744149684906006\n",
      "cnt: 0 - valLoss: 0.41568997502326965 - trainLoss: 0.40743356943130493\n",
      "cnt: 0 - valLoss: 0.4156823456287384 - trainLoss: 0.4074256122112274\n",
      "cnt: 0 - valLoss: 0.41567447781562805 - trainLoss: 0.4074176251888275\n",
      "cnt: 0 - valLoss: 0.41566693782806396 - trainLoss: 0.4074097275733948\n",
      "cnt: 0 - valLoss: 0.41565918922424316 - trainLoss: 0.40740177035331726\n",
      "cnt: 0 - valLoss: 0.4156514108181 - trainLoss: 0.40739381313323975\n",
      "cnt: 0 - valLoss: 0.4156439006328583 - trainLoss: 0.40738585591316223\n",
      "cnt: 0 - valLoss: 0.4156363904476166 - trainLoss: 0.4073779582977295\n",
      "cnt: 0 - valLoss: 0.4156288802623749 - trainLoss: 0.407370001077652\n",
      "cnt: 0 - valLoss: 0.41562125086784363 - trainLoss: 0.40736204385757446\n",
      "cnt: 0 - valLoss: 0.41561371088027954 - trainLoss: 0.4073541462421417\n",
      "cnt: 0 - valLoss: 0.41560620069503784 - trainLoss: 0.407346248626709\n",
      "cnt: 0 - valLoss: 0.4155987799167633 - trainLoss: 0.40733829140663147\n",
      "cnt: 0 - valLoss: 0.41559112071990967 - trainLoss: 0.40733039379119873\n",
      "cnt: 0 - valLoss: 0.41558346152305603 - trainLoss: 0.4073224663734436\n",
      "cnt: 0 - valLoss: 0.4155758023262024 - trainLoss: 0.4073145389556885\n",
      "cnt: 0 - valLoss: 0.41556838154792786 - trainLoss: 0.4073066711425781\n",
      "cnt: 0 - valLoss: 0.41556110978126526 - trainLoss: 0.4072987139225006\n",
      "cnt: 0 - valLoss: 0.4155537188053131 - trainLoss: 0.40729081630706787\n",
      "cnt: 0 - valLoss: 0.415546178817749 - trainLoss: 0.40728291869163513\n",
      "cnt: 0 - valLoss: 0.4155386984348297 - trainLoss: 0.40727499127388\n",
      "cnt: 0 - valLoss: 0.41553083062171936 - trainLoss: 0.40726709365844727\n",
      "cnt: 0 - valLoss: 0.4155235290527344 - trainLoss: 0.4072591960430145\n",
      "cnt: 0 - valLoss: 0.41551607847213745 - trainLoss: 0.4072512984275818\n",
      "cnt: 0 - valLoss: 0.4155086278915405 - trainLoss: 0.40724340081214905\n",
      "cnt: 0 - valLoss: 0.4155014455318451 - trainLoss: 0.4072355031967163\n",
      "cnt: 0 - valLoss: 0.41549423336982727 - trainLoss: 0.40722760558128357\n",
      "cnt: 0 - valLoss: 0.41548672318458557 - trainLoss: 0.40721970796585083\n",
      "cnt: 0 - valLoss: 0.4154791533946991 - trainLoss: 0.4072118401527405\n",
      "cnt: 0 - valLoss: 0.415471613407135 - trainLoss: 0.40720394253730774\n",
      "cnt: 0 - valLoss: 0.4154643416404724 - trainLoss: 0.4071961045265198\n",
      "cnt: 0 - valLoss: 0.415457159280777 - trainLoss: 0.40718820691108704\n",
      "cnt: 0 - valLoss: 0.415449857711792 - trainLoss: 0.4071803092956543\n",
      "cnt: 0 - valLoss: 0.41544246673583984 - trainLoss: 0.40717247128486633\n",
      "cnt: 0 - valLoss: 0.41543519496917725 - trainLoss: 0.4071645438671112\n",
      "cnt: 0 - valLoss: 0.41542768478393555 - trainLoss: 0.40715667605400085\n",
      "cnt: 0 - valLoss: 0.4154205322265625 - trainLoss: 0.4071488380432129\n",
      "cnt: 0 - valLoss: 0.41541311144828796 - trainLoss: 0.40714097023010254\n",
      "cnt: 0 - valLoss: 0.4154059588909149 - trainLoss: 0.4071330726146698\n",
      "cnt: 0 - valLoss: 0.4153987467288971 - trainLoss: 0.40712523460388184\n",
      "cnt: 0 - valLoss: 0.41539132595062256 - trainLoss: 0.4071173667907715\n",
      "cnt: 0 - valLoss: 0.4153839647769928 - trainLoss: 0.40710949897766113\n",
      "cnt: 0 - valLoss: 0.4153766930103302 - trainLoss: 0.40710166096687317\n",
      "cnt: 0 - valLoss: 0.41536954045295715 - trainLoss: 0.4070937931537628\n",
      "cnt: 0 - valLoss: 0.4153622090816498 - trainLoss: 0.40708592534065247\n",
      "cnt: 0 - valLoss: 0.41535523533821106 - trainLoss: 0.4070780873298645\n",
      "cnt: 0 - valLoss: 0.41534796357154846 - trainLoss: 0.40707021951675415\n",
      "cnt: 0 - valLoss: 0.41534072160720825 - trainLoss: 0.4070624113082886\n",
      "cnt: 0 - valLoss: 0.4153333306312561 - trainLoss: 0.4070545732975006\n",
      "cnt: 0 - valLoss: 0.41532617807388306 - trainLoss: 0.40704667568206787\n",
      "cnt: 0 - valLoss: 0.4153198301792145 - trainLoss: 0.4070388376712799\n",
      "cnt: 0 - valLoss: 0.4153138995170593 - trainLoss: 0.40703094005584717\n",
      "cnt: 0 - valLoss: 0.41530683636665344 - trainLoss: 0.40702301263809204\n",
      "cnt: 0 - valLoss: 0.4152997136116028 - trainLoss: 0.4070151448249817\n",
      "cnt: 0 - valLoss: 0.41529324650764465 - trainLoss: 0.40700721740722656\n",
      "cnt: 0 - valLoss: 0.4152863621711731 - trainLoss: 0.4069993495941162\n",
      "cnt: 0 - valLoss: 0.41527923941612244 - trainLoss: 0.40699148178100586\n",
      "cnt: 0 - valLoss: 0.41527244448661804 - trainLoss: 0.4069835841655731\n",
      "cnt: 0 - valLoss: 0.4152654707431793 - trainLoss: 0.4069756865501404\n",
      "cnt: 0 - valLoss: 0.4152584671974182 - trainLoss: 0.40696778893470764\n",
      "cnt: 0 - valLoss: 0.41525140404701233 - trainLoss: 0.4069599211215973\n",
      "cnt: 0 - valLoss: 0.41524460911750793 - trainLoss: 0.40695205330848694\n",
      "cnt: 0 - valLoss: 0.4152374267578125 - trainLoss: 0.4069441854953766\n",
      "cnt: 0 - valLoss: 0.4152306318283081 - trainLoss: 0.40693631768226624\n",
      "cnt: 0 - valLoss: 0.41522347927093506 - trainLoss: 0.4069284200668335\n",
      "cnt: 0 - valLoss: 0.4152165651321411 - trainLoss: 0.40692055225372314\n",
      "cnt: 0 - valLoss: 0.4152098298072815 - trainLoss: 0.4069126844406128\n",
      "cnt: 0 - valLoss: 0.4152030050754547 - trainLoss: 0.40690481662750244\n",
      "cnt: 0 - valLoss: 0.41519564390182495 - trainLoss: 0.4068969488143921\n",
      "cnt: 0 - valLoss: 0.41518861055374146 - trainLoss: 0.40688905119895935\n",
      "cnt: 0 - valLoss: 0.41518256068229675 - trainLoss: 0.4068812429904938\n",
      "cnt: 0 - valLoss: 0.4151756167411804 - trainLoss: 0.4068733751773834\n",
      "cnt: 0 - valLoss: 0.41516855359077454 - trainLoss: 0.40686553716659546\n",
      "cnt: 0 - valLoss: 0.41516196727752686 - trainLoss: 0.4068576991558075\n",
      "cnt: 0 - valLoss: 0.4151550829410553 - trainLoss: 0.40684980154037476\n",
      "cnt: 0 - valLoss: 0.41514796018600464 - trainLoss: 0.40684202313423157\n",
      "cnt: 0 - valLoss: 0.41514140367507935 - trainLoss: 0.4068341553211212\n",
      "cnt: 0 - valLoss: 0.4151342213153839 - trainLoss: 0.40682631731033325\n",
      "cnt: 0 - valLoss: 0.41512739658355713 - trainLoss: 0.4068184792995453\n",
      "cnt: 0 - valLoss: 0.4151206314563751 - trainLoss: 0.4068106412887573\n",
      "cnt: 0 - valLoss: 0.4151137173175812 - trainLoss: 0.40680283308029175\n",
      "cnt: 0 - valLoss: 0.41510674357414246 - trainLoss: 0.4067949950695038\n",
      "cnt: 0 - valLoss: 0.41509997844696045 - trainLoss: 0.4067871570587158\n",
      "cnt: 0 - valLoss: 0.41509342193603516 - trainLoss: 0.40677931904792786\n",
      "cnt: 0 - valLoss: 0.415086567401886 - trainLoss: 0.4067714810371399\n",
      "cnt: 0 - valLoss: 0.4150793254375458 - trainLoss: 0.4067636728286743\n",
      "cnt: 0 - valLoss: 0.41507256031036377 - trainLoss: 0.40675583481788635\n",
      "cnt: 0 - valLoss: 0.41506603360176086 - trainLoss: 0.4067480266094208\n",
      "cnt: 0 - valLoss: 0.4150594472885132 - trainLoss: 0.40674012899398804\n",
      "cnt: 0 - valLoss: 0.41505274176597595 - trainLoss: 0.4067322611808777\n",
      "cnt: 0 - valLoss: 0.4150462746620178 - trainLoss: 0.40672439336776733\n",
      "cnt: 0 - valLoss: 0.4150400757789612 - trainLoss: 0.406716525554657\n",
      "cnt: 0 - valLoss: 0.4150335192680359 - trainLoss: 0.40670865774154663\n",
      "cnt: 0 - valLoss: 0.41502681374549866 - trainLoss: 0.4067007601261139\n",
      "cnt: 0 - valLoss: 0.4150204062461853 - trainLoss: 0.40669289231300354\n",
      "cnt: 0 - valLoss: 0.41501352190971375 - trainLoss: 0.4066850543022156\n",
      "cnt: 0 - valLoss: 0.4150067865848541 - trainLoss: 0.4066771864891052\n",
      "cnt: 0 - valLoss: 0.4150000810623169 - trainLoss: 0.40666940808296204\n",
      "cnt: 0 - valLoss: 0.41499313712120056 - trainLoss: 0.40666159987449646\n",
      "cnt: 0 - valLoss: 0.4149865508079529 - trainLoss: 0.4066537916660309\n",
      "cnt: 0 - valLoss: 0.4149797558784485 - trainLoss: 0.40664589405059814\n",
      "cnt: 0 - valLoss: 0.41497328877449036 - trainLoss: 0.4066380560398102\n",
      "cnt: 0 - valLoss: 0.41496655344963074 - trainLoss: 0.40663018822669983\n",
      "cnt: 0 - valLoss: 0.41496002674102783 - trainLoss: 0.4066223204135895\n",
      "cnt: 0 - valLoss: 0.414953351020813 - trainLoss: 0.4066144824028015\n",
      "cnt: 0 - valLoss: 0.4149467349052429 - trainLoss: 0.40660661458969116\n",
      "cnt: 0 - valLoss: 0.4149400293827057 - trainLoss: 0.4065987467765808\n",
      "cnt: 0 - valLoss: 0.4149336814880371 - trainLoss: 0.40659090876579285\n",
      "cnt: 0 - valLoss: 0.4149268865585327 - trainLoss: 0.4065830409526825\n",
      "cnt: 0 - valLoss: 0.4149201214313507 - trainLoss: 0.4065752327442169\n",
      "cnt: 0 - valLoss: 0.4149136543273926 - trainLoss: 0.40656736493110657\n",
      "cnt: 0 - valLoss: 0.4149071276187897 - trainLoss: 0.4065594971179962\n",
      "cnt: 0 - valLoss: 0.41490018367767334 - trainLoss: 0.40655168890953064\n",
      "cnt: 0 - valLoss: 0.4148935377597809 - trainLoss: 0.40654394030570984\n",
      "cnt: 0 - valLoss: 0.4148866832256317 - trainLoss: 0.40653616189956665\n",
      "cnt: 0 - valLoss: 0.41487982869148254 - trainLoss: 0.40652841329574585\n",
      "cnt: 0 - valLoss: 0.41487348079681396 - trainLoss: 0.40652063488960266\n",
      "cnt: 0 - valLoss: 0.4148668348789215 - trainLoss: 0.40651294589042664\n",
      "cnt: 0 - valLoss: 0.4148600101470947 - trainLoss: 0.40650519728660583\n",
      "cnt: 0 - valLoss: 0.4148528277873993 - trainLoss: 0.4064975082874298\n",
      "cnt: 0 - valLoss: 0.4148460626602173 - trainLoss: 0.406489759683609\n",
      "cnt: 0 - valLoss: 0.4148395359516144 - trainLoss: 0.4064820408821106\n",
      "cnt: 0 - valLoss: 0.4148326516151428 - trainLoss: 0.4064743220806122\n",
      "cnt: 0 - valLoss: 0.41482555866241455 - trainLoss: 0.406466543674469\n",
      "cnt: 0 - valLoss: 0.4148190915584564 - trainLoss: 0.4064587950706482\n",
      "cnt: 0 - valLoss: 0.4148121774196625 - trainLoss: 0.4064510464668274\n",
      "cnt: 0 - valLoss: 0.41480544209480286 - trainLoss: 0.406443327665329\n",
      "cnt: 0 - valLoss: 0.4147984981536865 - trainLoss: 0.40643560886383057\n",
      "cnt: 0 - valLoss: 0.4147917628288269 - trainLoss: 0.40642786026000977\n",
      "cnt: 0 - valLoss: 0.41478487849235535 - trainLoss: 0.4064200818538666\n",
      "cnt: 0 - valLoss: 0.4147782623767853 - trainLoss: 0.40641236305236816\n",
      "cnt: 0 - valLoss: 0.41477152705192566 - trainLoss: 0.40640464425086975\n",
      "cnt: 0 - valLoss: 0.41476479172706604 - trainLoss: 0.40639692544937134\n",
      "cnt: 0 - valLoss: 0.41475796699523926 - trainLoss: 0.40638917684555054\n",
      "cnt: 0 - valLoss: 0.414751261472702 - trainLoss: 0.4063814580440521\n",
      "cnt: 0 - valLoss: 0.4147445857524872 - trainLoss: 0.4063737392425537\n",
      "cnt: 0 - valLoss: 0.41473788022994995 - trainLoss: 0.4063660204410553\n",
      "cnt: 0 - valLoss: 0.414730966091156 - trainLoss: 0.4063583016395569\n",
      "cnt: 0 - valLoss: 0.41472455859184265 - trainLoss: 0.4063505530357361\n",
      "cnt: 0 - valLoss: 0.4147176444530487 - trainLoss: 0.40634283423423767\n",
      "cnt: 0 - valLoss: 0.41471096873283386 - trainLoss: 0.40633511543273926\n",
      "cnt: 0 - valLoss: 0.4147041440010071 - trainLoss: 0.40632742643356323\n",
      "cnt: 0 - valLoss: 0.4146975874900818 - trainLoss: 0.40631967782974243\n",
      "cnt: 0 - valLoss: 0.4146910309791565 - trainLoss: 0.4063119888305664\n",
      "cnt: 0 - valLoss: 0.41468408703804016 - trainLoss: 0.4063042998313904\n",
      "cnt: 0 - valLoss: 0.41467759013175964 - trainLoss: 0.4062965214252472\n",
      "cnt: 0 - valLoss: 0.41467082500457764 - trainLoss: 0.40628883242607117\n",
      "cnt: 0 - valLoss: 0.4146641790866852 - trainLoss: 0.40628117322921753\n",
      "cnt: 0 - valLoss: 0.41465750336647034 - trainLoss: 0.4062734544277191\n",
      "cnt: 0 - valLoss: 0.41465112566947937 - trainLoss: 0.4062657356262207\n",
      "cnt: 0 - valLoss: 0.414644330739975 - trainLoss: 0.40625807642936707\n",
      "cnt: 0 - valLoss: 0.4146377146244049 - trainLoss: 0.40625032782554626\n",
      "cnt: 0 - valLoss: 0.41463080048561096 - trainLoss: 0.4062426686286926\n",
      "cnt: 0 - valLoss: 0.41462454199790955 - trainLoss: 0.4062349796295166\n",
      "cnt: 0 - valLoss: 0.4146176278591156 - trainLoss: 0.4062272608280182\n",
      "cnt: 0 - valLoss: 0.4146110713481903 - trainLoss: 0.4062195420265198\n",
      "cnt: 0 - valLoss: 0.4146045744419098 - trainLoss: 0.40621188282966614\n",
      "cnt: 0 - valLoss: 0.414597749710083 - trainLoss: 0.4062041938304901\n",
      "cnt: 0 - valLoss: 0.4145910441875458 - trainLoss: 0.4061965346336365\n",
      "cnt: 0 - valLoss: 0.41458410024642944 - trainLoss: 0.40618881583213806\n",
      "cnt: 0 - valLoss: 0.4145777225494385 - trainLoss: 0.40618112683296204\n",
      "cnt: 0 - valLoss: 0.414571076631546 - trainLoss: 0.4061734676361084\n",
      "cnt: 0 - valLoss: 0.414564311504364 - trainLoss: 0.40616574883461\n",
      "cnt: 0 - valLoss: 0.41455766558647156 - trainLoss: 0.40615811944007874\n",
      "cnt: 0 - valLoss: 0.4145512878894806 - trainLoss: 0.4061504304409027\n",
      "cnt: 0 - valLoss: 0.4145444929599762 - trainLoss: 0.4061427414417267\n",
      "cnt: 0 - valLoss: 0.41453787684440613 - trainLoss: 0.40613508224487305\n",
      "cnt: 0 - valLoss: 0.41453102231025696 - trainLoss: 0.4061274230480194\n",
      "cnt: 0 - valLoss: 0.41452473402023315 - trainLoss: 0.4061197340488434\n",
      "cnt: 0 - valLoss: 0.4145180583000183 - trainLoss: 0.40611207485198975\n",
      "cnt: 0 - valLoss: 0.4145112633705139 - trainLoss: 0.4061044156551361\n",
      "cnt: 0 - valLoss: 0.41450488567352295 - trainLoss: 0.40609675645828247\n",
      "cnt: 0 - valLoss: 0.4144982099533081 - trainLoss: 0.40608909726142883\n",
      "cnt: 0 - valLoss: 0.41449159383773804 - trainLoss: 0.4060814082622528\n",
      "cnt: 0 - valLoss: 0.4144848883152008 - trainLoss: 0.40607377886772156\n",
      "cnt: 0 - valLoss: 0.4144785702228546 - trainLoss: 0.4060661196708679\n",
      "cnt: 0 - valLoss: 0.41447171568870544 - trainLoss: 0.40605849027633667\n",
      "cnt: 0 - valLoss: 0.414465069770813 - trainLoss: 0.40605074167251587\n",
      "cnt: 0 - valLoss: 0.41445836424827576 - trainLoss: 0.4060431122779846\n",
      "cnt: 0 - valLoss: 0.41445210576057434 - trainLoss: 0.40603548288345337\n",
      "cnt: 0 - valLoss: 0.4144454002380371 - trainLoss: 0.40602779388427734\n",
      "cnt: 0 - valLoss: 0.41443878412246704 - trainLoss: 0.4060201048851013\n",
      "cnt: 0 - valLoss: 0.4144318997859955 - trainLoss: 0.40601247549057007\n",
      "cnt: 0 - valLoss: 0.41442573070526123 - trainLoss: 0.4060048460960388\n",
      "cnt: 0 - valLoss: 0.4144190549850464 - trainLoss: 0.4059971868991852\n",
      "cnt: 0 - valLoss: 0.41441240906715393 - trainLoss: 0.40598952770233154\n",
      "cnt: 0 - valLoss: 0.4144057631492615 - trainLoss: 0.4059818983078003\n",
      "cnt: 0 - valLoss: 0.4143993854522705 - trainLoss: 0.40597420930862427\n",
      "cnt: 0 - valLoss: 0.4143925607204437 - trainLoss: 0.405966579914093\n",
      "cnt: 0 - valLoss: 0.4143858253955841 - trainLoss: 0.40595895051956177\n",
      "cnt: 0 - valLoss: 0.4143795073032379 - trainLoss: 0.4059513509273529\n",
      "cnt: 0 - valLoss: 0.4143730103969574 - trainLoss: 0.40594375133514404\n",
      "cnt: 0 - valLoss: 0.41436654329299927 - trainLoss: 0.4059361219406128\n",
      "cnt: 0 - valLoss: 0.41436004638671875 - trainLoss: 0.4059285521507263\n",
      "cnt: 0 - valLoss: 0.41435378789901733 - trainLoss: 0.40592092275619507\n",
      "cnt: 0 - valLoss: 0.41434744000434875 - trainLoss: 0.4059133529663086\n",
      "cnt: 0 - valLoss: 0.41434088349342346 - trainLoss: 0.40590575337409973\n",
      "cnt: 0 - valLoss: 0.4143344759941101 - trainLoss: 0.40589821338653564\n",
      "cnt: 0 - valLoss: 0.414328008890152 - trainLoss: 0.4058905839920044\n",
      "cnt: 0 - valLoss: 0.41432181000709534 - trainLoss: 0.4058830142021179\n",
      "cnt: 0 - valLoss: 0.41431528329849243 - trainLoss: 0.40587541460990906\n",
      "cnt: 0 - valLoss: 0.4143090844154358 - trainLoss: 0.4058678448200226\n",
      "cnt: 0 - valLoss: 0.4143028259277344 - trainLoss: 0.4058602750301361\n",
      "cnt: 0 - valLoss: 0.41429680585861206 - trainLoss: 0.40585264563560486\n",
      "cnt: 0 - valLoss: 0.4142901599407196 - trainLoss: 0.405845046043396\n",
      "cnt: 0 - valLoss: 0.4142840504646301 - trainLoss: 0.4058374762535095\n",
      "cnt: 0 - valLoss: 0.41427773237228394 - trainLoss: 0.40582987666130066\n",
      "cnt: 0 - valLoss: 0.41427168250083923 - trainLoss: 0.4058222770690918\n",
      "cnt: 0 - valLoss: 0.41426530480384827 - trainLoss: 0.4058147072792053\n",
      "cnt: 0 - valLoss: 0.41425904631614685 - trainLoss: 0.40580710768699646\n",
      "cnt: 0 - valLoss: 0.4142521917819977 - trainLoss: 0.40579953789711\n",
      "cnt: 0 - valLoss: 0.41424548625946045 - trainLoss: 0.40579184889793396\n",
      "cnt: 0 - valLoss: 0.41423866152763367 - trainLoss: 0.40578407049179077\n",
      "cnt: 0 - valLoss: 0.41423189640045166 - trainLoss: 0.40577632188796997\n",
      "cnt: 0 - valLoss: 0.41422539949417114 - trainLoss: 0.40576860308647156\n",
      "cnt: 0 - valLoss: 0.4142186641693115 - trainLoss: 0.40576085448265076\n",
      "cnt: 0 - valLoss: 0.414212167263031 - trainLoss: 0.40575313568115234\n",
      "cnt: 0 - valLoss: 0.4142055809497833 - trainLoss: 0.40574541687965393\n",
      "cnt: 0 - valLoss: 0.41419923305511475 - trainLoss: 0.4057376980781555\n",
      "cnt: 0 - valLoss: 0.4141925573348999 - trainLoss: 0.4057300090789795\n",
      "cnt: 0 - valLoss: 0.4141862392425537 - trainLoss: 0.40572232007980347\n",
      "cnt: 0 - valLoss: 0.41417983174324036 - trainLoss: 0.40571466088294983\n",
      "cnt: 0 - valLoss: 0.41417375206947327 - trainLoss: 0.4057069420814514\n",
      "cnt: 0 - valLoss: 0.414167195558548 - trainLoss: 0.4056992530822754\n",
      "cnt: 0 - valLoss: 0.41416096687316895 - trainLoss: 0.40569156408309937\n",
      "cnt: 0 - valLoss: 0.41415444016456604 - trainLoss: 0.4056839048862457\n",
      "cnt: 0 - valLoss: 0.4141482710838318 - trainLoss: 0.4056762158870697\n",
      "cnt: 0 - valLoss: 0.41414183378219604 - trainLoss: 0.40566855669021606\n",
      "cnt: 0 - valLoss: 0.4141351580619812 - trainLoss: 0.40566083788871765\n",
      "cnt: 0 - valLoss: 0.41412925720214844 - trainLoss: 0.4056531488895416\n",
      "cnt: 0 - valLoss: 0.41412273049354553 - trainLoss: 0.4056454598903656\n",
      "cnt: 0 - valLoss: 0.41411635279655457 - trainLoss: 0.40563780069351196\n",
      "cnt: 0 - valLoss: 0.41410982608795166 - trainLoss: 0.4056301414966583\n",
      "cnt: 0 - valLoss: 0.4141036570072174 - trainLoss: 0.4056224524974823\n",
      "cnt: 0 - valLoss: 0.4140969514846802 - trainLoss: 0.40561479330062866\n",
      "cnt: 0 - valLoss: 0.4140908420085907 - trainLoss: 0.405607134103775\n",
      "cnt: 0 - valLoss: 0.414083868265152 - trainLoss: 0.4055994153022766\n",
      "cnt: 0 - valLoss: 0.41407787799835205 - trainLoss: 0.40559181571006775\n",
      "cnt: 0 - valLoss: 0.41407161951065063 - trainLoss: 0.4055841267108917\n",
      "cnt: 0 - valLoss: 0.4140649139881134 - trainLoss: 0.4055764973163605\n",
      "cnt: 0 - valLoss: 0.4140585660934448 - trainLoss: 0.4055688679218292\n",
      "cnt: 0 - valLoss: 0.4140520989894867 - trainLoss: 0.405561238527298\n",
      "cnt: 0 - valLoss: 0.41404569149017334 - trainLoss: 0.4055536091327667\n",
      "cnt: 0 - valLoss: 0.41403868794441223 - trainLoss: 0.4055459797382355\n",
      "cnt: 0 - valLoss: 0.414032906293869 - trainLoss: 0.4055383503437042\n",
      "cnt: 0 - valLoss: 0.4140264093875885 - trainLoss: 0.40553075075149536\n",
      "cnt: 0 - valLoss: 0.414019912481308 - trainLoss: 0.4055231213569641\n",
      "cnt: 0 - valLoss: 0.41401323676109314 - trainLoss: 0.40551549196243286\n",
      "cnt: 0 - valLoss: 0.4140075445175171 - trainLoss: 0.405507892370224\n",
      "cnt: 0 - valLoss: 0.4140012860298157 - trainLoss: 0.4055003225803375\n",
      "cnt: 0 - valLoss: 0.41399428248405457 - trainLoss: 0.4054926931858063\n",
      "cnt: 0 - valLoss: 0.4139884412288666 - trainLoss: 0.4054850935935974\n",
      "cnt: 0 - valLoss: 0.41398197412490845 - trainLoss: 0.40547749400138855\n",
      "cnt: 0 - valLoss: 0.41397523880004883 - trainLoss: 0.4054698944091797\n",
      "cnt: 0 - valLoss: 0.41396892070770264 - trainLoss: 0.4054623246192932\n",
      "cnt: 0 - valLoss: 0.4139626622200012 - trainLoss: 0.40545472502708435\n",
      "cnt: 0 - valLoss: 0.41395625472068787 - trainLoss: 0.4054471552371979\n",
      "cnt: 0 - valLoss: 0.41394978761672974 - trainLoss: 0.405439555644989\n",
      "cnt: 0 - valLoss: 0.41394364833831787 - trainLoss: 0.40543198585510254\n",
      "cnt: 0 - valLoss: 0.413936972618103 - trainLoss: 0.40542441606521606\n",
      "cnt: 0 - valLoss: 0.4139305055141449 - trainLoss: 0.405416876077652\n",
      "cnt: 0 - valLoss: 0.4139244854450226 - trainLoss: 0.4054093360900879\n",
      "cnt: 0 - valLoss: 0.41391775012016296 - trainLoss: 0.4054017663002014\n",
      "cnt: 0 - valLoss: 0.41391146183013916 - trainLoss: 0.40539422631263733\n",
      "cnt: 0 - valLoss: 0.4139053523540497 - trainLoss: 0.40538665652275085\n",
      "cnt: 0 - valLoss: 0.4138985872268677 - trainLoss: 0.4053790867328644\n",
      "cnt: 0 - valLoss: 0.4138924181461334 - trainLoss: 0.4053715467453003\n",
      "cnt: 0 - valLoss: 0.41388607025146484 - trainLoss: 0.4053640067577362\n",
      "cnt: 0 - valLoss: 0.4138795733451843 - trainLoss: 0.4053564965724945\n",
      "cnt: 0 - valLoss: 0.41387322545051575 - trainLoss: 0.4053489565849304\n",
      "cnt: 0 - valLoss: 0.41386717557907104 - trainLoss: 0.40534138679504395\n",
      "cnt: 0 - valLoss: 0.41386082768440247 - trainLoss: 0.40533381700515747\n",
      "cnt: 0 - valLoss: 0.4138549268245697 - trainLoss: 0.4053262174129486\n",
      "cnt: 0 - valLoss: 0.413848876953125 - trainLoss: 0.40531861782073975\n",
      "cnt: 0 - valLoss: 0.41384220123291016 - trainLoss: 0.4053110182285309\n",
      "cnt: 0 - valLoss: 0.41383612155914307 - trainLoss: 0.405303418636322\n",
      "cnt: 0 - valLoss: 0.41383039951324463 - trainLoss: 0.40529578924179077\n",
      "cnt: 0 - valLoss: 0.41382351517677307 - trainLoss: 0.4052882194519043\n",
      "cnt: 0 - valLoss: 0.4138171672821045 - trainLoss: 0.4052806496620178\n",
      "cnt: 0 - valLoss: 0.41381093859672546 - trainLoss: 0.40527310967445374\n",
      "cnt: 0 - valLoss: 0.4138043522834778 - trainLoss: 0.4052655100822449\n",
      "cnt: 0 - valLoss: 0.41379788517951965 - trainLoss: 0.4052579998970032\n",
      "cnt: 0 - valLoss: 0.4137914478778839 - trainLoss: 0.4052504003047943\n",
      "cnt: 0 - valLoss: 0.4137847423553467 - trainLoss: 0.4052428603172302\n",
      "cnt: 0 - valLoss: 0.4137788712978363 - trainLoss: 0.4052353501319885\n",
      "cnt: 0 - valLoss: 0.41377177834510803 - trainLoss: 0.40522781014442444\n",
      "cnt: 0 - valLoss: 0.41376540064811707 - trainLoss: 0.40522027015686035\n",
      "cnt: 0 - valLoss: 0.4137588441371918 - trainLoss: 0.40521273016929626\n",
      "cnt: 0 - valLoss: 0.41375237703323364 - trainLoss: 0.40520524978637695\n",
      "cnt: 0 - valLoss: 0.4137459397315979 - trainLoss: 0.40519773960113525\n",
      "cnt: 0 - valLoss: 0.4137391149997711 - trainLoss: 0.4051901698112488\n",
      "cnt: 0 - valLoss: 0.4137321710586548 - trainLoss: 0.4051826298236847\n",
      "cnt: 0 - valLoss: 0.413725882768631 - trainLoss: 0.4051750898361206\n",
      "cnt: 0 - valLoss: 0.4137198030948639 - trainLoss: 0.4051675796508789\n",
      "cnt: 0 - valLoss: 0.413713276386261 - trainLoss: 0.4051600694656372\n",
      "cnt: 0 - valLoss: 0.41370630264282227 - trainLoss: 0.4051525294780731\n",
      "cnt: 0 - valLoss: 0.41370078921318054 - trainLoss: 0.40514498949050903\n",
      "cnt: 0 - valLoss: 0.41369393467903137 - trainLoss: 0.40513744950294495\n",
      "cnt: 0 - valLoss: 0.41368743777275085 - trainLoss: 0.40512993931770325\n",
      "cnt: 0 - valLoss: 0.41368141770362854 - trainLoss: 0.40512242913246155\n",
      "cnt: 0 - valLoss: 0.41367459297180176 - trainLoss: 0.40511491894721985\n",
      "cnt: 0 - valLoss: 0.41366884112358093 - trainLoss: 0.40510743856430054\n",
      "cnt: 0 - valLoss: 0.41366201639175415 - trainLoss: 0.40509989857673645\n",
      "cnt: 0 - valLoss: 0.41365641355514526 - trainLoss: 0.4050924479961395\n",
      "cnt: 0 - valLoss: 0.4136492908000946 - trainLoss: 0.4050849378108978\n",
      "cnt: 0 - valLoss: 0.4136436879634857 - trainLoss: 0.4050774574279785\n",
      "cnt: 0 - valLoss: 0.4136365056037903 - trainLoss: 0.4050699472427368\n",
      "cnt: 0 - valLoss: 0.41363078355789185 - trainLoss: 0.4050624668598175\n",
      "cnt: 0 - valLoss: 0.41362473368644714 - trainLoss: 0.4050549864768982\n",
      "cnt: 0 - valLoss: 0.4136194586753845 - trainLoss: 0.4050474464893341\n",
      "cnt: 0 - valLoss: 0.4136141240596771 - trainLoss: 0.4050394296646118\n",
      "cnt: 0 - valLoss: 0.4136076867580414 - trainLoss: 0.4050314128398895\n",
      "cnt: 0 - valLoss: 0.41360312700271606 - trainLoss: 0.40502339601516724\n",
      "cnt: 0 - valLoss: 0.41359761357307434 - trainLoss: 0.40501540899276733\n",
      "cnt: 0 - valLoss: 0.41359204053878784 - trainLoss: 0.40500733256340027\n",
      "cnt: 0 - valLoss: 0.4135863482952118 - trainLoss: 0.404999315738678\n",
      "cnt: 0 - valLoss: 0.41358059644699097 - trainLoss: 0.40499147772789\n",
      "cnt: 0 - valLoss: 0.41357502341270447 - trainLoss: 0.4049837291240692\n",
      "cnt: 0 - valLoss: 0.41356930136680603 - trainLoss: 0.4049758315086365\n",
      "cnt: 0 - valLoss: 0.4135637879371643 - trainLoss: 0.4049679636955261\n",
      "cnt: 0 - valLoss: 0.4135582447052002 - trainLoss: 0.40496009588241577\n",
      "cnt: 0 - valLoss: 0.4135524034500122 - trainLoss: 0.4049522280693054\n",
      "cnt: 0 - valLoss: 0.41354700922966003 - trainLoss: 0.40494436025619507\n",
      "cnt: 0 - valLoss: 0.4135415256023407 - trainLoss: 0.40493646264076233\n",
      "cnt: 0 - valLoss: 0.41353651881217957 - trainLoss: 0.40492862462997437\n",
      "cnt: 0 - valLoss: 0.4135306775569916 - trainLoss: 0.4049207866191864\n",
      "cnt: 0 - valLoss: 0.41352546215057373 - trainLoss: 0.40491288900375366\n",
      "cnt: 0 - valLoss: 0.4135197699069977 - trainLoss: 0.4049050807952881\n",
      "cnt: 0 - valLoss: 0.41351476311683655 - trainLoss: 0.40489721298217773\n",
      "cnt: 0 - valLoss: 0.4135091304779053 - trainLoss: 0.4048893451690674\n",
      "cnt: 0 - valLoss: 0.41350358724594116 - trainLoss: 0.4048815071582794\n",
      "cnt: 0 - valLoss: 0.41349828243255615 - trainLoss: 0.40487363934516907\n",
      "cnt: 0 - valLoss: 0.41349244117736816 - trainLoss: 0.4048658311367035\n",
      "cnt: 0 - valLoss: 0.4134864807128906 - trainLoss: 0.4048581123352051\n",
      "cnt: 0 - valLoss: 0.41348060965538025 - trainLoss: 0.4048503637313843\n",
      "cnt: 0 - valLoss: 0.41347500681877136 - trainLoss: 0.40484264492988586\n",
      "cnt: 0 - valLoss: 0.4134691655635834 - trainLoss: 0.40483492612838745\n",
      "cnt: 0 - valLoss: 0.41346344351768494 - trainLoss: 0.40482720732688904\n",
      "cnt: 0 - valLoss: 0.4134577214717865 - trainLoss: 0.4048194885253906\n",
      "cnt: 0 - valLoss: 0.41345223784446716 - trainLoss: 0.4048117697238922\n",
      "cnt: 0 - valLoss: 0.41344642639160156 - trainLoss: 0.4048040211200714\n",
      "cnt: 0 - valLoss: 0.41344019770622253 - trainLoss: 0.404796302318573\n",
      "cnt: 0 - valLoss: 0.4134345054626465 - trainLoss: 0.4047885835170746\n",
      "cnt: 0 - valLoss: 0.4134286940097809 - trainLoss: 0.40478086471557617\n",
      "cnt: 0 - valLoss: 0.4134223759174347 - trainLoss: 0.40477311611175537\n",
      "cnt: 0 - valLoss: 0.4134165644645691 - trainLoss: 0.40476536750793457\n",
      "cnt: 0 - valLoss: 0.41341084241867065 - trainLoss: 0.40475761890411377\n",
      "cnt: 0 - valLoss: 0.41340532898902893 - trainLoss: 0.4047498106956482\n",
      "cnt: 0 - valLoss: 0.4133993983268738 - trainLoss: 0.4047420024871826\n",
      "cnt: 0 - valLoss: 0.4133937656879425 - trainLoss: 0.4047343134880066\n",
      "cnt: 0 - valLoss: 0.41338762640953064 - trainLoss: 0.40472665429115295\n",
      "cnt: 0 - valLoss: 0.41338154673576355 - trainLoss: 0.4047189950942993\n",
      "cnt: 0 - valLoss: 0.4133756160736084 - trainLoss: 0.40471139550209045\n",
      "cnt: 0 - valLoss: 0.4133700132369995 - trainLoss: 0.4047037661075592\n",
      "cnt: 0 - valLoss: 0.413363516330719 - trainLoss: 0.4046962261199951\n",
      "cnt: 0 - valLoss: 0.4133577048778534 - trainLoss: 0.40468865633010864\n",
      "cnt: 0 - valLoss: 0.41335156559944153 - trainLoss: 0.40468114614486694\n",
      "cnt: 0 - valLoss: 0.4133455157279968 - trainLoss: 0.40467366576194763\n",
      "cnt: 0 - valLoss: 0.4133393168449402 - trainLoss: 0.40466615557670593\n",
      "cnt: 0 - valLoss: 0.4133335053920746 - trainLoss: 0.4046587347984314\n",
      "cnt: 0 - valLoss: 0.413327157497406 - trainLoss: 0.4046512544155121\n",
      "cnt: 0 - valLoss: 0.41332098841667175 - trainLoss: 0.40464380383491516\n",
      "cnt: 0 - valLoss: 0.4133148491382599 - trainLoss: 0.40463629364967346\n",
      "cnt: 0 - valLoss: 0.4133090376853943 - trainLoss: 0.40462884306907654\n",
      "cnt: 0 - valLoss: 0.4133026599884033 - trainLoss: 0.4046213924884796\n",
      "cnt: 0 - valLoss: 0.4132964313030243 - trainLoss: 0.4046139717102051\n",
      "cnt: 0 - valLoss: 0.4132901132106781 - trainLoss: 0.40460655093193054\n",
      "cnt: 0 - valLoss: 0.41328367590904236 - trainLoss: 0.404599130153656\n",
      "cnt: 0 - valLoss: 0.41327741742134094 - trainLoss: 0.40459173917770386\n",
      "cnt: 0 - valLoss: 0.41327154636383057 - trainLoss: 0.4045843183994293\n",
      "cnt: 0 - valLoss: 0.4132649898529053 - trainLoss: 0.4045769274234772\n",
      "cnt: 0 - valLoss: 0.41325893998146057 - trainLoss: 0.40456947684288025\n",
      "cnt: 0 - valLoss: 0.413252592086792 - trainLoss: 0.4045620858669281\n",
      "cnt: 0 - valLoss: 0.4132465422153473 - trainLoss: 0.40455466508865356\n",
      "cnt: 0 - valLoss: 0.4132406413555145 - trainLoss: 0.4045473039150238\n",
      "cnt: 0 - valLoss: 0.4132341742515564 - trainLoss: 0.40453991293907166\n",
      "cnt: 0 - valLoss: 0.41322752833366394 - trainLoss: 0.4045324921607971\n",
      "cnt: 0 - valLoss: 0.41322097182273865 - trainLoss: 0.40452513098716736\n",
      "cnt: 0 - valLoss: 0.41321510076522827 - trainLoss: 0.4045177400112152\n",
      "cnt: 0 - valLoss: 0.41320887207984924 - trainLoss: 0.40451034903526306\n",
      "cnt: 0 - valLoss: 0.4132028818130493 - trainLoss: 0.4045029580593109\n",
      "cnt: 0 - valLoss: 0.4131963849067688 - trainLoss: 0.40449556708335876\n",
      "cnt: 0 - valLoss: 0.4131903350353241 - trainLoss: 0.404488205909729\n",
      "cnt: 0 - valLoss: 0.41318434476852417 - trainLoss: 0.40448087453842163\n",
      "cnt: 0 - valLoss: 0.41317838430404663 - trainLoss: 0.4044734537601471\n",
      "cnt: 0 - valLoss: 0.4131719470024109 - trainLoss: 0.4044661223888397\n",
      "cnt: 0 - valLoss: 0.41316595673561096 - trainLoss: 0.40445876121520996\n",
      "cnt: 0 - valLoss: 0.4131600856781006 - trainLoss: 0.4044513702392578\n",
      "cnt: 0 - valLoss: 0.4131541848182678 - trainLoss: 0.40444403886795044\n",
      "cnt: 0 - valLoss: 0.4131473898887634 - trainLoss: 0.4044366478919983\n",
      "cnt: 0 - valLoss: 0.41314125061035156 - trainLoss: 0.40442928671836853\n",
      "cnt: 0 - valLoss: 0.4131351411342621 - trainLoss: 0.40442192554473877\n",
      "cnt: 0 - valLoss: 0.4131290912628174 - trainLoss: 0.4044145941734314\n",
      "cnt: 0 - valLoss: 0.4131230413913727 - trainLoss: 0.404407262802124\n",
      "cnt: 0 - valLoss: 0.41311731934547424 - trainLoss: 0.40439990162849426\n",
      "cnt: 0 - valLoss: 0.4131118953227997 - trainLoss: 0.4043925404548645\n",
      "cnt: 0 - valLoss: 0.4131060242652893 - trainLoss: 0.4043850898742676\n",
      "cnt: 0 - valLoss: 0.41310015320777893 - trainLoss: 0.40437760949134827\n",
      "cnt: 0 - valLoss: 0.41309434175491333 - trainLoss: 0.4043702185153961\n",
      "cnt: 0 - valLoss: 0.4130885601043701 - trainLoss: 0.4043627679347992\n",
      "cnt: 0 - valLoss: 0.4130825102329254 - trainLoss: 0.40435534715652466\n",
      "cnt: 0 - valLoss: 0.4130767285823822 - trainLoss: 0.40434789657592773\n",
      "cnt: 0 - valLoss: 0.4130707383155823 - trainLoss: 0.4043404757976532\n",
      "cnt: 0 - valLoss: 0.4130650758743286 - trainLoss: 0.40433305501937866\n",
      "cnt: 0 - valLoss: 0.4130590856075287 - trainLoss: 0.4043256342411041\n",
      "cnt: 0 - valLoss: 0.41305360198020935 - trainLoss: 0.4043181836605072\n",
      "cnt: 0 - valLoss: 0.4130474627017975 - trainLoss: 0.40431079268455505\n",
      "cnt: 0 - valLoss: 0.4130419194698334 - trainLoss: 0.40430334210395813\n",
      "cnt: 0 - valLoss: 0.4130367636680603 - trainLoss: 0.4042959213256836\n",
      "cnt: 0 - valLoss: 0.4130309224128723 - trainLoss: 0.40428847074508667\n",
      "cnt: 0 - valLoss: 0.41302525997161865 - trainLoss: 0.40428102016448975\n",
      "cnt: 0 - valLoss: 0.41301968693733215 - trainLoss: 0.40427353978157043\n",
      "cnt: 0 - valLoss: 0.4130140542984009 - trainLoss: 0.4042661190032959\n",
      "cnt: 0 - valLoss: 0.4130081832408905 - trainLoss: 0.404258668422699\n",
      "cnt: 0 - valLoss: 0.4130023717880249 - trainLoss: 0.40425121784210205\n",
      "cnt: 0 - valLoss: 0.41299718618392944 - trainLoss: 0.40424370765686035\n",
      "cnt: 0 - valLoss: 0.41299089789390564 - trainLoss: 0.4042363166809082\n",
      "cnt: 0 - valLoss: 0.41298505663871765 - trainLoss: 0.40422889590263367\n",
      "cnt: 0 - valLoss: 0.41297945380210876 - trainLoss: 0.40422141551971436\n",
      "cnt: 0 - valLoss: 0.4129737913608551 - trainLoss: 0.4042139947414398\n",
      "cnt: 0 - valLoss: 0.4129678010940552 - trainLoss: 0.40420660376548767\n",
      "cnt: 0 - valLoss: 0.4129619002342224 - trainLoss: 0.40419915318489075\n",
      "cnt: 0 - valLoss: 0.4129560887813568 - trainLoss: 0.4041917324066162\n",
      "cnt: 0 - valLoss: 0.4129505455493927 - trainLoss: 0.4041843116283417\n",
      "cnt: 0 - valLoss: 0.41294485330581665 - trainLoss: 0.4041769206523895\n",
      "cnt: 0 - valLoss: 0.4129389524459839 - trainLoss: 0.404169499874115\n",
      "cnt: 0 - valLoss: 0.4129333198070526 - trainLoss: 0.40416204929351807\n",
      "cnt: 0 - valLoss: 0.4129275679588318 - trainLoss: 0.4041546583175659\n",
      "cnt: 0 - valLoss: 0.4129219055175781 - trainLoss: 0.404147207736969\n",
      "cnt: 0 - valLoss: 0.41291579604148865 - trainLoss: 0.40413984656333923\n",
      "cnt: 0 - valLoss: 0.412910133600235 - trainLoss: 0.4041323959827423\n",
      "cnt: 0 - valLoss: 0.4129047691822052 - trainLoss: 0.4041249752044678\n",
      "cnt: 0 - valLoss: 0.412898987531662 - trainLoss: 0.40411755442619324\n",
      "cnt: 0 - valLoss: 0.41289302706718445 - trainLoss: 0.4041101932525635\n",
      "cnt: 0 - valLoss: 0.41288724541664124 - trainLoss: 0.40410277247428894\n",
      "cnt: 0 - valLoss: 0.4128819406032562 - trainLoss: 0.4040953814983368\n",
      "cnt: 0 - valLoss: 0.412875771522522 - trainLoss: 0.40408796072006226\n",
      "cnt: 0 - valLoss: 0.4128701686859131 - trainLoss: 0.4040805697441101\n",
      "cnt: 0 - valLoss: 0.412864625453949 - trainLoss: 0.40407317876815796\n",
      "cnt: 0 - valLoss: 0.41285908222198486 - trainLoss: 0.4040657579898834\n",
      "cnt: 0 - valLoss: 0.41285327076911926 - trainLoss: 0.4040583670139313\n",
      "cnt: 0 - valLoss: 0.4128473997116089 - trainLoss: 0.40405088663101196\n",
      "cnt: 0 - valLoss: 0.4128420352935791 - trainLoss: 0.4040435254573822\n",
      "cnt: 0 - valLoss: 0.4128359854221344 - trainLoss: 0.40403610467910767\n",
      "cnt: 0 - valLoss: 0.4128305912017822 - trainLoss: 0.4040287137031555\n",
      "cnt: 0 - valLoss: 0.41282495856285095 - trainLoss: 0.40402132272720337\n",
      "cnt: 0 - valLoss: 0.4128197133541107 - trainLoss: 0.4040139317512512\n",
      "cnt: 0 - valLoss: 0.4128141403198242 - trainLoss: 0.4040065109729767\n",
      "cnt: 0 - valLoss: 0.41280847787857056 - trainLoss: 0.40399911999702454\n",
      "cnt: 0 - valLoss: 0.4128029942512512 - trainLoss: 0.40399169921875\n",
      "cnt: 0 - valLoss: 0.41279762983322144 - trainLoss: 0.40398433804512024\n",
      "cnt: 0 - valLoss: 0.41279202699661255 - trainLoss: 0.4039769172668457\n",
      "cnt: 0 - valLoss: 0.4127865135669708 - trainLoss: 0.40396955609321594\n",
      "cnt: 0 - valLoss: 0.41278076171875 - trainLoss: 0.4039621651172638\n",
      "cnt: 0 - valLoss: 0.41277599334716797 - trainLoss: 0.40395474433898926\n",
      "cnt: 0 - valLoss: 0.4127699136734009 - trainLoss: 0.4039473831653595\n",
      "cnt: 0 - valLoss: 0.41276463866233826 - trainLoss: 0.40393999218940735\n",
      "cnt: 0 - valLoss: 0.41275909543037415 - trainLoss: 0.4039326310157776\n",
      "cnt: 0 - valLoss: 0.41275355219841003 - trainLoss: 0.4039252698421478\n",
      "cnt: 0 - valLoss: 0.4127480685710907 - trainLoss: 0.40391790866851807\n",
      "cnt: 0 - valLoss: 0.41274234652519226 - trainLoss: 0.40391045808792114\n",
      "cnt: 0 - valLoss: 0.41273707151412964 - trainLoss: 0.40390312671661377\n",
      "cnt: 0 - valLoss: 0.412731796503067 - trainLoss: 0.40389570593833923\n",
      "cnt: 0 - valLoss: 0.41272640228271484 - trainLoss: 0.40388837456703186\n",
      "cnt: 0 - valLoss: 0.41272062063217163 - trainLoss: 0.4038810133934021\n",
      "cnt: 0 - valLoss: 0.4127148985862732 - trainLoss: 0.40387365221977234\n",
      "cnt: 0 - valLoss: 0.41270989179611206 - trainLoss: 0.4038662612438202\n",
      "cnt: 0 - valLoss: 0.4127042293548584 - trainLoss: 0.4038589298725128\n",
      "cnt: 0 - valLoss: 0.41269931197166443 - trainLoss: 0.40385153889656067\n",
      "cnt: 0 - valLoss: 0.41269373893737793 - trainLoss: 0.4038441479206085\n",
      "cnt: 0 - valLoss: 0.41268858313560486 - trainLoss: 0.403836727142334\n",
      "cnt: 0 - valLoss: 0.41268283128738403 - trainLoss: 0.4038293659687042\n",
      "cnt: 0 - valLoss: 0.41267749667167664 - trainLoss: 0.40382200479507446\n",
      "cnt: 0 - valLoss: 0.4126723110675812 - trainLoss: 0.4038146436214447\n",
      "cnt: 0 - valLoss: 0.41266682744026184 - trainLoss: 0.40380728244781494\n",
      "cnt: 0 - valLoss: 0.4126614034175873 - trainLoss: 0.4037999212741852\n",
      "cnt: 0 - valLoss: 0.4126562774181366 - trainLoss: 0.4037925601005554\n",
      "cnt: 0 - valLoss: 0.4126507341861725 - trainLoss: 0.40378516912460327\n",
      "cnt: 0 - valLoss: 0.4126458466053009 - trainLoss: 0.4037778377532959\n",
      "cnt: 0 - valLoss: 0.4126405119895935 - trainLoss: 0.40377044677734375\n",
      "cnt: 0 - valLoss: 0.41263481974601746 - trainLoss: 0.4037631154060364\n",
      "cnt: 0 - valLoss: 0.41262972354888916 - trainLoss: 0.40375572443008423\n",
      "cnt: 0 - valLoss: 0.4126247465610504 - trainLoss: 0.40374842286109924\n",
      "cnt: 0 - valLoss: 0.41261905431747437 - trainLoss: 0.4037410616874695\n",
      "cnt: 0 - valLoss: 0.4126138389110565 - trainLoss: 0.4037337005138397\n",
      "cnt: 0 - valLoss: 0.41260889172554016 - trainLoss: 0.40372633934020996\n",
      "cnt: 0 - valLoss: 0.4126037657260895 - trainLoss: 0.4037189781665802\n",
      "cnt: 0 - valLoss: 0.41259855031967163 - trainLoss: 0.4037116467952728\n",
      "cnt: 0 - valLoss: 0.41259297728538513 - trainLoss: 0.40370431542396545\n",
      "cnt: 0 - valLoss: 0.4125877618789673 - trainLoss: 0.4036969542503357\n",
      "cnt: 0 - valLoss: 0.4125828444957733 - trainLoss: 0.40368959307670593\n",
      "cnt: 0 - valLoss: 0.4125773310661316 - trainLoss: 0.40368229150772095\n",
      "cnt: 0 - valLoss: 0.41257229447364807 - trainLoss: 0.4036749303340912\n",
      "cnt: 0 - valLoss: 0.4125671088695526 - trainLoss: 0.4036675989627838\n",
      "cnt: 0 - valLoss: 0.41256198287010193 - trainLoss: 0.40366026759147644\n",
      "cnt: 0 - valLoss: 0.41255688667297363 - trainLoss: 0.40365293622016907\n",
      "cnt: 0 - valLoss: 0.4125514030456543 - trainLoss: 0.4036455750465393\n",
      "cnt: 0 - valLoss: 0.41254666447639465 - trainLoss: 0.4036382734775543\n",
      "cnt: 0 - valLoss: 0.41254156827926636 - trainLoss: 0.40363094210624695\n",
      "cnt: 0 - valLoss: 0.412535697221756 - trainLoss: 0.40362364053726196\n",
      "cnt: 0 - valLoss: 0.41253039240837097 - trainLoss: 0.4036162495613098\n",
      "cnt: 0 - valLoss: 0.412525475025177 - trainLoss: 0.40360885858535767\n",
      "cnt: 0 - valLoss: 0.41251975297927856 - trainLoss: 0.4036014974117279\n",
      "cnt: 0 - valLoss: 0.41251441836357117 - trainLoss: 0.40359410643577576\n",
      "cnt: 0 - valLoss: 0.4125092029571533 - trainLoss: 0.403586745262146\n",
      "cnt: 0 - valLoss: 0.41250374913215637 - trainLoss: 0.40357938408851624\n",
      "cnt: 0 - valLoss: 0.41249880194664 - trainLoss: 0.4035720229148865\n",
      "cnt: 0 - valLoss: 0.41249334812164307 - trainLoss: 0.40356460213661194\n",
      "cnt: 0 - valLoss: 0.41248783469200134 - trainLoss: 0.4035572409629822\n",
      "cnt: 0 - valLoss: 0.4124830663204193 - trainLoss: 0.4035499095916748\n",
      "cnt: 0 - valLoss: 0.4124777019023895 - trainLoss: 0.40354257822036743\n",
      "cnt: 0 - valLoss: 0.412472665309906 - trainLoss: 0.40353521704673767\n",
      "cnt: 0 - valLoss: 0.4124671220779419 - trainLoss: 0.4035278558731079\n",
      "cnt: 0 - valLoss: 0.4124620854854584 - trainLoss: 0.40352052450180054\n",
      "cnt: 0 - valLoss: 0.4124569594860077 - trainLoss: 0.40351319313049316\n",
      "cnt: 0 - valLoss: 0.4124510884284973 - trainLoss: 0.4035058617591858\n",
      "cnt: 0 - valLoss: 0.412445992231369 - trainLoss: 0.4034985303878784\n",
      "cnt: 0 - valLoss: 0.41244110465049744 - trainLoss: 0.40349122881889343\n",
      "cnt: 0 - valLoss: 0.4124353528022766 - trainLoss: 0.40348389744758606\n",
      "cnt: 0 - valLoss: 0.4124305844306946 - trainLoss: 0.4034765660762787\n",
      "cnt: 0 - valLoss: 0.4124254584312439 - trainLoss: 0.4034692645072937\n",
      "cnt: 0 - valLoss: 0.41241976618766785 - trainLoss: 0.40346193313598633\n",
      "cnt: 0 - valLoss: 0.4124147593975067 - trainLoss: 0.40345460176467896\n",
      "cnt: 0 - valLoss: 0.4124096930027008 - trainLoss: 0.40344732999801636\n",
      "cnt: 0 - valLoss: 0.41240406036376953 - trainLoss: 0.403439998626709\n",
      "cnt: 0 - valLoss: 0.4123992621898651 - trainLoss: 0.403432697057724\n",
      "cnt: 0 - valLoss: 0.4123939573764801 - trainLoss: 0.4034253656864166\n",
      "cnt: 0 - valLoss: 0.4123886525630951 - trainLoss: 0.40341806411743164\n",
      "cnt: 0 - valLoss: 0.41238319873809814 - trainLoss: 0.40341076254844666\n",
      "cnt: 0 - valLoss: 0.41237807273864746 - trainLoss: 0.40340349078178406\n",
      "cnt: 0 - valLoss: 0.41237306594848633 - trainLoss: 0.4033961296081543\n",
      "cnt: 0 - valLoss: 0.4123675227165222 - trainLoss: 0.4033888578414917\n",
      "cnt: 0 - valLoss: 0.41236263513565063 - trainLoss: 0.4033815562725067\n",
      "cnt: 0 - valLoss: 0.41235730051994324 - trainLoss: 0.40337425470352173\n",
      "cnt: 0 - valLoss: 0.4123518168926239 - trainLoss: 0.40336689352989197\n",
      "cnt: 0 - valLoss: 0.41234707832336426 - trainLoss: 0.40335962176322937\n",
      "cnt: 0 - valLoss: 0.4123417139053345 - trainLoss: 0.403352290391922\n",
      "cnt: 0 - valLoss: 0.4123363196849823 - trainLoss: 0.403344988822937\n",
      "cnt: 0 - valLoss: 0.41233158111572266 - trainLoss: 0.40333765745162964\n",
      "cnt: 0 - valLoss: 0.412325918674469 - trainLoss: 0.40333038568496704\n",
      "cnt: 0 - valLoss: 0.4123208224773407 - trainLoss: 0.40332305431365967\n",
      "cnt: 0 - valLoss: 0.41231563687324524 - trainLoss: 0.4033157229423523\n",
      "cnt: 0 - valLoss: 0.41231048107147217 - trainLoss: 0.4033084213733673\n",
      "cnt: 0 - valLoss: 0.41230541467666626 - trainLoss: 0.4033011794090271\n",
      "cnt: 0 - valLoss: 0.412300169467926 - trainLoss: 0.4032938480377197\n",
      "cnt: 0 - valLoss: 0.4122951030731201 - trainLoss: 0.40328654646873474\n",
      "cnt: 0 - valLoss: 0.41228923201560974 - trainLoss: 0.40327924489974976\n",
      "cnt: 0 - valLoss: 0.4122847616672516 - trainLoss: 0.40327197313308716\n",
      "cnt: 0 - valLoss: 0.4122799336910248 - trainLoss: 0.4032646715641022\n",
      "cnt: 0 - valLoss: 0.4122745394706726 - trainLoss: 0.40325742959976196\n",
      "cnt: 0 - valLoss: 0.4122696816921234 - trainLoss: 0.403250128030777\n",
      "cnt: 0 - valLoss: 0.4122648239135742 - trainLoss: 0.40324288606643677\n",
      "cnt: 0 - valLoss: 0.41225942969322205 - trainLoss: 0.40323564410209656\n",
      "cnt: 0 - valLoss: 0.412254273891449 - trainLoss: 0.4032283425331116\n",
      "cnt: 0 - valLoss: 0.41224905848503113 - trainLoss: 0.403221070766449\n",
      "cnt: 0 - valLoss: 0.4122440814971924 - trainLoss: 0.40321382880210876\n",
      "cnt: 0 - valLoss: 0.41223886609077454 - trainLoss: 0.4032065272331238\n",
      "cnt: 0 - valLoss: 0.4122343063354492 - trainLoss: 0.40319928526878357\n",
      "cnt: 0 - valLoss: 0.41222885251045227 - trainLoss: 0.40319204330444336\n",
      "cnt: 0 - valLoss: 0.4122239351272583 - trainLoss: 0.40318480134010315\n",
      "cnt: 0 - valLoss: 0.4122191071510315 - trainLoss: 0.40317755937576294\n",
      "cnt: 0 - valLoss: 0.41221415996551514 - trainLoss: 0.40317028760910034\n",
      "cnt: 0 - valLoss: 0.4122091829776764 - trainLoss: 0.40316301584243774\n",
      "cnt: 0 - valLoss: 0.41220441460609436 - trainLoss: 0.4031558036804199\n",
      "cnt: 0 - valLoss: 0.4121997356414795 - trainLoss: 0.4031485617160797\n",
      "cnt: 0 - valLoss: 0.4121949076652527 - trainLoss: 0.4031412899494171\n",
      "cnt: 0 - valLoss: 0.4121900796890259 - trainLoss: 0.4031340777873993\n",
      "cnt: 0 - valLoss: 0.41218486428260803 - trainLoss: 0.4031268358230591\n",
      "cnt: 0 - valLoss: 0.41218048334121704 - trainLoss: 0.40311959385871887\n",
      "cnt: 0 - valLoss: 0.41217556595802307 - trainLoss: 0.40311238169670105\n",
      "cnt: 0 - valLoss: 0.4121706485748291 - trainLoss: 0.40310513973236084\n",
      "cnt: 0 - valLoss: 0.412166029214859 - trainLoss: 0.403097927570343\n",
      "cnt: 0 - valLoss: 0.412161260843277 - trainLoss: 0.4030906856060028\n",
      "cnt: 0 - valLoss: 0.4121565818786621 - trainLoss: 0.403083473443985\n",
      "cnt: 0 - valLoss: 0.4121517837047577 - trainLoss: 0.40307626128196716\n",
      "cnt: 0 - valLoss: 0.4121469259262085 - trainLoss: 0.40306901931762695\n",
      "cnt: 0 - valLoss: 0.4121415615081787 - trainLoss: 0.40306180715560913\n",
      "cnt: 0 - valLoss: 0.41213715076446533 - trainLoss: 0.4030546545982361\n",
      "cnt: 0 - valLoss: 0.4121318459510803 - trainLoss: 0.40304744243621826\n",
      "cnt: 0 - valLoss: 0.4121270775794983 - trainLoss: 0.40304023027420044\n",
      "cnt: 0 - valLoss: 0.41212233901023865 - trainLoss: 0.4030330181121826\n",
      "cnt: 0 - valLoss: 0.4121176600456238 - trainLoss: 0.4030258357524872\n",
      "cnt: 0 - valLoss: 0.41211292147636414 - trainLoss: 0.40301862359046936\n",
      "cnt: 0 - valLoss: 0.4121081829071045 - trainLoss: 0.40301141142845154\n",
      "cnt: 0 - valLoss: 0.4121023416519165 - trainLoss: 0.40300416946411133\n",
      "cnt: 0 - valLoss: 0.41209790110588074 - trainLoss: 0.4029969573020935\n",
      "cnt: 0 - valLoss: 0.41209307312965393 - trainLoss: 0.4029897451400757\n",
      "cnt: 0 - valLoss: 0.4120881259441376 - trainLoss: 0.4029825031757355\n",
      "cnt: 0 - valLoss: 0.41208335757255554 - trainLoss: 0.40297529101371765\n",
      "cnt: 0 - valLoss: 0.4120786786079407 - trainLoss: 0.40296807885169983\n",
      "cnt: 0 - valLoss: 0.4120740592479706 - trainLoss: 0.4029608964920044\n",
      "cnt: 0 - valLoss: 0.41206884384155273 - trainLoss: 0.4029536843299866\n",
      "cnt: 0 - valLoss: 0.41206398606300354 - trainLoss: 0.40294647216796875\n",
      "cnt: 0 - valLoss: 0.41205906867980957 - trainLoss: 0.4029392600059509\n",
      "cnt: 0 - valLoss: 0.4120545983314514 - trainLoss: 0.4029321074485779\n",
      "cnt: 0 - valLoss: 0.41204965114593506 - trainLoss: 0.40292486548423767\n",
      "cnt: 0 - valLoss: 0.41204503178596497 - trainLoss: 0.40291765332221985\n",
      "cnt: 0 - valLoss: 0.41204023361206055 - trainLoss: 0.4029104709625244\n",
      "cnt: 0 - valLoss: 0.4120353162288666 - trainLoss: 0.40290331840515137\n",
      "cnt: 0 - valLoss: 0.4120302200317383 - trainLoss: 0.40289613604545593\n",
      "cnt: 0 - valLoss: 0.4120255410671234 - trainLoss: 0.4028889238834381\n",
      "cnt: 0 - valLoss: 0.41202062368392944 - trainLoss: 0.4028817117214203\n",
      "cnt: 0 - valLoss: 0.4120159447193146 - trainLoss: 0.40287458896636963\n",
      "cnt: 0 - valLoss: 0.41201135516166687 - trainLoss: 0.4028673768043518\n",
      "cnt: 0 - valLoss: 0.4120067358016968 - trainLoss: 0.40286019444465637\n",
      "cnt: 0 - valLoss: 0.41200169920921326 - trainLoss: 0.40285301208496094\n",
      "cnt: 0 - valLoss: 0.4119965136051178 - trainLoss: 0.4028458595275879\n",
      "cnt: 0 - valLoss: 0.41199180483818054 - trainLoss: 0.40283870697021484\n",
      "cnt: 0 - valLoss: 0.4119871258735657 - trainLoss: 0.4028315544128418\n",
      "cnt: 0 - valLoss: 0.4119825065135956 - trainLoss: 0.402824342250824\n",
      "cnt: 0 - valLoss: 0.4119778871536255 - trainLoss: 0.4028171896934509\n",
      "cnt: 0 - valLoss: 0.4119722843170166 - trainLoss: 0.40281006693840027\n",
      "cnt: 0 - valLoss: 0.4119672477245331 - trainLoss: 0.4028029441833496\n",
      "cnt: 0 - valLoss: 0.41196244955062866 - trainLoss: 0.40279585123062134\n",
      "cnt: 0 - valLoss: 0.4119570255279541 - trainLoss: 0.40278875827789307\n",
      "cnt: 0 - valLoss: 0.41195186972618103 - trainLoss: 0.4027816653251648\n",
      "cnt: 0 - valLoss: 0.41194650530815125 - trainLoss: 0.40277454257011414\n",
      "cnt: 0 - valLoss: 0.4119418263435364 - trainLoss: 0.4027674198150635\n",
      "cnt: 0 - valLoss: 0.41193655133247375 - trainLoss: 0.4027603268623352\n",
      "cnt: 0 - valLoss: 0.41193148493766785 - trainLoss: 0.40275323390960693\n",
      "cnt: 0 - valLoss: 0.41192612051963806 - trainLoss: 0.40274614095687866\n",
      "cnt: 0 - valLoss: 0.41192108392715454 - trainLoss: 0.4027390480041504\n",
      "cnt: 0 - valLoss: 0.41191619634628296 - trainLoss: 0.4027319550514221\n",
      "cnt: 0 - valLoss: 0.4119110107421875 - trainLoss: 0.40272489190101624\n",
      "cnt: 0 - valLoss: 0.4119054675102234 - trainLoss: 0.40271779894828796\n",
      "cnt: 0 - valLoss: 0.41190052032470703 - trainLoss: 0.4027107059955597\n",
      "cnt: 0 - valLoss: 0.4118954539299011 - trainLoss: 0.4027036428451538\n",
      "cnt: 0 - valLoss: 0.4118907153606415 - trainLoss: 0.40269654989242554\n",
      "cnt: 0 - valLoss: 0.41188547015190125 - trainLoss: 0.40268945693969727\n",
      "cnt: 0 - valLoss: 0.4118804633617401 - trainLoss: 0.40268242359161377\n",
      "cnt: 0 - valLoss: 0.4118752181529999 - trainLoss: 0.4026753008365631\n",
      "cnt: 0 - valLoss: 0.41187021136283875 - trainLoss: 0.4026682376861572\n",
      "cnt: 0 - valLoss: 0.411865234375 - trainLoss: 0.40266117453575134\n",
      "cnt: 0 - valLoss: 0.411859929561615 - trainLoss: 0.40265411138534546\n",
      "cnt: 0 - valLoss: 0.41185516119003296 - trainLoss: 0.4026470184326172\n",
      "cnt: 0 - valLoss: 0.4118502140045166 - trainLoss: 0.4026399254798889\n",
      "cnt: 0 - valLoss: 0.41184502840042114 - trainLoss: 0.40263280272483826\n",
      "cnt: 0 - valLoss: 0.41184061765670776 - trainLoss: 0.40262570977211\n",
      "cnt: 0 - valLoss: 0.41183575987815857 - trainLoss: 0.4026186466217041\n",
      "cnt: 0 - valLoss: 0.4118305444717407 - trainLoss: 0.40261155366897583\n",
      "cnt: 0 - valLoss: 0.4118255078792572 - trainLoss: 0.40260449051856995\n",
      "cnt: 0 - valLoss: 0.41182059049606323 - trainLoss: 0.4025973677635193\n",
      "cnt: 0 - valLoss: 0.41181617975234985 - trainLoss: 0.4025903046131134\n",
      "cnt: 0 - valLoss: 0.41181090474128723 - trainLoss: 0.4025832414627075\n",
      "cnt: 0 - valLoss: 0.4118061661720276 - trainLoss: 0.40257614850997925\n",
      "cnt: 0 - valLoss: 0.4118012487888336 - trainLoss: 0.40256908535957336\n",
      "cnt: 0 - valLoss: 0.411796510219574 - trainLoss: 0.4025620222091675\n",
      "cnt: 0 - valLoss: 0.4117913544178009 - trainLoss: 0.4025549292564392\n",
      "cnt: 0 - valLoss: 0.4117867946624756 - trainLoss: 0.4025478959083557\n",
      "cnt: 0 - valLoss: 0.41178178787231445 - trainLoss: 0.40254080295562744\n",
      "cnt: 0 - valLoss: 0.41177669167518616 - trainLoss: 0.40253373980522156\n",
      "cnt: 0 - valLoss: 0.411771684885025 - trainLoss: 0.4025266468524933\n",
      "cnt: 0 - valLoss: 0.4117673337459564 - trainLoss: 0.4025195837020874\n",
      "cnt: 0 - valLoss: 0.4117624759674072 - trainLoss: 0.4025125205516815\n",
      "cnt: 0 - valLoss: 0.4117574691772461 - trainLoss: 0.402505487203598\n",
      "cnt: 0 - valLoss: 0.41175252199172974 - trainLoss: 0.40249839425086975\n",
      "cnt: 0 - valLoss: 0.41174745559692383 - trainLoss: 0.40249133110046387\n",
      "cnt: 0 - valLoss: 0.4117427170276642 - trainLoss: 0.402484267950058\n",
      "cnt: 0 - valLoss: 0.4117380678653717 - trainLoss: 0.4024772047996521\n",
      "cnt: 0 - valLoss: 0.4117327630519867 - trainLoss: 0.4024701714515686\n",
      "cnt: 0 - valLoss: 0.41172805428504944 - trainLoss: 0.4024631381034851\n",
      "cnt: 0 - valLoss: 0.41172313690185547 - trainLoss: 0.4024560749530792\n",
      "cnt: 0 - valLoss: 0.41171795129776 - trainLoss: 0.40244901180267334\n",
      "cnt: 0 - valLoss: 0.4117133319377899 - trainLoss: 0.40244197845458984\n",
      "cnt: 0 - valLoss: 0.41170865297317505 - trainLoss: 0.40243494510650635\n",
      "cnt: 0 - valLoss: 0.41170331835746765 - trainLoss: 0.40242788195610046\n",
      "cnt: 0 - valLoss: 0.411698579788208 - trainLoss: 0.40242084860801697\n",
      "cnt: 0 - valLoss: 0.41169384121894836 - trainLoss: 0.40241384506225586\n",
      "cnt: 0 - valLoss: 0.4116883873939514 - trainLoss: 0.40240678191185\n",
      "cnt: 0 - valLoss: 0.41168397665023804 - trainLoss: 0.40239977836608887\n",
      "cnt: 0 - valLoss: 0.4116789698600769 - trainLoss: 0.40239274501800537\n",
      "cnt: 0 - valLoss: 0.4116741120815277 - trainLoss: 0.4023857116699219\n",
      "cnt: 0 - valLoss: 0.4116688668727875 - trainLoss: 0.4023786783218384\n",
      "cnt: 0 - valLoss: 0.4116641879081726 - trainLoss: 0.4023716151714325\n",
      "cnt: 0 - valLoss: 0.41165927052497864 - trainLoss: 0.402364581823349\n",
      "cnt: 0 - valLoss: 0.4116542339324951 - trainLoss: 0.4023575782775879\n",
      "cnt: 0 - valLoss: 0.4116491675376892 - trainLoss: 0.40235060453414917\n",
      "cnt: 0 - valLoss: 0.4116443395614624 - trainLoss: 0.4023435711860657\n",
      "cnt: 0 - valLoss: 0.41163939237594604 - trainLoss: 0.40233656764030457\n",
      "cnt: 0 - valLoss: 0.4116344451904297 - trainLoss: 0.40232956409454346\n",
      "cnt: 0 - valLoss: 0.41162949800491333 - trainLoss: 0.40232256054878235\n",
      "cnt: 0 - valLoss: 0.4116244614124298 - trainLoss: 0.40231558680534363\n",
      "cnt: 0 - valLoss: 0.411619633436203 - trainLoss: 0.40230855345726013\n",
      "cnt: 0 - valLoss: 0.4116145372390747 - trainLoss: 0.4023015797138214\n",
      "cnt: 0 - valLoss: 0.41160979866981506 - trainLoss: 0.4022946059703827\n",
      "cnt: 0 - valLoss: 0.4116048812866211 - trainLoss: 0.4022876024246216\n",
      "cnt: 0 - valLoss: 0.4115997552871704 - trainLoss: 0.4022805988788605\n",
      "cnt: 0 - valLoss: 0.41159510612487793 - trainLoss: 0.40227359533309937\n",
      "cnt: 0 - valLoss: 0.41159000992774963 - trainLoss: 0.40226662158966064\n",
      "cnt: 0 - valLoss: 0.41158491373062134 - trainLoss: 0.40225961804389954\n",
      "cnt: 0 - valLoss: 0.41158047318458557 - trainLoss: 0.4022526443004608\n",
      "cnt: 0 - valLoss: 0.4115753471851349 - trainLoss: 0.4022456407546997\n",
      "cnt: 0 - valLoss: 0.41157039999961853 - trainLoss: 0.4022386968135834\n",
      "cnt: 0 - valLoss: 0.4115656614303589 - trainLoss: 0.40223169326782227\n",
      "cnt: 0 - valLoss: 0.41156092286109924 - trainLoss: 0.40222468972206116\n",
      "cnt: 0 - valLoss: 0.41155555844306946 - trainLoss: 0.40221771597862244\n",
      "cnt: 0 - valLoss: 0.4115510880947113 - trainLoss: 0.4022107720375061\n",
      "cnt: 0 - valLoss: 0.41154617071151733 - trainLoss: 0.402203768491745\n",
      "cnt: 0 - valLoss: 0.4115413725376129 - trainLoss: 0.40219682455062866\n",
      "cnt: 0 - valLoss: 0.41153639554977417 - trainLoss: 0.40218988060951233\n",
      "cnt: 0 - valLoss: 0.4115316569805145 - trainLoss: 0.4021829068660736\n",
      "cnt: 0 - valLoss: 0.41152650117874146 - trainLoss: 0.4021759629249573\n",
      "cnt: 0 - valLoss: 0.41152146458625793 - trainLoss: 0.4021691083908081\n",
      "cnt: 0 - valLoss: 0.4115165174007416 - trainLoss: 0.40216222405433655\n",
      "cnt: 0 - valLoss: 0.41151130199432373 - trainLoss: 0.4021553695201874\n",
      "cnt: 0 - valLoss: 0.4115063548088074 - trainLoss: 0.40214845538139343\n",
      "cnt: 0 - valLoss: 0.4115014970302582 - trainLoss: 0.4021415710449219\n",
      "cnt: 0 - valLoss: 0.4114969074726105 - trainLoss: 0.4021347165107727\n",
      "cnt: 0 - valLoss: 0.41149210929870605 - trainLoss: 0.40212783217430115\n",
      "cnt: 0 - valLoss: 0.41148707270622253 - trainLoss: 0.402120977640152\n",
      "cnt: 0 - valLoss: 0.4114823341369629 - trainLoss: 0.4021140933036804\n",
      "cnt: 0 - valLoss: 0.4114774465560913 - trainLoss: 0.40210720896720886\n",
      "cnt: 0 - valLoss: 0.41147324442863464 - trainLoss: 0.4021003544330597\n",
      "cnt: 0 - valLoss: 0.4114679992198944 - trainLoss: 0.4020934998989105\n",
      "cnt: 0 - valLoss: 0.41146320104599 - trainLoss: 0.40208661556243896\n",
      "cnt: 0 - valLoss: 0.4114586114883423 - trainLoss: 0.4020797908306122\n",
      "cnt: 0 - valLoss: 0.4114542007446289 - trainLoss: 0.402072936296463\n",
      "cnt: 0 - valLoss: 0.41144898533821106 - trainLoss: 0.40206611156463623\n",
      "cnt: 0 - valLoss: 0.41144487261772156 - trainLoss: 0.40205925703048706\n",
      "cnt: 0 - valLoss: 0.4114401042461395 - trainLoss: 0.4020524322986603\n",
      "cnt: 0 - valLoss: 0.4114353656768799 - trainLoss: 0.4020456075668335\n",
      "cnt: 0 - valLoss: 0.4114309549331665 - trainLoss: 0.4020388126373291\n",
      "cnt: 0 - valLoss: 0.41142627596855164 - trainLoss: 0.4020319879055023\n",
      "cnt: 0 - valLoss: 0.4114213287830353 - trainLoss: 0.40202516317367554\n",
      "cnt: 0 - valLoss: 0.41141656041145325 - trainLoss: 0.4020184278488159\n",
      "cnt: 0 - valLoss: 0.41141167283058167 - trainLoss: 0.4020116329193115\n",
      "cnt: 0 - valLoss: 0.4114067256450653 - trainLoss: 0.4020048677921295\n",
      "cnt: 0 - valLoss: 0.41140231490135193 - trainLoss: 0.4019981026649475\n",
      "cnt: 0 - valLoss: 0.41139742732048035 - trainLoss: 0.4019913375377655\n",
      "cnt: 0 - valLoss: 0.4113922119140625 - trainLoss: 0.4019845426082611\n",
      "cnt: 0 - valLoss: 0.41138795018196106 - trainLoss: 0.4019778072834015\n",
      "cnt: 0 - valLoss: 0.4113828241825104 - trainLoss: 0.4019710421562195\n",
      "cnt: 0 - valLoss: 0.4113779366016388 - trainLoss: 0.4019642770290375\n",
      "cnt: 0 - valLoss: 0.41137370467185974 - trainLoss: 0.40195751190185547\n",
      "cnt: 0 - valLoss: 0.4113685190677643 - trainLoss: 0.40195074677467346\n",
      "cnt: 0 - valLoss: 0.41136372089385986 - trainLoss: 0.40194401144981384\n",
      "cnt: 0 - valLoss: 0.4113595187664032 - trainLoss: 0.4019372761249542\n",
      "cnt: 0 - valLoss: 0.4113542437553406 - trainLoss: 0.4019305109977722\n",
      "cnt: 0 - valLoss: 0.41134950518608093 - trainLoss: 0.4019237756729126\n",
      "cnt: 0 - valLoss: 0.4113451838493347 - trainLoss: 0.401917040348053\n",
      "cnt: 0 - valLoss: 0.4113403260707855 - trainLoss: 0.40191027522087097\n",
      "cnt: 0 - valLoss: 0.4113353490829468 - trainLoss: 0.40190351009368896\n",
      "cnt: 0 - valLoss: 0.41133108735084534 - trainLoss: 0.40189680457115173\n",
      "cnt: 0 - valLoss: 0.411326140165329 - trainLoss: 0.4018900394439697\n",
      "cnt: 0 - valLoss: 0.41132113337516785 - trainLoss: 0.4018833041191101\n",
      "cnt: 0 - valLoss: 0.4113162159919739 - trainLoss: 0.4018765687942505\n",
      "cnt: 0 - valLoss: 0.4113121032714844 - trainLoss: 0.40186983346939087\n",
      "cnt: 0 - valLoss: 0.4113070070743561 - trainLoss: 0.40186309814453125\n",
      "cnt: 0 - valLoss: 0.411302387714386 - trainLoss: 0.40185636281967163\n",
      "cnt: 0 - valLoss: 0.4112982451915741 - trainLoss: 0.4018496572971344\n",
      "cnt: 0 - valLoss: 0.41129350662231445 - trainLoss: 0.4018428921699524\n",
      "cnt: 0 - valLoss: 0.4112887680530548 - trainLoss: 0.40183618664741516\n",
      "cnt: 0 - valLoss: 0.411284476518631 - trainLoss: 0.40182948112487793\n",
      "cnt: 0 - valLoss: 0.4112797975540161 - trainLoss: 0.4018227458000183\n",
      "cnt: 0 - valLoss: 0.41127514839172363 - trainLoss: 0.4018160402774811\n",
      "cnt: 0 - valLoss: 0.41127026081085205 - trainLoss: 0.40180936455726624\n",
      "cnt: 0 - valLoss: 0.41126611828804016 - trainLoss: 0.401802659034729\n",
      "cnt: 0 - valLoss: 0.4112614691257477 - trainLoss: 0.40179601311683655\n",
      "cnt: 0 - valLoss: 0.4112565815448761 - trainLoss: 0.4017893075942993\n",
      "cnt: 0 - valLoss: 0.41125252842903137 - trainLoss: 0.4017826318740845\n",
      "cnt: 0 - valLoss: 0.41124752163887024 - trainLoss: 0.40177592635154724\n",
      "cnt: 0 - valLoss: 0.4112429916858673 - trainLoss: 0.4017692804336548\n",
      "cnt: 0 - valLoss: 0.41123902797698975 - trainLoss: 0.40176260471343994\n",
      "cnt: 0 - valLoss: 0.4112341105937958 - trainLoss: 0.4017559587955475\n",
      "cnt: 0 - valLoss: 0.41122958064079285 - trainLoss: 0.40174928307533264\n",
      "cnt: 0 - valLoss: 0.41122499108314514 - trainLoss: 0.4017426073551178\n",
      "cnt: 0 - valLoss: 0.411220908164978 - trainLoss: 0.40173593163490295\n",
      "cnt: 0 - valLoss: 0.4112163186073303 - trainLoss: 0.4017292559146881\n",
      "cnt: 0 - valLoss: 0.41121146082878113 - trainLoss: 0.40172258019447327\n",
      "cnt: 0 - valLoss: 0.41120749711990356 - trainLoss: 0.4017159044742584\n",
      "cnt: 0 - valLoss: 0.4112030863761902 - trainLoss: 0.4017092287540436\n",
      "cnt: 0 - valLoss: 0.4111981689929962 - trainLoss: 0.40170255303382874\n",
      "cnt: 0 - valLoss: 0.41119369864463806 - trainLoss: 0.4016958773136139\n",
      "cnt: 0 - valLoss: 0.4111897051334381 - trainLoss: 0.40168920159339905\n",
      "cnt: 0 - valLoss: 0.4111851751804352 - trainLoss: 0.4016825258731842\n",
      "cnt: 0 - valLoss: 0.411180704832077 - trainLoss: 0.401675820350647\n",
      "cnt: 0 - valLoss: 0.4111759066581726 - trainLoss: 0.4016691744327545\n",
      "cnt: 0 - valLoss: 0.4111720621585846 - trainLoss: 0.4016624987125397\n",
      "cnt: 0 - valLoss: 0.4111675024032593 - trainLoss: 0.40165582299232483\n",
      "cnt: 0 - valLoss: 0.4111630320549011 - trainLoss: 0.4016491174697876\n",
      "cnt: 0 - valLoss: 0.41115912795066833 - trainLoss: 0.40164244174957275\n",
      "cnt: 0 - valLoss: 0.41115444898605347 - trainLoss: 0.4016357362270355\n",
      "cnt: 0 - valLoss: 0.4111500084400177 - trainLoss: 0.4016290307044983\n",
      "cnt: 0 - valLoss: 0.41114571690559387 - trainLoss: 0.40162235498428345\n",
      "cnt: 0 - valLoss: 0.4111413359642029 - trainLoss: 0.4016156792640686\n",
      "cnt: 0 - valLoss: 0.41113710403442383 - trainLoss: 0.40160900354385376\n",
      "cnt: 0 - valLoss: 0.41113245487213135 - trainLoss: 0.40160229802131653\n",
      "cnt: 0 - valLoss: 0.41112813353538513 - trainLoss: 0.4015956223011017\n",
      "cnt: 0 - valLoss: 0.41112416982650757 - trainLoss: 0.40158891677856445\n",
      "cnt: 0 - valLoss: 0.4111201763153076 - trainLoss: 0.4015822410583496\n",
      "cnt: 0 - valLoss: 0.4111163020133972 - trainLoss: 0.4015755355358124\n",
      "cnt: 0 - valLoss: 0.4111121594905853 - trainLoss: 0.401568740606308\n",
      "cnt: 0 - valLoss: 0.4111088514328003 - trainLoss: 0.40156200528144836\n",
      "cnt: 0 - valLoss: 0.4111047387123108 - trainLoss: 0.40155524015426636\n",
      "cnt: 0 - valLoss: 0.4111006259918213 - trainLoss: 0.40154847502708435\n",
      "cnt: 0 - valLoss: 0.41109728813171387 - trainLoss: 0.4015417695045471\n",
      "cnt: 0 - valLoss: 0.4110930860042572 - trainLoss: 0.4015350639820099\n",
      "cnt: 0 - valLoss: 0.4110892415046692 - trainLoss: 0.40152832865715027\n",
      "cnt: 0 - valLoss: 0.4110853672027588 - trainLoss: 0.40152159333229065\n",
      "cnt: 0 - valLoss: 0.41108205914497375 - trainLoss: 0.40151485800743103\n",
      "cnt: 0 - valLoss: 0.41107821464538574 - trainLoss: 0.4015081226825714\n",
      "cnt: 0 - valLoss: 0.41107407212257385 - trainLoss: 0.4015013873577118\n",
      "cnt: 0 - valLoss: 0.4110702574253082 - trainLoss: 0.4014946520328522\n",
      "cnt: 0 - valLoss: 0.41106703877449036 - trainLoss: 0.40148794651031494\n",
      "cnt: 0 - valLoss: 0.4110628664493561 - trainLoss: 0.40148118138313293\n",
      "cnt: 0 - valLoss: 0.41105905175209045 - trainLoss: 0.4014744758605957\n",
      "cnt: 0 - valLoss: 0.4110548794269562 - trainLoss: 0.40146777033805847\n",
      "cnt: 0 - valLoss: 0.41105154156684875 - trainLoss: 0.40146106481552124\n",
      "cnt: 0 - valLoss: 0.4110476076602936 - trainLoss: 0.4014543294906616\n",
      "cnt: 0 - valLoss: 0.4110434353351593 - trainLoss: 0.4014476239681244\n",
      "cnt: 0 - valLoss: 0.4110402464866638 - trainLoss: 0.40144091844558716\n",
      "cnt: 0 - valLoss: 0.4110362231731415 - trainLoss: 0.40143418312072754\n",
      "cnt: 0 - valLoss: 0.4110320508480072 - trainLoss: 0.4014274775981903\n",
      "cnt: 0 - valLoss: 0.4110281765460968 - trainLoss: 0.4014207720756531\n",
      "cnt: 0 - valLoss: 0.41102492809295654 - trainLoss: 0.40141406655311584\n",
      "cnt: 0 - valLoss: 0.41102084517478943 - trainLoss: 0.4014073610305786\n",
      "cnt: 0 - valLoss: 0.4110168516635895 - trainLoss: 0.401400625705719\n",
      "cnt: 0 - valLoss: 0.4110126495361328 - trainLoss: 0.40139394998550415\n",
      "cnt: 0 - valLoss: 0.4110093116760254 - trainLoss: 0.4013872444629669\n",
      "cnt: 0 - valLoss: 0.41100531816482544 - trainLoss: 0.4013805091381073\n",
      "cnt: 0 - valLoss: 0.4110013246536255 - trainLoss: 0.40137383341789246\n",
      "cnt: 0 - valLoss: 0.4109973609447479 - trainLoss: 0.4013671278953552\n",
      "cnt: 0 - valLoss: 0.4109939634799957 - trainLoss: 0.4013604521751404\n",
      "cnt: 0 - valLoss: 0.4109898805618286 - trainLoss: 0.40135377645492554\n",
      "cnt: 0 - valLoss: 0.4109858274459839 - trainLoss: 0.4013470709323883\n",
      "cnt: 0 - valLoss: 0.4109823703765869 - trainLoss: 0.40134039521217346\n",
      "cnt: 0 - valLoss: 0.4109784662723541 - trainLoss: 0.4013337194919586\n",
      "cnt: 0 - valLoss: 0.41097450256347656 - trainLoss: 0.4013270139694214\n",
      "cnt: 0 - valLoss: 0.41097038984298706 - trainLoss: 0.40132030844688416\n",
      "cnt: 0 - valLoss: 0.410967081785202 - trainLoss: 0.4013136327266693\n",
      "cnt: 0 - valLoss: 0.41096287965774536 - trainLoss: 0.40130704641342163\n",
      "cnt: 0 - valLoss: 0.4109584391117096 - trainLoss: 0.4013004004955292\n",
      "cnt: 0 - valLoss: 0.4109542667865753 - trainLoss: 0.4012937843799591\n",
      "cnt: 0 - valLoss: 0.410950630903244 - trainLoss: 0.4012871980667114\n",
      "cnt: 0 - valLoss: 0.41094645857810974 - trainLoss: 0.40128058195114136\n",
      "cnt: 0 - valLoss: 0.4109421670436859 - trainLoss: 0.4012739956378937\n",
      "cnt: 0 - valLoss: 0.41093847155570984 - trainLoss: 0.4012673795223236\n",
      "cnt: 0 - valLoss: 0.41093429923057556 - trainLoss: 0.40126076340675354\n",
      "cnt: 0 - valLoss: 0.41092997789382935 - trainLoss: 0.40125417709350586\n",
      "cnt: 0 - valLoss: 0.41092565655708313 - trainLoss: 0.4012475609779358\n",
      "cnt: 0 - valLoss: 0.4109220802783966 - trainLoss: 0.4012410044670105\n",
      "cnt: 0 - valLoss: 0.4109179675579071 - trainLoss: 0.40123438835144043\n",
      "cnt: 0 - valLoss: 0.41091346740722656 - trainLoss: 0.40122780203819275\n",
      "cnt: 0 - valLoss: 0.41090935468673706 - trainLoss: 0.4012211859226227\n",
      "cnt: 0 - valLoss: 0.4109056890010834 - trainLoss: 0.4012146294116974\n",
      "cnt: 0 - valLoss: 0.41090157628059387 - trainLoss: 0.4012080132961273\n",
      "cnt: 0 - valLoss: 0.41089779138565063 - trainLoss: 0.401201456785202\n",
      "cnt: 0 - valLoss: 0.4108932614326477 - trainLoss: 0.40119487047195435\n",
      "cnt: 0 - valLoss: 0.4108901023864746 - trainLoss: 0.40118831396102905\n",
      "cnt: 0 - valLoss: 0.41088593006134033 - trainLoss: 0.401181697845459\n",
      "cnt: 0 - valLoss: 0.41088250279426575 - trainLoss: 0.4011751413345337\n",
      "cnt: 0 - valLoss: 0.4108790457248688 - trainLoss: 0.4011683762073517\n",
      "cnt: 0 - valLoss: 0.4108760356903076 - trainLoss: 0.40116167068481445\n",
      "cnt: 0 - valLoss: 0.4108721911907196 - trainLoss: 0.4011549949645996\n",
      "cnt: 0 - valLoss: 0.41086840629577637 - trainLoss: 0.4011482298374176\n",
      "cnt: 0 - valLoss: 0.41086524724960327 - trainLoss: 0.40114155411720276\n",
      "cnt: 0 - valLoss: 0.41086170077323914 - trainLoss: 0.4011348485946655\n",
      "cnt: 0 - valLoss: 0.4108580946922302 - trainLoss: 0.4011281430721283\n",
      "cnt: 0 - valLoss: 0.4108541011810303 - trainLoss: 0.40112143754959106\n",
      "cnt: 0 - valLoss: 0.4108511209487915 - trainLoss: 0.40111473202705383\n",
      "cnt: 0 - valLoss: 0.4108470678329468 - trainLoss: 0.401108056306839\n",
      "cnt: 0 - valLoss: 0.41084328293800354 - trainLoss: 0.40110138058662415\n",
      "cnt: 0 - valLoss: 0.4108394682407379 - trainLoss: 0.4010946750640869\n",
      "cnt: 0 - valLoss: 0.41083621978759766 - trainLoss: 0.4010879695415497\n",
      "cnt: 0 - valLoss: 0.4108326733112335 - trainLoss: 0.40108129382133484\n",
      "cnt: 0 - valLoss: 0.4108281135559082 - trainLoss: 0.40107461810112\n",
      "cnt: 0 - valLoss: 0.41082489490509033 - trainLoss: 0.40106797218322754\n",
      "cnt: 0 - valLoss: 0.41082149744033813 - trainLoss: 0.4010612964630127\n",
      "cnt: 0 - valLoss: 0.4108176529407501 - trainLoss: 0.40105465054512024\n",
      "cnt: 0 - valLoss: 0.4108137786388397 - trainLoss: 0.4010479748249054\n",
      "cnt: 0 - valLoss: 0.4108102321624756 - trainLoss: 0.40104132890701294\n",
      "cnt: 0 - valLoss: 0.4108065962791443 - trainLoss: 0.4010346829891205\n",
      "cnt: 0 - valLoss: 0.41080284118652344 - trainLoss: 0.40102800726890564\n",
      "cnt: 0 - valLoss: 0.410799115896225 - trainLoss: 0.40102139115333557\n",
      "cnt: 0 - valLoss: 0.4107958674430847 - trainLoss: 0.4010147452354431\n",
      "cnt: 0 - valLoss: 0.41079214215278625 - trainLoss: 0.40100812911987305\n",
      "cnt: 0 - valLoss: 0.41078832745552063 - trainLoss: 0.4010014533996582\n",
      "cnt: 0 - valLoss: 0.4107840359210968 - trainLoss: 0.40099483728408813\n",
      "cnt: 0 - valLoss: 0.4107801020145416 - trainLoss: 0.40098822116851807\n",
      "cnt: 0 - valLoss: 0.41077637672424316 - trainLoss: 0.400981605052948\n",
      "cnt: 0 - valLoss: 0.41077256202697754 - trainLoss: 0.4009750485420227\n",
      "cnt: 0 - valLoss: 0.41076865792274475 - trainLoss: 0.400968462228775\n",
      "cnt: 0 - valLoss: 0.41076451539993286 - trainLoss: 0.40096181631088257\n",
      "cnt: 0 - valLoss: 0.4107607901096344 - trainLoss: 0.4009552597999573\n",
      "cnt: 0 - valLoss: 0.4107568562030792 - trainLoss: 0.4009486436843872\n",
      "cnt: 0 - valLoss: 0.41075319051742554 - trainLoss: 0.4009420573711395\n",
      "cnt: 0 - valLoss: 0.410749226808548 - trainLoss: 0.40093544125556946\n",
      "cnt: 0 - valLoss: 0.4107453525066376 - trainLoss: 0.40092888474464417\n",
      "cnt: 0 - valLoss: 0.41074180603027344 - trainLoss: 0.4009222984313965\n",
      "cnt: 0 - valLoss: 0.4107378125190735 - trainLoss: 0.4009156823158264\n",
      "cnt: 0 - valLoss: 0.4107341468334198 - trainLoss: 0.4009091258049011\n",
      "cnt: 0 - valLoss: 0.4107300341129303 - trainLoss: 0.40090253949165344\n",
      "cnt: 0 - valLoss: 0.41072624921798706 - trainLoss: 0.40089595317840576\n",
      "cnt: 0 - valLoss: 0.41072264313697815 - trainLoss: 0.4008893668651581\n",
      "cnt: 0 - valLoss: 0.41071897745132446 - trainLoss: 0.4008827805519104\n",
      "cnt: 0 - valLoss: 0.4107154309749603 - trainLoss: 0.4008762538433075\n",
      "cnt: 0 - valLoss: 0.41071105003356934 - trainLoss: 0.4008696377277374\n",
      "cnt: 0 - valLoss: 0.41070762276649475 - trainLoss: 0.40086305141448975\n",
      "cnt: 0 - valLoss: 0.41070425510406494 - trainLoss: 0.40085652470588684\n",
      "cnt: 0 - valLoss: 0.4107009470462799 - trainLoss: 0.40084996819496155\n",
      "cnt: 0 - valLoss: 0.41069653630256653 - trainLoss: 0.4008433520793915\n",
      "cnt: 0 - valLoss: 0.4106929898262024 - trainLoss: 0.4008367657661438\n",
      "cnt: 0 - valLoss: 0.41068994998931885 - trainLoss: 0.4008302092552185\n",
      "cnt: 0 - valLoss: 0.4106861352920532 - trainLoss: 0.4008236527442932\n",
      "cnt: 0 - valLoss: 0.4106822609901428 - trainLoss: 0.40081703662872314\n",
      "cnt: 0 - valLoss: 0.4106789231300354 - trainLoss: 0.40081045031547546\n",
      "cnt: 0 - valLoss: 0.41067543625831604 - trainLoss: 0.40080389380455017\n",
      "cnt: 0 - valLoss: 0.41067180037498474 - trainLoss: 0.4007973074913025\n",
      "cnt: 0 - valLoss: 0.4106684923171997 - trainLoss: 0.4007907211780548\n",
      "cnt: 0 - valLoss: 0.41066476702690125 - trainLoss: 0.4007841646671295\n",
      "cnt: 0 - valLoss: 0.4106612801551819 - trainLoss: 0.4007776081562042\n",
      "cnt: 0 - valLoss: 0.4106577932834625 - trainLoss: 0.40077102184295654\n",
      "cnt: 0 - valLoss: 0.41065433621406555 - trainLoss: 0.40076449513435364\n",
      "cnt: 0 - valLoss: 0.4106506407260895 - trainLoss: 0.40075790882110596\n",
      "cnt: 0 - valLoss: 0.41064703464508057 - trainLoss: 0.40075135231018066\n",
      "cnt: 0 - valLoss: 0.4106440842151642 - trainLoss: 0.400744765996933\n",
      "cnt: 0 - valLoss: 0.4106403589248657 - trainLoss: 0.4007382094860077\n",
      "cnt: 0 - valLoss: 0.4106368124485016 - trainLoss: 0.4007316827774048\n",
      "cnt: 0 - valLoss: 0.41063329577445984 - trainLoss: 0.4007251262664795\n",
      "cnt: 0 - valLoss: 0.4106299579143524 - trainLoss: 0.4007185399532318\n",
      "cnt: 0 - valLoss: 0.41062676906585693 - trainLoss: 0.4007120132446289\n",
      "cnt: 0 - valLoss: 0.4106222987174988 - trainLoss: 0.4007054567337036\n",
      "cnt: 0 - valLoss: 0.4106193780899048 - trainLoss: 0.4006989300251007\n",
      "cnt: 0 - valLoss: 0.41061604022979736 - trainLoss: 0.4006923735141754\n",
      "cnt: 0 - valLoss: 0.41061291098594666 - trainLoss: 0.4006858170032501\n",
      "cnt: 0 - valLoss: 0.4106093943119049 - trainLoss: 0.40067926049232483\n",
      "cnt: 0 - valLoss: 0.41060584783554077 - trainLoss: 0.4006727337837219\n",
      "cnt: 0 - valLoss: 0.41060206294059753 - trainLoss: 0.40066617727279663\n",
      "cnt: 0 - valLoss: 0.4105992019176483 - trainLoss: 0.4006596505641937\n",
      "cnt: 0 - valLoss: 0.41059577465057373 - trainLoss: 0.4006531238555908\n",
      "cnt: 0 - valLoss: 0.4105915427207947 - trainLoss: 0.4006465673446655\n",
      "cnt: 0 - valLoss: 0.4105883538722992 - trainLoss: 0.4006400406360626\n",
      "cnt: 0 - valLoss: 0.4105847179889679 - trainLoss: 0.40063348412513733\n",
      "cnt: 0 - valLoss: 0.4105815887451172 - trainLoss: 0.4006269574165344\n",
      "cnt: 0 - valLoss: 0.41057708859443665 - trainLoss: 0.4006204605102539\n",
      "cnt: 0 - valLoss: 0.4105738401412964 - trainLoss: 0.4006139636039734\n",
      "cnt: 0 - valLoss: 0.41057029366493225 - trainLoss: 0.4006074368953705\n",
      "cnt: 0 - valLoss: 0.41056689620018005 - trainLoss: 0.4006009101867676\n",
      "cnt: 0 - valLoss: 0.41056278347969055 - trainLoss: 0.40059441328048706\n",
      "cnt: 0 - valLoss: 0.41055867075920105 - trainLoss: 0.40058791637420654\n",
      "cnt: 0 - valLoss: 0.4105556607246399 - trainLoss: 0.4005814492702484\n",
      "cnt: 0 - valLoss: 0.41055217385292053 - trainLoss: 0.40057504177093506\n",
      "cnt: 0 - valLoss: 0.41054779291152954 - trainLoss: 0.4005686640739441\n",
      "cnt: 0 - valLoss: 0.4105442464351654 - trainLoss: 0.4005622863769531\n",
      "cnt: 0 - valLoss: 0.41054099798202515 - trainLoss: 0.40055596828460693\n",
      "cnt: 0 - valLoss: 0.4105372130870819 - trainLoss: 0.40054962038993835\n",
      "cnt: 0 - valLoss: 0.4105333089828491 - trainLoss: 0.40054330229759216\n",
      "cnt: 0 - valLoss: 0.41052961349487305 - trainLoss: 0.4005369544029236\n",
      "cnt: 0 - valLoss: 0.4105256497859955 - trainLoss: 0.4005306363105774\n",
      "cnt: 0 - valLoss: 0.4105224311351776 - trainLoss: 0.4005243182182312\n",
      "cnt: 0 - valLoss: 0.4105180501937866 - trainLoss: 0.4005179703235626\n",
      "cnt: 0 - valLoss: 0.410514771938324 - trainLoss: 0.4005116820335388\n",
      "cnt: 0 - valLoss: 0.4105110168457031 - trainLoss: 0.40050530433654785\n",
      "cnt: 0 - valLoss: 0.4105070233345032 - trainLoss: 0.40049901604652405\n",
      "cnt: 0 - valLoss: 0.4105033874511719 - trainLoss: 0.40049269795417786\n",
      "cnt: 0 - valLoss: 0.41049981117248535 - trainLoss: 0.4004863202571869\n",
      "cnt: 0 - valLoss: 0.41049644351005554 - trainLoss: 0.4004800319671631\n",
      "cnt: 0 - valLoss: 0.4104920029640198 - trainLoss: 0.4004737138748169\n",
      "cnt: 0 - valLoss: 0.41048914194107056 - trainLoss: 0.4004673957824707\n",
      "cnt: 0 - valLoss: 0.4104849696159363 - trainLoss: 0.4004611074924469\n",
      "cnt: 0 - valLoss: 0.4104819595813751 - trainLoss: 0.4004547595977783\n",
      "cnt: 0 - valLoss: 0.4104773700237274 - trainLoss: 0.4004484713077545\n",
      "cnt: 0 - valLoss: 0.41047403216362 - trainLoss: 0.4004421532154083\n",
      "cnt: 0 - valLoss: 0.4104708433151245 - trainLoss: 0.40043583512306213\n",
      "cnt: 0 - valLoss: 0.41046643257141113 - trainLoss: 0.40042954683303833\n",
      "cnt: 0 - valLoss: 0.4104635715484619 - trainLoss: 0.4004232585430145\n",
      "cnt: 0 - valLoss: 0.41045936942100525 - trainLoss: 0.40041691064834595\n",
      "cnt: 0 - valLoss: 0.41045600175857544 - trainLoss: 0.40041062235832214\n",
      "cnt: 0 - valLoss: 0.4104525148868561 - trainLoss: 0.40040430426597595\n",
      "cnt: 0 - valLoss: 0.41044872999191284 - trainLoss: 0.40039804577827454\n",
      "cnt: 0 - valLoss: 0.4104451537132263 - trainLoss: 0.40039175748825073\n",
      "cnt: 0 - valLoss: 0.4104408323764801 - trainLoss: 0.4003855288028717\n",
      "cnt: 0 - valLoss: 0.41043737530708313 - trainLoss: 0.4003792703151703\n",
      "cnt: 0 - valLoss: 0.41043373942375183 - trainLoss: 0.40037304162979126\n",
      "cnt: 0 - valLoss: 0.4104298949241638 - trainLoss: 0.40036681294441223\n",
      "cnt: 0 - valLoss: 0.4104257822036743 - trainLoss: 0.4003605544567108\n",
      "cnt: 0 - valLoss: 0.41042232513427734 - trainLoss: 0.4003543257713318\n",
      "cnt: 0 - valLoss: 0.41041889786720276 - trainLoss: 0.40034806728363037\n",
      "cnt: 0 - valLoss: 0.41041451692581177 - trainLoss: 0.40034186840057373\n",
      "cnt: 0 - valLoss: 0.41041097044944763 - trainLoss: 0.4003356397151947\n",
      "cnt: 0 - valLoss: 0.4104076623916626 - trainLoss: 0.40032944083213806\n",
      "cnt: 0 - valLoss: 0.4104042053222656 - trainLoss: 0.40032318234443665\n",
      "cnt: 0 - valLoss: 0.41039982438087463 - trainLoss: 0.40031692385673523\n",
      "cnt: 0 - valLoss: 0.41039592027664185 - trainLoss: 0.4003107249736786\n",
      "cnt: 0 - valLoss: 0.4103926122188568 - trainLoss: 0.40030452609062195\n",
      "cnt: 0 - valLoss: 0.41038963198661804 - trainLoss: 0.40029826760292053\n",
      "cnt: 0 - valLoss: 0.41038504242897034 - trainLoss: 0.4002920687198639\n",
      "cnt: 0 - valLoss: 0.4103819727897644 - trainLoss: 0.40028586983680725\n",
      "cnt: 0 - valLoss: 0.410378634929657 - trainLoss: 0.4002797603607178\n",
      "cnt: 0 - valLoss: 0.41037455201148987 - trainLoss: 0.4002735912799835\n",
      "cnt: 0 - valLoss: 0.41037118434906006 - trainLoss: 0.40026748180389404\n",
      "cnt: 0 - valLoss: 0.4103674590587616 - trainLoss: 0.4002613425254822\n",
      "cnt: 0 - valLoss: 0.4103638231754303 - trainLoss: 0.4002552032470703\n",
      "cnt: 0 - valLoss: 0.4103604853153229 - trainLoss: 0.40024909377098083\n",
      "cnt: 0 - valLoss: 0.41035690903663635 - trainLoss: 0.40024295449256897\n",
      "cnt: 0 - valLoss: 0.41035330295562744 - trainLoss: 0.4002368748188019\n",
      "cnt: 0 - valLoss: 0.4103490114212036 - trainLoss: 0.4002307057380676\n",
      "cnt: 0 - valLoss: 0.41034600138664246 - trainLoss: 0.40022459626197815\n",
      "cnt: 0 - valLoss: 0.41034263372421265 - trainLoss: 0.40021848678588867\n",
      "cnt: 0 - valLoss: 0.4103391170501709 - trainLoss: 0.4002123475074768\n",
      "cnt: 0 - valLoss: 0.4103352427482605 - trainLoss: 0.40020623803138733\n",
      "cnt: 0 - valLoss: 0.41033169627189636 - trainLoss: 0.40020012855529785\n",
      "cnt: 0 - valLoss: 0.41032811999320984 - trainLoss: 0.40019404888153076\n",
      "cnt: 0 - valLoss: 0.41032519936561584 - trainLoss: 0.4001879394054413\n",
      "cnt: 0 - valLoss: 0.41032102704048157 - trainLoss: 0.4001818299293518\n",
      "cnt: 0 - valLoss: 0.41031762957572937 - trainLoss: 0.40017572045326233\n",
      "cnt: 0 - valLoss: 0.41031432151794434 - trainLoss: 0.40016961097717285\n",
      "cnt: 0 - valLoss: 0.4103110730648041 - trainLoss: 0.4001635015010834\n",
      "cnt: 0 - valLoss: 0.41030701994895935 - trainLoss: 0.4001573920249939\n",
      "cnt: 0 - valLoss: 0.41030353307724 - trainLoss: 0.4001512825489044\n",
      "cnt: 0 - valLoss: 0.4103001654148102 - trainLoss: 0.40014520287513733\n",
      "cnt: 0 - valLoss: 0.41029709577560425 - trainLoss: 0.40013909339904785\n",
      "cnt: 0 - valLoss: 0.41029295325279236 - trainLoss: 0.40013301372528076\n",
      "cnt: 0 - valLoss: 0.4102901220321655 - trainLoss: 0.4001269042491913\n",
      "cnt: 0 - valLoss: 0.4102862775325775 - trainLoss: 0.4001207947731018\n",
      "cnt: 0 - valLoss: 0.41028299927711487 - trainLoss: 0.40011468529701233\n",
      "cnt: 0 - valLoss: 0.41027936339378357 - trainLoss: 0.4001086354255676\n",
      "cnt: 0 - valLoss: 0.4102763533592224 - trainLoss: 0.40010255575180054\n",
      "cnt: 0 - valLoss: 0.41027265787124634 - trainLoss: 0.40009644627571106\n",
      "cnt: 0 - valLoss: 0.4102693498134613 - trainLoss: 0.40009039640426636\n",
      "cnt: 0 - valLoss: 0.41026562452316284 - trainLoss: 0.4000842869281769\n",
      "cnt: 0 - valLoss: 0.41026225686073303 - trainLoss: 0.4000782072544098\n",
      "cnt: 0 - valLoss: 0.410258412361145 - trainLoss: 0.4000721573829651\n",
      "cnt: 0 - valLoss: 0.41025522351264954 - trainLoss: 0.4000660479068756\n",
      "cnt: 0 - valLoss: 0.4102518856525421 - trainLoss: 0.4000599682331085\n",
      "cnt: 0 - valLoss: 0.41024860739707947 - trainLoss: 0.4000539183616638\n",
      "cnt: 0 - valLoss: 0.41024500131607056 - trainLoss: 0.40004783868789673\n",
      "cnt: 0 - valLoss: 0.4102416932582855 - trainLoss: 0.40004175901412964\n",
      "cnt: 0 - valLoss: 0.41023844480514526 - trainLoss: 0.40003567934036255\n",
      "cnt: 0 - valLoss: 0.41023507714271545 - trainLoss: 0.40002959966659546\n",
      "cnt: 0 - valLoss: 0.4102312922477722 - trainLoss: 0.40002354979515076\n",
      "cnt: 0 - valLoss: 0.41022807359695435 - trainLoss: 0.40001747012138367\n",
      "cnt: 0 - valLoss: 0.4102248549461365 - trainLoss: 0.4000113904476166\n",
      "cnt: 0 - valLoss: 0.4102213978767395 - trainLoss: 0.40000537037849426\n",
      "cnt: 0 - valLoss: 0.41021740436553955 - trainLoss: 0.39999938011169434\n",
      "cnt: 0 - valLoss: 0.41021379828453064 - trainLoss: 0.399993360042572\n",
      "cnt: 0 - valLoss: 0.4102106988430023 - trainLoss: 0.3999873399734497\n",
      "cnt: 0 - valLoss: 0.41020601987838745 - trainLoss: 0.399981290102005\n",
      "cnt: 0 - valLoss: 0.41020333766937256 - trainLoss: 0.39997532963752747\n",
      "cnt: 0 - valLoss: 0.4101998507976532 - trainLoss: 0.39996930956840515\n",
      "cnt: 0 - valLoss: 0.4101964235305786 - trainLoss: 0.3999633193016052\n",
      "cnt: 0 - valLoss: 0.41019222140312195 - trainLoss: 0.3999573290348053\n",
      "cnt: 0 - valLoss: 0.41018953919410706 - trainLoss: 0.399951308965683\n",
      "cnt: 0 - valLoss: 0.4101863503456116 - trainLoss: 0.39994531869888306\n",
      "cnt: 0 - valLoss: 0.41018202900886536 - trainLoss: 0.39993932843208313\n",
      "cnt: 0 - valLoss: 0.41017916798591614 - trainLoss: 0.3999333083629608\n",
      "cnt: 0 - valLoss: 0.4101752042770386 - trainLoss: 0.3999273478984833\n",
      "cnt: 0 - valLoss: 0.41017279028892517 - trainLoss: 0.39992135763168335\n",
      "cnt: 0 - valLoss: 0.41016876697540283 - trainLoss: 0.3999153673648834\n",
      "cnt: 0 - valLoss: 0.4101661741733551 - trainLoss: 0.3999093770980835\n",
      "cnt: 0 - valLoss: 0.4101623594760895 - trainLoss: 0.39990341663360596\n",
      "cnt: 0 - valLoss: 0.4101594388484955 - trainLoss: 0.39989742636680603\n",
      "cnt: 0 - valLoss: 0.41015565395355225 - trainLoss: 0.3998914361000061\n",
      "cnt: 0 - valLoss: 0.4101531207561493 - trainLoss: 0.39988547563552856\n",
      "cnt: 0 - valLoss: 0.4101492166519165 - trainLoss: 0.39987948536872864\n",
      "cnt: 0 - valLoss: 0.4101463258266449 - trainLoss: 0.3998735249042511\n",
      "cnt: 0 - valLoss: 0.4101427495479584 - trainLoss: 0.39986753463745117\n",
      "cnt: 0 - valLoss: 0.41013970971107483 - trainLoss: 0.39986157417297363\n",
      "cnt: 0 - valLoss: 0.41013583540916443 - trainLoss: 0.3998555839061737\n",
      "cnt: 0 - valLoss: 0.41013291478157043 - trainLoss: 0.39984962344169617\n",
      "cnt: 0 - valLoss: 0.4101293087005615 - trainLoss: 0.39984363317489624\n",
      "cnt: 0 - valLoss: 0.4101261496543884 - trainLoss: 0.3998377025127411\n",
      "cnt: 0 - valLoss: 0.41012296080589294 - trainLoss: 0.39983171224594116\n",
      "cnt: 0 - valLoss: 0.41011980175971985 - trainLoss: 0.3998257517814636\n",
      "cnt: 0 - valLoss: 0.41011613607406616 - trainLoss: 0.39981985092163086\n",
      "cnt: 0 - valLoss: 0.4101131558418274 - trainLoss: 0.39981386065483093\n",
      "cnt: 0 - valLoss: 0.41010937094688416 - trainLoss: 0.3998079299926758\n",
      "cnt: 0 - valLoss: 0.4101071357727051 - trainLoss: 0.39980193972587585\n",
      "cnt: 0 - valLoss: 0.4101032614707947 - trainLoss: 0.3997959792613983\n",
      "cnt: 0 - valLoss: 0.4101003408432007 - trainLoss: 0.3997900187969208\n",
      "cnt: 0 - valLoss: 0.41009679436683655 - trainLoss: 0.39978405833244324\n",
      "cnt: 0 - valLoss: 0.4100937843322754 - trainLoss: 0.3997781276702881\n",
      "cnt: 0 - valLoss: 0.41008999943733215 - trainLoss: 0.39977216720581055\n",
      "cnt: 0 - valLoss: 0.41008734703063965 - trainLoss: 0.3997661769390106\n",
      "cnt: 0 - valLoss: 0.4100838303565979 - trainLoss: 0.3997602164745331\n",
      "cnt: 0 - valLoss: 0.41008076071739197 - trainLoss: 0.39975425601005554\n",
      "cnt: 0 - valLoss: 0.4100780189037323 - trainLoss: 0.3997482657432556\n",
      "cnt: 0 - valLoss: 0.4100751280784607 - trainLoss: 0.3997422754764557\n",
      "cnt: 0 - valLoss: 0.4100714921951294 - trainLoss: 0.39973631501197815\n",
      "cnt: 0 - valLoss: 0.41006898880004883 - trainLoss: 0.3997303247451782\n",
      "cnt: 0 - valLoss: 0.41006532311439514 - trainLoss: 0.3997243046760559\n",
      "cnt: 0 - valLoss: 0.410062700510025 - trainLoss: 0.39971834421157837\n",
      "cnt: 0 - valLoss: 0.41005948185920715 - trainLoss: 0.39971238374710083\n",
      "cnt: 0 - valLoss: 0.410056471824646 - trainLoss: 0.3997063934803009\n",
      "cnt: 0 - valLoss: 0.41005346179008484 - trainLoss: 0.399700403213501\n",
      "cnt: 0 - valLoss: 0.41005027294158936 - trainLoss: 0.39969444274902344\n",
      "cnt: 0 - valLoss: 0.41004717350006104 - trainLoss: 0.3996884524822235\n",
      "cnt: 0 - valLoss: 0.4100446105003357 - trainLoss: 0.39968252182006836\n",
      "cnt: 0 - valLoss: 0.4100411534309387 - trainLoss: 0.39967653155326843\n",
      "cnt: 0 - valLoss: 0.4100385904312134 - trainLoss: 0.3996705710887909\n",
      "cnt: 0 - valLoss: 0.4100353717803955 - trainLoss: 0.39966458082199097\n",
      "cnt: 0 - valLoss: 0.4100321829319 - trainLoss: 0.3996586203575134\n",
      "cnt: 0 - valLoss: 0.41002926230430603 - trainLoss: 0.3996526598930359\n",
      "cnt: 0 - valLoss: 0.4100269079208374 - trainLoss: 0.39964666962623596\n",
      "cnt: 0 - valLoss: 0.4100232422351837 - trainLoss: 0.3996407091617584\n",
      "cnt: 0 - valLoss: 0.4100209176540375 - trainLoss: 0.3996347486972809\n",
      "cnt: 0 - valLoss: 0.4100174009799957 - trainLoss: 0.39962878823280334\n",
      "cnt: 0 - valLoss: 0.41001492738723755 - trainLoss: 0.3996227979660034\n",
      "cnt: 0 - valLoss: 0.4100114405155182 - trainLoss: 0.3996168375015259\n",
      "cnt: 0 - valLoss: 0.41000890731811523 - trainLoss: 0.39961087703704834\n",
      "cnt: 0 - valLoss: 0.4100056290626526 - trainLoss: 0.3996048867702484\n",
      "cnt: 0 - valLoss: 0.41000255942344666 - trainLoss: 0.39959898591041565\n",
      "cnt: 0 - valLoss: 0.4099990725517273 - trainLoss: 0.3995930552482605\n",
      "cnt: 0 - valLoss: 0.40999630093574524 - trainLoss: 0.39958715438842773\n",
      "cnt: 0 - valLoss: 0.40999239683151245 - trainLoss: 0.3995811939239502\n",
      "cnt: 0 - valLoss: 0.4099898338317871 - trainLoss: 0.39957529306411743\n",
      "cnt: 0 - valLoss: 0.4099864661693573 - trainLoss: 0.3995693325996399\n",
      "cnt: 0 - valLoss: 0.40998291969299316 - trainLoss: 0.3995634615421295\n",
      "cnt: 0 - valLoss: 0.4099797308444977 - trainLoss: 0.399557501077652\n",
      "cnt: 0 - valLoss: 0.4099770188331604 - trainLoss: 0.3995516002178192\n",
      "cnt: 0 - valLoss: 0.40997326374053955 - trainLoss: 0.3995456099510193\n",
      "cnt: 0 - valLoss: 0.4099705219268799 - trainLoss: 0.3995397388935089\n",
      "cnt: 0 - valLoss: 0.4099673628807068 - trainLoss: 0.39953383803367615\n",
      "cnt: 0 - valLoss: 0.4099642336368561 - trainLoss: 0.399527907371521\n",
      "cnt: 0 - valLoss: 0.4099613428115845 - trainLoss: 0.39952200651168823\n",
      "cnt: 0 - valLoss: 0.409957617521286 - trainLoss: 0.3995160758495331\n",
      "cnt: 0 - valLoss: 0.4099549651145935 - trainLoss: 0.3995101749897003\n",
      "cnt: 0 - valLoss: 0.4099513292312622 - trainLoss: 0.39950424432754517\n",
      "cnt: 0 - valLoss: 0.4099483788013458 - trainLoss: 0.3994983434677124\n",
      "cnt: 0 - valLoss: 0.40994489192962646 - trainLoss: 0.399492472410202\n",
      "cnt: 0 - valLoss: 0.4099421799182892 - trainLoss: 0.3994865417480469\n",
      "cnt: 0 - valLoss: 0.40993839502334595 - trainLoss: 0.3994806408882141\n",
      "cnt: 0 - valLoss: 0.4099356234073639 - trainLoss: 0.39947474002838135\n",
      "cnt: 0 - valLoss: 0.40993228554725647 - trainLoss: 0.3994688093662262\n",
      "cnt: 0 - valLoss: 0.4099297523498535 - trainLoss: 0.3994629383087158\n",
      "cnt: 0 - valLoss: 0.4099268615245819 - trainLoss: 0.39945703744888306\n",
      "cnt: 0 - valLoss: 0.4099232256412506 - trainLoss: 0.3994511365890503\n",
      "cnt: 0 - valLoss: 0.40992027521133423 - trainLoss: 0.39944520592689514\n",
      "cnt: 0 - valLoss: 0.40991678833961487 - trainLoss: 0.39943933486938477\n",
      "cnt: 0 - valLoss: 0.4099142253398895 - trainLoss: 0.399433434009552\n",
      "cnt: 0 - valLoss: 0.4099107086658478 - trainLoss: 0.39942753314971924\n",
      "cnt: 0 - valLoss: 0.40990763902664185 - trainLoss: 0.3994216322898865\n",
      "cnt: 0 - valLoss: 0.4099043309688568 - trainLoss: 0.3994157612323761\n",
      "cnt: 0 - valLoss: 0.4099011719226837 - trainLoss: 0.3994098901748657\n",
      "cnt: 0 - valLoss: 0.40989798307418823 - trainLoss: 0.39940398931503296\n",
      "cnt: 0 - valLoss: 0.4098950922489166 - trainLoss: 0.3993980884552002\n",
      "cnt: 0 - valLoss: 0.4098922610282898 - trainLoss: 0.3993922472000122\n",
      "cnt: 0 - valLoss: 0.409889280796051 - trainLoss: 0.39938634634017944\n",
      "cnt: 0 - valLoss: 0.409885972738266 - trainLoss: 0.39938050508499146\n",
      "cnt: 0 - valLoss: 0.4098834693431854 - trainLoss: 0.39937472343444824\n",
      "cnt: 0 - valLoss: 0.409879595041275 - trainLoss: 0.39936891198158264\n",
      "cnt: 0 - valLoss: 0.40987691283226013 - trainLoss: 0.39936310052871704\n",
      "cnt: 0 - valLoss: 0.4098738133907318 - trainLoss: 0.39935725927352905\n",
      "cnt: 0 - valLoss: 0.40987083315849304 - trainLoss: 0.39935147762298584\n",
      "cnt: 0 - valLoss: 0.4098672568798065 - trainLoss: 0.3993456959724426\n",
      "cnt: 0 - valLoss: 0.40986472368240356 - trainLoss: 0.399339884519577\n",
      "cnt: 0 - valLoss: 0.4098612368106842 - trainLoss: 0.3993341028690338\n",
      "cnt: 0 - valLoss: 0.40985873341560364 - trainLoss: 0.3993282914161682\n",
      "cnt: 0 - valLoss: 0.40985536575317383 - trainLoss: 0.399322509765625\n",
      "cnt: 0 - valLoss: 0.40985238552093506 - trainLoss: 0.3993166983127594\n",
      "cnt: 0 - valLoss: 0.4098489582538605 - trainLoss: 0.3993109166622162\n",
      "cnt: 0 - valLoss: 0.40984681248664856 - trainLoss: 0.3993051052093506\n",
      "cnt: 0 - valLoss: 0.40984320640563965 - trainLoss: 0.3992993235588074\n",
      "cnt: 0 - valLoss: 0.40984007716178894 - trainLoss: 0.39929357171058655\n",
      "cnt: 0 - valLoss: 0.40983709692955017 - trainLoss: 0.39928776025772095\n",
      "cnt: 0 - valLoss: 0.4098340570926666 - trainLoss: 0.39928197860717773\n",
      "cnt: 0 - valLoss: 0.4098316431045532 - trainLoss: 0.3992761969566345\n",
      "cnt: 0 - valLoss: 0.4098285436630249 - trainLoss: 0.3992704153060913\n",
      "cnt: 0 - valLoss: 0.40982523560523987 - trainLoss: 0.3992646336555481\n",
      "cnt: 0 - valLoss: 0.40982213616371155 - trainLoss: 0.3992588520050049\n",
      "cnt: 0 - valLoss: 0.40981948375701904 - trainLoss: 0.3992530405521393\n",
      "cnt: 0 - valLoss: 0.4098161458969116 - trainLoss: 0.39924728870391846\n",
      "cnt: 0 - valLoss: 0.40981271862983704 - trainLoss: 0.39924150705337524\n",
      "cnt: 0 - valLoss: 0.4098100960254669 - trainLoss: 0.3992357850074768\n",
      "cnt: 0 - valLoss: 0.4098066985607147 - trainLoss: 0.3992299437522888\n",
      "cnt: 0 - valLoss: 0.4098040759563446 - trainLoss: 0.3992242217063904\n",
      "cnt: 0 - valLoss: 0.40980100631713867 - trainLoss: 0.39921844005584717\n",
      "cnt: 0 - valLoss: 0.40979769825935364 - trainLoss: 0.39921271800994873\n",
      "cnt: 0 - valLoss: 0.40979474782943726 - trainLoss: 0.3992069363594055\n",
      "cnt: 0 - valLoss: 0.4097922742366791 - trainLoss: 0.3992012143135071\n",
      "cnt: 0 - valLoss: 0.4097887873649597 - trainLoss: 0.39919546246528625\n",
      "cnt: 0 - valLoss: 0.40978631377220154 - trainLoss: 0.3991897404193878\n",
      "cnt: 0 - valLoss: 0.4097825884819031 - trainLoss: 0.3991840183734894\n",
      "cnt: 0 - valLoss: 0.4097800552845001 - trainLoss: 0.39917829632759094\n",
      "cnt: 0 - valLoss: 0.4097767770290375 - trainLoss: 0.3991725444793701\n",
      "cnt: 0 - valLoss: 0.4097743332386017 - trainLoss: 0.39916685223579407\n",
      "cnt: 0 - valLoss: 0.40977081656455994 - trainLoss: 0.39916110038757324\n",
      "cnt: 0 - valLoss: 0.409768283367157 - trainLoss: 0.3991553783416748\n",
      "cnt: 0 - valLoss: 0.40976518392562866 - trainLoss: 0.39914968609809875\n",
      "cnt: 0 - valLoss: 0.40976232290267944 - trainLoss: 0.39914393424987793\n",
      "cnt: 0 - valLoss: 0.4097592532634735 - trainLoss: 0.3991382122039795\n",
      "cnt: 0 - valLoss: 0.4097563624382019 - trainLoss: 0.39913251996040344\n",
      "cnt: 0 - valLoss: 0.40975314378738403 - trainLoss: 0.399126797914505\n",
      "cnt: 0 - valLoss: 0.4097508490085602 - trainLoss: 0.39912107586860657\n",
      "cnt: 0 - valLoss: 0.4097469449043274 - trainLoss: 0.39911535382270813\n",
      "cnt: 0 - valLoss: 0.4097445607185364 - trainLoss: 0.3991096615791321\n",
      "cnt: 0 - valLoss: 0.4097413420677185 - trainLoss: 0.39910390973091125\n",
      "cnt: 0 - valLoss: 0.40973857045173645 - trainLoss: 0.3990982174873352\n",
      "cnt: 0 - valLoss: 0.4097351133823395 - trainLoss: 0.39909252524375916\n",
      "cnt: 0 - valLoss: 0.40973255038261414 - trainLoss: 0.3990868031978607\n",
      "cnt: 0 - valLoss: 0.40972912311553955 - trainLoss: 0.39908117055892944\n",
      "cnt: 0 - valLoss: 0.40972700715065 - trainLoss: 0.3990754187107086\n",
      "cnt: 0 - valLoss: 0.4097234606742859 - trainLoss: 0.39906972646713257\n",
      "cnt: 0 - valLoss: 0.4097207486629486 - trainLoss: 0.39906400442123413\n",
      "cnt: 0 - valLoss: 0.4097175896167755 - trainLoss: 0.3990583121776581\n",
      "cnt: 0 - valLoss: 0.40971502661705017 - trainLoss: 0.39905261993408203\n",
      "cnt: 0 - valLoss: 0.4097115993499756 - trainLoss: 0.399046927690506\n",
      "cnt: 0 - valLoss: 0.4097084701061249 - trainLoss: 0.39904123544692993\n",
      "cnt: 0 - valLoss: 0.40970560908317566 - trainLoss: 0.3990355432033539\n",
      "cnt: 0 - valLoss: 0.40970227122306824 - trainLoss: 0.39902985095977783\n",
      "cnt: 0 - valLoss: 0.4096994698047638 - trainLoss: 0.3990241587162018\n",
      "cnt: 0 - valLoss: 0.4096965193748474 - trainLoss: 0.39901846647262573\n",
      "cnt: 0 - valLoss: 0.40969428420066833 - trainLoss: 0.3990127444267273\n",
      "cnt: 0 - valLoss: 0.4096915125846863 - trainLoss: 0.3990069627761841\n",
      "cnt: 0 - valLoss: 0.4096892774105072 - trainLoss: 0.3990011513233185\n",
      "cnt: 0 - valLoss: 0.40968579053878784 - trainLoss: 0.39899542927742004\n",
      "cnt: 0 - valLoss: 0.4096832275390625 - trainLoss: 0.3989897072315216\n",
      "cnt: 0 - valLoss: 0.40967997908592224 - trainLoss: 0.39898401498794556\n",
      "cnt: 0 - valLoss: 0.4096774160861969 - trainLoss: 0.3989782929420471\n",
      "cnt: 0 - valLoss: 0.4096740186214447 - trainLoss: 0.3989725708961487\n",
      "cnt: 0 - valLoss: 0.4096715748310089 - trainLoss: 0.39896684885025024\n",
      "cnt: 0 - valLoss: 0.4096682071685791 - trainLoss: 0.3989611864089966\n",
      "cnt: 0 - valLoss: 0.40966546535491943 - trainLoss: 0.39895546436309814\n",
      "cnt: 0 - valLoss: 0.409662663936615 - trainLoss: 0.3989497721195221\n",
      "cnt: 0 - valLoss: 0.40965935587882996 - trainLoss: 0.39894405007362366\n",
      "cnt: 0 - valLoss: 0.4096565842628479 - trainLoss: 0.3989383578300476\n",
      "cnt: 0 - valLoss: 0.4096532464027405 - trainLoss: 0.39893266558647156\n",
      "cnt: 0 - valLoss: 0.40965116024017334 - trainLoss: 0.3989269733428955\n",
      "cnt: 0 - valLoss: 0.40964755415916443 - trainLoss: 0.3989212214946747\n",
      "cnt: 0 - valLoss: 0.40964508056640625 - trainLoss: 0.39891552925109863\n",
      "cnt: 0 - valLoss: 0.4096417725086212 - trainLoss: 0.39890989661216736\n",
      "cnt: 0 - valLoss: 0.40963849425315857 - trainLoss: 0.3989041745662689\n",
      "cnt: 0 - valLoss: 0.40963590145111084 - trainLoss: 0.39889851212501526\n",
      "cnt: 0 - valLoss: 0.40963271260261536 - trainLoss: 0.398892879486084\n",
      "cnt: 0 - valLoss: 0.40962961316108704 - trainLoss: 0.39888718724250793\n",
      "cnt: 0 - valLoss: 0.40962666273117065 - trainLoss: 0.3988815248012543\n",
      "cnt: 0 - valLoss: 0.40962380170822144 - trainLoss: 0.3988758623600006\n",
      "cnt: 0 - valLoss: 0.40962064266204834 - trainLoss: 0.39887022972106934\n",
      "cnt: 0 - valLoss: 0.40961766242980957 - trainLoss: 0.3988645374774933\n",
      "cnt: 0 - valLoss: 0.4096144735813141 - trainLoss: 0.3988588750362396\n",
      "cnt: 0 - valLoss: 0.4096115529537201 - trainLoss: 0.39885324239730835\n",
      "cnt: 0 - valLoss: 0.40960878133773804 - trainLoss: 0.3988475799560547\n",
      "cnt: 0 - valLoss: 0.40960556268692017 - trainLoss: 0.398841917514801\n",
      "cnt: 0 - valLoss: 0.4096027612686157 - trainLoss: 0.39883628487586975\n",
      "cnt: 0 - valLoss: 0.4095993936061859 - trainLoss: 0.3988306224346161\n",
      "cnt: 0 - valLoss: 0.40959617495536804 - trainLoss: 0.3988249599933624\n",
      "cnt: 0 - valLoss: 0.4095931649208069 - trainLoss: 0.39881932735443115\n",
      "cnt: 0 - valLoss: 0.4095900058746338 - trainLoss: 0.3988136947154999\n",
      "cnt: 0 - valLoss: 0.40958696603775024 - trainLoss: 0.3988080322742462\n",
      "cnt: 0 - valLoss: 0.40958452224731445 - trainLoss: 0.39880239963531494\n",
      "cnt: 0 - valLoss: 0.4095814526081085 - trainLoss: 0.3987967073917389\n",
      "cnt: 0 - valLoss: 0.4095785319805145 - trainLoss: 0.39879110455513\n",
      "cnt: 0 - valLoss: 0.40957555174827576 - trainLoss: 0.39878544211387634\n",
      "cnt: 0 - valLoss: 0.4095722734928131 - trainLoss: 0.39877983927726746\n",
      "cnt: 0 - valLoss: 0.4095698893070221 - trainLoss: 0.3987741768360138\n",
      "cnt: 0 - valLoss: 0.4095667004585266 - trainLoss: 0.39876851439476013\n",
      "cnt: 0 - valLoss: 0.40956369042396545 - trainLoss: 0.39876288175582886\n",
      "cnt: 0 - valLoss: 0.4095606505870819 - trainLoss: 0.3987572491168976\n",
      "cnt: 0 - valLoss: 0.40955790877342224 - trainLoss: 0.3987516164779663\n",
      "cnt: 0 - valLoss: 0.409555047750473 - trainLoss: 0.3987460136413574\n",
      "cnt: 0 - valLoss: 0.40955209732055664 - trainLoss: 0.39874038100242615\n",
      "cnt: 0 - valLoss: 0.409549355506897 - trainLoss: 0.3987347185611725\n",
      "cnt: 0 - valLoss: 0.4095459580421448 - trainLoss: 0.3987291157245636\n",
      "cnt: 0 - valLoss: 0.4095432162284851 - trainLoss: 0.3987234830856323\n",
      "cnt: 0 - valLoss: 0.409540593624115 - trainLoss: 0.39871782064437866\n",
      "cnt: 0 - valLoss: 0.40953749418258667 - trainLoss: 0.3987122178077698\n",
      "cnt: 0 - valLoss: 0.4095345735549927 - trainLoss: 0.3987065851688385\n",
      "cnt: 0 - valLoss: 0.40953174233436584 - trainLoss: 0.3987009525299072\n",
      "cnt: 0 - valLoss: 0.4095286428928375 - trainLoss: 0.39869531989097595\n",
      "cnt: 0 - valLoss: 0.40952587127685547 - trainLoss: 0.39868974685668945\n",
      "cnt: 0 - valLoss: 0.4095229506492615 - trainLoss: 0.3986841142177582\n",
      "cnt: 0 - valLoss: 0.40952008962631226 - trainLoss: 0.3986784815788269\n",
      "cnt: 0 - valLoss: 0.40951648354530334 - trainLoss: 0.39867284893989563\n",
      "cnt: 0 - valLoss: 0.4095136523246765 - trainLoss: 0.39866727590560913\n",
      "cnt: 0 - valLoss: 0.4095107614994049 - trainLoss: 0.39866161346435547\n",
      "cnt: 0 - valLoss: 0.4095081388950348 - trainLoss: 0.3986560106277466\n",
      "cnt: 0 - valLoss: 0.4095054864883423 - trainLoss: 0.3986504375934601\n",
      "cnt: 0 - valLoss: 0.40950262546539307 - trainLoss: 0.3986448049545288\n",
      "cnt: 0 - valLoss: 0.4095000922679901 - trainLoss: 0.3986392021179199\n",
      "cnt: 0 - valLoss: 0.40949681401252747 - trainLoss: 0.39863356947898865\n",
      "cnt: 0 - valLoss: 0.4094942510128021 - trainLoss: 0.39862799644470215\n",
      "cnt: 0 - valLoss: 0.40949124097824097 - trainLoss: 0.3986223638057709\n",
      "cnt: 0 - valLoss: 0.4094885289669037 - trainLoss: 0.398616760969162\n",
      "cnt: 0 - valLoss: 0.40948569774627686 - trainLoss: 0.3986111879348755\n",
      "cnt: 0 - valLoss: 0.40948286652565 - trainLoss: 0.3986055850982666\n",
      "cnt: 0 - valLoss: 0.4094798266887665 - trainLoss: 0.3985999524593353\n",
      "cnt: 0 - valLoss: 0.40947747230529785 - trainLoss: 0.39859437942504883\n",
      "cnt: 0 - valLoss: 0.40947455167770386 - trainLoss: 0.39858880639076233\n",
      "cnt: 0 - valLoss: 0.4094720184803009 - trainLoss: 0.39858320355415344\n",
      "cnt: 0 - valLoss: 0.40946856141090393 - trainLoss: 0.39857757091522217\n",
      "cnt: 0 - valLoss: 0.40946587920188904 - trainLoss: 0.39857199788093567\n",
      "cnt: 0 - valLoss: 0.4094632863998413 - trainLoss: 0.39856642484664917\n",
      "cnt: 0 - valLoss: 0.40946075320243835 - trainLoss: 0.39856085181236267\n",
      "cnt: 0 - valLoss: 0.40945836901664734 - trainLoss: 0.3985552191734314\n",
      "cnt: 0 - valLoss: 0.40945616364479065 - trainLoss: 0.3985495865345001\n",
      "cnt: 0 - valLoss: 0.409453809261322 - trainLoss: 0.39854395389556885\n",
      "cnt: 0 - valLoss: 0.409451425075531 - trainLoss: 0.3985382914543152\n",
      "cnt: 0 - valLoss: 0.40944892168045044 - trainLoss: 0.3985326588153839\n",
      "cnt: 0 - valLoss: 0.40944692492485046 - trainLoss: 0.39852702617645264\n",
      "cnt: 0 - valLoss: 0.4094441831111908 - trainLoss: 0.398521363735199\n",
      "cnt: 0 - valLoss: 0.40944209694862366 - trainLoss: 0.3985157608985901\n",
      "cnt: 0 - valLoss: 0.40943989157676697 - trainLoss: 0.3985100984573364\n",
      "cnt: 0 - valLoss: 0.4094371497631073 - trainLoss: 0.39850449562072754\n",
      "cnt: 0 - valLoss: 0.40943458676338196 - trainLoss: 0.39849886298179626\n",
      "cnt: 0 - valLoss: 0.4094308912754059 - trainLoss: 0.39849328994750977\n",
      "cnt: 0 - valLoss: 0.4094284474849701 - trainLoss: 0.39848771691322327\n",
      "cnt: 0 - valLoss: 0.4094254970550537 - trainLoss: 0.3984821140766144\n",
      "cnt: 0 - valLoss: 0.40942278504371643 - trainLoss: 0.3984765410423279\n",
      "cnt: 0 - valLoss: 0.4094201922416687 - trainLoss: 0.398470938205719\n",
      "cnt: 0 - valLoss: 0.40941759943962097 - trainLoss: 0.3984653949737549\n",
      "cnt: 0 - valLoss: 0.40941452980041504 - trainLoss: 0.398459792137146\n",
      "cnt: 0 - valLoss: 0.4094116985797882 - trainLoss: 0.3984542787075043\n",
      "cnt: 0 - valLoss: 0.40940919518470764 - trainLoss: 0.3984487056732178\n",
      "cnt: 0 - valLoss: 0.4094065725803375 - trainLoss: 0.3984431326389313\n",
      "cnt: 0 - valLoss: 0.4094037413597107 - trainLoss: 0.3984375596046448\n",
      "cnt: 0 - valLoss: 0.40940114855766296 - trainLoss: 0.3984319865703583\n",
      "cnt: 0 - valLoss: 0.4093983471393585 - trainLoss: 0.3984263837337494\n",
      "cnt: 0 - valLoss: 0.4093950688838959 - trainLoss: 0.3984208405017853\n",
      "cnt: 0 - valLoss: 0.4093923568725586 - trainLoss: 0.3984152674674988\n",
      "cnt: 0 - valLoss: 0.40938976407051086 - trainLoss: 0.39840972423553467\n",
      "cnt: 0 - valLoss: 0.4093869626522064 - trainLoss: 0.39840415120124817\n",
      "cnt: 0 - valLoss: 0.4093841016292572 - trainLoss: 0.39839860796928406\n",
      "cnt: 0 - valLoss: 0.40938177704811096 - trainLoss: 0.39839303493499756\n",
      "cnt: 0 - valLoss: 0.40937909483909607 - trainLoss: 0.39838746190071106\n",
      "cnt: 0 - valLoss: 0.40937644243240356 - trainLoss: 0.39838194847106934\n",
      "cnt: 0 - valLoss: 0.40937381982803345 - trainLoss: 0.39837637543678284\n",
      "cnt: 0 - valLoss: 0.4093713164329529 - trainLoss: 0.39837080240249634\n",
      "cnt: 0 - valLoss: 0.40936753153800964 - trainLoss: 0.3983652591705322\n",
      "cnt: 0 - valLoss: 0.4093652069568634 - trainLoss: 0.3983597457408905\n",
      "cnt: 0 - valLoss: 0.4093627333641052 - trainLoss: 0.398354172706604\n",
      "cnt: 0 - valLoss: 0.4093599021434784 - trainLoss: 0.3983485996723175\n",
      "cnt: 0 - valLoss: 0.409357488155365 - trainLoss: 0.3983430862426758\n",
      "cnt: 0 - valLoss: 0.4093547463417053 - trainLoss: 0.39833754301071167\n",
      "cnt: 0 - valLoss: 0.4093523919582367 - trainLoss: 0.39833196997642517\n",
      "cnt: 0 - valLoss: 0.40934914350509644 - trainLoss: 0.39832642674446106\n",
      "cnt: 0 - valLoss: 0.4093467891216278 - trainLoss: 0.39832091331481934\n",
      "cnt: 0 - valLoss: 0.4093444347381592 - trainLoss: 0.3983153700828552\n",
      "cnt: 0 - valLoss: 0.4093409776687622 - trainLoss: 0.3983097970485687\n",
      "cnt: 0 - valLoss: 0.40933871269226074 - trainLoss: 0.398304283618927\n",
      "cnt: 0 - valLoss: 0.409335732460022 - trainLoss: 0.3982987403869629\n",
      "cnt: 0 - valLoss: 0.40933334827423096 - trainLoss: 0.39829322695732117\n",
      "cnt: 0 - valLoss: 0.4093305766582489 - trainLoss: 0.39828768372535706\n",
      "cnt: 0 - valLoss: 0.4093283712863922 - trainLoss: 0.39828214049339294\n",
      "cnt: 0 - valLoss: 0.4093256890773773 - trainLoss: 0.39827659726142883\n",
      "cnt: 0 - valLoss: 0.40932318568229675 - trainLoss: 0.3982710540294647\n",
      "cnt: 0 - valLoss: 0.40932080149650574 - trainLoss: 0.3982655704021454\n",
      "cnt: 0 - valLoss: 0.40931740403175354 - trainLoss: 0.3982600271701813\n",
      "cnt: 0 - valLoss: 0.40931519865989685 - trainLoss: 0.39825448393821716\n",
      "cnt: 0 - valLoss: 0.40931233763694763 - trainLoss: 0.39824897050857544\n",
      "cnt: 0 - valLoss: 0.40930983424186707 - trainLoss: 0.39824342727661133\n",
      "cnt: 0 - valLoss: 0.40930765867233276 - trainLoss: 0.3982379138469696\n",
      "cnt: 0 - valLoss: 0.409304141998291 - trainLoss: 0.3982324004173279\n",
      "cnt: 0 - valLoss: 0.40930119156837463 - trainLoss: 0.39822694659233093\n",
      "cnt: 0 - valLoss: 0.40929800271987915 - trainLoss: 0.39822152256965637\n",
      "cnt: 0 - valLoss: 0.40929415822029114 - trainLoss: 0.3982160985469818\n",
      "cnt: 0 - valLoss: 0.4092909097671509 - trainLoss: 0.3982106149196625\n",
      "cnt: 0 - valLoss: 0.409287691116333 - trainLoss: 0.3982051610946655\n",
      "cnt: 0 - valLoss: 0.4092848300933838 - trainLoss: 0.3981997072696686\n",
      "cnt: 0 - valLoss: 0.4092814028263092 - trainLoss: 0.39819425344467163\n",
      "cnt: 0 - valLoss: 0.4092783033847809 - trainLoss: 0.39818882942199707\n",
      "cnt: 0 - valLoss: 0.4092755615711212 - trainLoss: 0.3981834352016449\n",
      "cnt: 0 - valLoss: 0.40927237272262573 - trainLoss: 0.3981779217720032\n",
      "cnt: 0 - valLoss: 0.4092693626880646 - trainLoss: 0.398172527551651\n",
      "cnt: 0 - valLoss: 0.40926602482795715 - trainLoss: 0.39816710352897644\n",
      "cnt: 0 - valLoss: 0.40926334261894226 - trainLoss: 0.3981616795063019\n",
      "cnt: 0 - valLoss: 0.40926021337509155 - trainLoss: 0.39815622568130493\n",
      "cnt: 0 - valLoss: 0.4092574417591095 - trainLoss: 0.39815086126327515\n",
      "cnt: 0 - valLoss: 0.409254789352417 - trainLoss: 0.3981454372406006\n",
      "cnt: 0 - valLoss: 0.4092511236667633 - trainLoss: 0.3981400430202484\n",
      "cnt: 0 - valLoss: 0.4092484414577484 - trainLoss: 0.39813461899757385\n",
      "cnt: 0 - valLoss: 0.4092452824115753 - trainLoss: 0.3981292247772217\n",
      "cnt: 0 - valLoss: 0.40924254059791565 - trainLoss: 0.3981238305568695\n",
      "cnt: 0 - valLoss: 0.4092400372028351 - trainLoss: 0.39811843633651733\n",
      "cnt: 0 - valLoss: 0.4092370271682739 - trainLoss: 0.3981130123138428\n",
      "cnt: 0 - valLoss: 0.40923377871513367 - trainLoss: 0.3981076180934906\n",
      "cnt: 0 - valLoss: 0.40923064947128296 - trainLoss: 0.3981022536754608\n",
      "cnt: 0 - valLoss: 0.40922775864601135 - trainLoss: 0.39809685945510864\n",
      "cnt: 0 - valLoss: 0.40922483801841736 - trainLoss: 0.39809146523475647\n",
      "cnt: 0 - valLoss: 0.409221887588501 - trainLoss: 0.3980860710144043\n",
      "cnt: 0 - valLoss: 0.409219354391098 - trainLoss: 0.3980806767940521\n",
      "cnt: 0 - valLoss: 0.40921667218208313 - trainLoss: 0.39807528257369995\n",
      "cnt: 0 - valLoss: 0.4092130661010742 - trainLoss: 0.39806991815567017\n",
      "cnt: 0 - valLoss: 0.40921032428741455 - trainLoss: 0.3980645537376404\n",
      "cnt: 0 - valLoss: 0.40920770168304443 - trainLoss: 0.3980591297149658\n",
      "cnt: 0 - valLoss: 0.40920454263687134 - trainLoss: 0.3980537950992584\n",
      "cnt: 0 - valLoss: 0.4092021584510803 - trainLoss: 0.39804840087890625\n",
      "cnt: 0 - valLoss: 0.40919846296310425 - trainLoss: 0.3980430066585541\n",
      "cnt: 0 - valLoss: 0.4091956913471222 - trainLoss: 0.3980376124382019\n",
      "cnt: 0 - valLoss: 0.4091934561729431 - trainLoss: 0.3980322480201721\n",
      "cnt: 0 - valLoss: 0.40919047594070435 - trainLoss: 0.39802688360214233\n",
      "cnt: 0 - valLoss: 0.4091879427433014 - trainLoss: 0.39802151918411255\n",
      "cnt: 0 - valLoss: 0.40918517112731934 - trainLoss: 0.39801615476608276\n",
      "cnt: 0 - valLoss: 0.4091821014881134 - trainLoss: 0.3980107307434082\n",
      "cnt: 0 - valLoss: 0.4091792404651642 - trainLoss: 0.3980054259300232\n",
      "cnt: 0 - valLoss: 0.4091765582561493 - trainLoss: 0.398000031709671\n",
      "cnt: 0 - valLoss: 0.4091736674308777 - trainLoss: 0.39799466729164124\n",
      "cnt: 0 - valLoss: 0.4091711640357971 - trainLoss: 0.39798930287361145\n",
      "cnt: 0 - valLoss: 0.40916797518730164 - trainLoss: 0.3979839086532593\n",
      "cnt: 0 - valLoss: 0.4091654419898987 - trainLoss: 0.3979785740375519\n",
      "cnt: 0 - valLoss: 0.4091624319553375 - trainLoss: 0.3979732096195221\n",
      "cnt: 0 - valLoss: 0.40915969014167786 - trainLoss: 0.3979678153991699\n",
      "cnt: 0 - valLoss: 0.40915751457214355 - trainLoss: 0.3979625105857849\n",
      "cnt: 0 - valLoss: 0.4091542661190033 - trainLoss: 0.39795711636543274\n",
      "cnt: 0 - valLoss: 0.409151166677475 - trainLoss: 0.39795175194740295\n",
      "cnt: 0 - valLoss: 0.4091486632823944 - trainLoss: 0.39794641733169556\n",
      "cnt: 0 - valLoss: 0.40914633870124817 - trainLoss: 0.39794105291366577\n",
      "cnt: 0 - valLoss: 0.40914386510849 - trainLoss: 0.397935688495636\n",
      "cnt: 0 - valLoss: 0.40914008021354675 - trainLoss: 0.3979303538799286\n",
      "cnt: 0 - valLoss: 0.4091377258300781 - trainLoss: 0.3979249596595764\n",
      "cnt: 0 - valLoss: 0.4091346561908722 - trainLoss: 0.397919625043869\n",
      "cnt: 0 - valLoss: 0.40913254022598267 - trainLoss: 0.3979142904281616\n",
      "cnt: 0 - valLoss: 0.4091297686100006 - trainLoss: 0.39790892601013184\n",
      "cnt: 0 - valLoss: 0.40912681818008423 - trainLoss: 0.39790359139442444\n",
      "cnt: 0 - valLoss: 0.4091237187385559 - trainLoss: 0.39789822697639465\n",
      "cnt: 0 - valLoss: 0.40912163257598877 - trainLoss: 0.39789289236068726\n",
      "cnt: 0 - valLoss: 0.40911877155303955 - trainLoss: 0.39788755774497986\n",
      "cnt: 0 - valLoss: 0.4091162383556366 - trainLoss: 0.39788222312927246\n",
      "cnt: 0 - valLoss: 0.40911293029785156 - trainLoss: 0.3978768289089203\n",
      "cnt: 0 - valLoss: 0.4091106355190277 - trainLoss: 0.3978714942932129\n",
      "cnt: 0 - valLoss: 0.4091080129146576 - trainLoss: 0.3978661894798279\n",
      "cnt: 0 - valLoss: 0.4091053903102875 - trainLoss: 0.3978608548641205\n",
      "cnt: 0 - valLoss: 0.4091024100780487 - trainLoss: 0.3978555202484131\n",
      "cnt: 0 - valLoss: 0.40909937024116516 - trainLoss: 0.3978501856327057\n",
      "cnt: 0 - valLoss: 0.40909692645072937 - trainLoss: 0.3978448510169983\n",
      "cnt: 0 - valLoss: 0.4090944826602936 - trainLoss: 0.3978395164012909\n",
      "cnt: 0 - valLoss: 0.40909165143966675 - trainLoss: 0.3978341519832611\n",
      "cnt: 0 - valLoss: 0.4090885818004608 - trainLoss: 0.3978288471698761\n",
      "cnt: 0 - valLoss: 0.4090863764286041 - trainLoss: 0.3978235423564911\n",
      "cnt: 0 - valLoss: 0.40908339619636536 - trainLoss: 0.3978182077407837\n",
      "cnt: 0 - valLoss: 0.40908104181289673 - trainLoss: 0.3978128731250763\n",
      "cnt: 0 - valLoss: 0.40907809138298035 - trainLoss: 0.3978075683116913\n",
      "cnt: 0 - valLoss: 0.4090753197669983 - trainLoss: 0.3978022634983063\n",
      "cnt: 0 - valLoss: 0.4090728163719177 - trainLoss: 0.3977968990802765\n",
      "cnt: 0 - valLoss: 0.40907010436058044 - trainLoss: 0.39779162406921387\n",
      "cnt: 0 - valLoss: 0.40906745195388794 - trainLoss: 0.3977862596511841\n",
      "cnt: 0 - valLoss: 0.4090644121170044 - trainLoss: 0.39778098464012146\n",
      "cnt: 0 - valLoss: 0.4090620279312134 - trainLoss: 0.39777565002441406\n",
      "cnt: 0 - valLoss: 0.40905967354774475 - trainLoss: 0.39777031540870667\n",
      "cnt: 0 - valLoss: 0.409056693315506 - trainLoss: 0.39776501059532166\n",
      "cnt: 0 - valLoss: 0.4090538024902344 - trainLoss: 0.39775967597961426\n",
      "cnt: 0 - valLoss: 0.40905115008354187 - trainLoss: 0.39775437116622925\n",
      "cnt: 0 - valLoss: 0.40904876589775085 - trainLoss: 0.39774903655052185\n",
      "cnt: 0 - valLoss: 0.4090461730957031 - trainLoss: 0.39774376153945923\n",
      "cnt: 0 - valLoss: 0.40904322266578674 - trainLoss: 0.3977384567260742\n",
      "cnt: 0 - valLoss: 0.40904027223587036 - trainLoss: 0.3977331519126892\n",
      "cnt: 0 - valLoss: 0.4090381860733032 - trainLoss: 0.3977278769016266\n",
      "cnt: 0 - valLoss: 0.40903571248054504 - trainLoss: 0.3977225422859192\n",
      "cnt: 0 - valLoss: 0.40903303027153015 - trainLoss: 0.39771726727485657\n",
      "cnt: 0 - valLoss: 0.4090297818183899 - trainLoss: 0.39771193265914917\n",
      "cnt: 0 - valLoss: 0.40902766585350037 - trainLoss: 0.39770665764808655\n",
      "cnt: 0 - valLoss: 0.4090251624584198 - trainLoss: 0.39770135283470154\n",
      "cnt: 0 - valLoss: 0.4090225398540497 - trainLoss: 0.39769601821899414\n",
      "cnt: 0 - valLoss: 0.40901869535446167 - trainLoss: 0.39769071340560913\n",
      "cnt: 0 - valLoss: 0.409015953540802 - trainLoss: 0.3976854681968689\n",
      "cnt: 0 - valLoss: 0.40901297330856323 - trainLoss: 0.3976801335811615\n",
      "cnt: 0 - valLoss: 0.4090110957622528 - trainLoss: 0.3976748585700989\n",
      "cnt: 0 - valLoss: 0.4090077877044678 - trainLoss: 0.39766958355903625\n",
      "cnt: 0 - valLoss: 0.40900471806526184 - trainLoss: 0.39766427874565125\n",
      "cnt: 0 - valLoss: 0.4090023934841156 - trainLoss: 0.3976590037345886\n",
      "cnt: 0 - valLoss: 0.40899884700775146 - trainLoss: 0.3976536989212036\n",
      "cnt: 0 - valLoss: 0.4089960753917694 - trainLoss: 0.397648423910141\n",
      "cnt: 0 - valLoss: 0.40899357199668884 - trainLoss: 0.39764314889907837\n",
      "cnt: 0 - valLoss: 0.4089903235435486 - trainLoss: 0.39763784408569336\n",
      "cnt: 0 - valLoss: 0.40898779034614563 - trainLoss: 0.3976325988769531\n",
      "cnt: 0 - valLoss: 0.4089852571487427 - trainLoss: 0.3976273238658905\n",
      "cnt: 0 - valLoss: 0.40898096561431885 - trainLoss: 0.3976220190525055\n",
      "cnt: 0 - valLoss: 0.40897902846336365 - trainLoss: 0.39761674404144287\n",
      "cnt: 0 - valLoss: 0.40897607803344727 - trainLoss: 0.39761146903038025\n",
      "cnt: 0 - valLoss: 0.40897393226623535 - trainLoss: 0.39760616421699524\n",
      "cnt: 0 - valLoss: 0.40897083282470703 - trainLoss: 0.397600919008255\n",
      "cnt: 0 - valLoss: 0.40896743535995483 - trainLoss: 0.3975956439971924\n",
      "cnt: 0 - valLoss: 0.4089646339416504 - trainLoss: 0.39759036898612976\n",
      "cnt: 0 - valLoss: 0.40896207094192505 - trainLoss: 0.3975851237773895\n",
      "cnt: 0 - valLoss: 0.40895912051200867 - trainLoss: 0.3975798189640045\n",
      "cnt: 0 - valLoss: 0.40895622968673706 - trainLoss: 0.3975745737552643\n",
      "cnt: 0 - valLoss: 0.40895330905914307 - trainLoss: 0.39756929874420166\n",
      "cnt: 0 - valLoss: 0.40895089507102966 - trainLoss: 0.3975640535354614\n",
      "cnt: 0 - valLoss: 0.40894782543182373 - trainLoss: 0.3975587785243988\n",
      "cnt: 0 - valLoss: 0.408945232629776 - trainLoss: 0.3975535035133362\n",
      "cnt: 0 - valLoss: 0.40894144773483276 - trainLoss: 0.39754825830459595\n",
      "cnt: 0 - valLoss: 0.40893957018852234 - trainLoss: 0.39754295349121094\n",
      "cnt: 0 - valLoss: 0.408936470746994 - trainLoss: 0.3975377082824707\n",
      "cnt: 0 - valLoss: 0.4089345335960388 - trainLoss: 0.39753246307373047\n",
      "cnt: 0 - valLoss: 0.40893086791038513 - trainLoss: 0.39752721786499023\n",
      "cnt: 0 - valLoss: 0.40892913937568665 - trainLoss: 0.39752197265625\n",
      "cnt: 0 - valLoss: 0.40892481803894043 - trainLoss: 0.3975166976451874\n",
      "cnt: 0 - valLoss: 0.4089231491088867 - trainLoss: 0.39751145243644714\n",
      "cnt: 0 - valLoss: 0.40891993045806885 - trainLoss: 0.3975062072277069\n",
      "cnt: 0 - valLoss: 0.40891727805137634 - trainLoss: 0.3975009322166443\n",
      "cnt: 0 - valLoss: 0.4089139997959137 - trainLoss: 0.39749571681022644\n",
      "cnt: 0 - valLoss: 0.4089120626449585 - trainLoss: 0.3974904716014862\n",
      "cnt: 0 - valLoss: 0.4089089334011078 - trainLoss: 0.39748522639274597\n",
      "cnt: 0 - valLoss: 0.408906489610672 - trainLoss: 0.39747998118400574\n",
      "cnt: 0 - valLoss: 0.40890365839004517 - trainLoss: 0.3974747359752655\n",
      "cnt: 0 - valLoss: 0.40890055894851685 - trainLoss: 0.39746949076652527\n",
      "cnt: 0 - valLoss: 0.4088977575302124 - trainLoss: 0.39746424555778503\n",
      "cnt: 0 - valLoss: 0.40889567136764526 - trainLoss: 0.3974589705467224\n",
      "cnt: 0 - valLoss: 0.4088928997516632 - trainLoss: 0.3974537253379822\n",
      "cnt: 0 - valLoss: 0.40889009833335876 - trainLoss: 0.39744850993156433\n",
      "cnt: 0 - valLoss: 0.40888723731040955 - trainLoss: 0.3974432647228241\n",
      "cnt: 0 - valLoss: 0.40888461470603943 - trainLoss: 0.39743801951408386\n",
      "cnt: 0 - valLoss: 0.40888118743896484 - trainLoss: 0.397432804107666\n",
      "cnt: 0 - valLoss: 0.40887951850891113 - trainLoss: 0.39742758870124817\n",
      "cnt: 0 - valLoss: 0.4088766872882843 - trainLoss: 0.39742231369018555\n",
      "cnt: 0 - valLoss: 0.4088730812072754 - trainLoss: 0.3974170982837677\n",
      "cnt: 0 - valLoss: 0.4088705778121948 - trainLoss: 0.39741188287734985\n",
      "cnt: 0 - valLoss: 0.40886884927749634 - trainLoss: 0.3974066376686096\n",
      "cnt: 0 - valLoss: 0.40886572003364563 - trainLoss: 0.3974014222621918\n",
      "cnt: 0 - valLoss: 0.4088628590106964 - trainLoss: 0.3973962366580963\n",
      "cnt: 0 - valLoss: 0.4088602066040039 - trainLoss: 0.3973909914493561\n",
      "cnt: 0 - valLoss: 0.40885794162750244 - trainLoss: 0.39738577604293823\n",
      "cnt: 0 - valLoss: 0.40885528922080994 - trainLoss: 0.397380530834198\n",
      "cnt: 0 - valLoss: 0.4088519811630249 - trainLoss: 0.39737531542778015\n",
      "cnt: 0 - valLoss: 0.4088492691516876 - trainLoss: 0.3973701298236847\n",
      "cnt: 0 - valLoss: 0.40884748101234436 - trainLoss: 0.39736491441726685\n",
      "cnt: 0 - valLoss: 0.40884464979171753 - trainLoss: 0.3973596692085266\n",
      "cnt: 0 - valLoss: 0.4088415503501892 - trainLoss: 0.39735445380210876\n",
      "cnt: 0 - valLoss: 0.4088388979434967 - trainLoss: 0.3973492681980133\n",
      "cnt: 0 - valLoss: 0.4088367223739624 - trainLoss: 0.39734405279159546\n",
      "cnt: 0 - valLoss: 0.4088340103626251 - trainLoss: 0.3973388075828552\n",
      "cnt: 0 - valLoss: 0.4088311791419983 - trainLoss: 0.39733362197875977\n",
      "cnt: 0 - valLoss: 0.4088284373283386 - trainLoss: 0.3973284363746643\n",
      "cnt: 0 - valLoss: 0.4088261127471924 - trainLoss: 0.3973231911659241\n",
      "cnt: 0 - valLoss: 0.4088229537010193 - trainLoss: 0.397318035364151\n",
      "cnt: 0 - valLoss: 0.4088212549686432 - trainLoss: 0.39731279015541077\n",
      "cnt: 0 - valLoss: 0.4088173508644104 - trainLoss: 0.3973075747489929\n",
      "cnt: 0 - valLoss: 0.4088154733181 - trainLoss: 0.39730238914489746\n",
      "cnt: 0 - valLoss: 0.408812940120697 - trainLoss: 0.3972971439361572\n",
      "cnt: 0 - valLoss: 0.40881049633026123 - trainLoss: 0.39729195833206177\n",
      "cnt: 0 - valLoss: 0.40880686044692993 - trainLoss: 0.3972867429256439\n",
      "cnt: 0 - valLoss: 0.408805251121521 - trainLoss: 0.39728158712387085\n",
      "cnt: 0 - valLoss: 0.408802330493927 - trainLoss: 0.3972763419151306\n",
      "cnt: 0 - valLoss: 0.4087998867034912 - trainLoss: 0.39727115631103516\n",
      "cnt: 0 - valLoss: 0.40879717469215393 - trainLoss: 0.3972659707069397\n",
      "cnt: 0 - valLoss: 0.408794641494751 - trainLoss: 0.39726075530052185\n",
      "cnt: 0 - valLoss: 0.40879106521606445 - trainLoss: 0.39725562930107117\n",
      "cnt: 0 - valLoss: 0.408789724111557 - trainLoss: 0.39725038409233093\n",
      "cnt: 0 - valLoss: 0.40878698229789734 - trainLoss: 0.3972451984882355\n",
      "cnt: 0 - valLoss: 0.4087839722633362 - trainLoss: 0.3972399830818176\n",
      "cnt: 0 - valLoss: 0.4087812006473541 - trainLoss: 0.39723479747772217\n",
      "cnt: 0 - valLoss: 0.4087788164615631 - trainLoss: 0.39722955226898193\n",
      "cnt: 0 - valLoss: 0.40877676010131836 - trainLoss: 0.39722442626953125\n",
      "cnt: 0 - valLoss: 0.40877363085746765 - trainLoss: 0.397219181060791\n",
      "cnt: 0 - valLoss: 0.40877091884613037 - trainLoss: 0.39721402525901794\n",
      "cnt: 0 - valLoss: 0.4087682068347931 - trainLoss: 0.3972088098526001\n",
      "cnt: 0 - valLoss: 0.40876632928848267 - trainLoss: 0.39720359444618225\n",
      "cnt: 0 - valLoss: 0.40876278281211853 - trainLoss: 0.3971984088420868\n",
      "cnt: 0 - valLoss: 0.40876123309135437 - trainLoss: 0.39719319343566895\n",
      "cnt: 0 - valLoss: 0.40875810384750366 - trainLoss: 0.3971880078315735\n",
      "cnt: 0 - valLoss: 0.4087557792663574 - trainLoss: 0.39718279242515564\n",
      "cnt: 0 - valLoss: 0.40875348448753357 - trainLoss: 0.39717763662338257\n",
      "cnt: 0 - valLoss: 0.40875044465065 - trainLoss: 0.3971724510192871\n",
      "cnt: 0 - valLoss: 0.4087477922439575 - trainLoss: 0.39716726541519165\n",
      "cnt: 0 - valLoss: 0.40874624252319336 - trainLoss: 0.3971620500087738\n",
      "cnt: 0 - valLoss: 0.40874311327934265 - trainLoss: 0.39715683460235596\n",
      "cnt: 0 - valLoss: 0.4087409973144531 - trainLoss: 0.3971516489982605\n",
      "cnt: 0 - valLoss: 0.40873730182647705 - trainLoss: 0.39714646339416504\n",
      "cnt: 0 - valLoss: 0.40873512625694275 - trainLoss: 0.3971412777900696\n",
      "cnt: 0 - valLoss: 0.4087333679199219 - trainLoss: 0.3971360921859741\n",
      "cnt: 0 - valLoss: 0.408729612827301 - trainLoss: 0.39713090658187866\n",
      "cnt: 0 - valLoss: 0.40872809290885925 - trainLoss: 0.3971257507801056\n",
      "cnt: 0 - valLoss: 0.4087255299091339 - trainLoss: 0.39712053537368774\n",
      "cnt: 0 - valLoss: 0.4087226688861847 - trainLoss: 0.3971153795719147\n",
      "cnt: 0 - valLoss: 0.4087201654911041 - trainLoss: 0.3971101939678192\n",
      "cnt: 0 - valLoss: 0.4087178111076355 - trainLoss: 0.39710503816604614\n",
      "cnt: 0 - valLoss: 0.40871483087539673 - trainLoss: 0.3970998525619507\n",
      "cnt: 0 - valLoss: 0.40871337056159973 - trainLoss: 0.3970946669578552\n",
      "cnt: 0 - valLoss: 0.40870925784111023 - trainLoss: 0.39708951115608215\n",
      "cnt: 0 - valLoss: 0.40870779752731323 - trainLoss: 0.3970843553543091\n",
      "cnt: 0 - valLoss: 0.4087061285972595 - trainLoss: 0.3970791697502136\n",
      "cnt: 0 - valLoss: 0.40870317816734314 - trainLoss: 0.39707380533218384\n",
      "cnt: 0 - valLoss: 0.40870118141174316 - trainLoss: 0.39706850051879883\n",
      "cnt: 0 - valLoss: 0.4086988866329193 - trainLoss: 0.3970631957054138\n",
      "cnt: 0 - valLoss: 0.4086962044239044 - trainLoss: 0.3970578610897064\n",
      "cnt: 0 - valLoss: 0.4086940884590149 - trainLoss: 0.3970525562763214\n",
      "cnt: 0 - valLoss: 0.4086916744709015 - trainLoss: 0.3970472514629364\n",
      "cnt: 0 - valLoss: 0.40868905186653137 - trainLoss: 0.3970419764518738\n",
      "cnt: 0 - valLoss: 0.4086870551109314 - trainLoss: 0.3970366418361664\n",
      "cnt: 0 - valLoss: 0.40868476033210754 - trainLoss: 0.397031307220459\n",
      "cnt: 0 - valLoss: 0.40868282318115234 - trainLoss: 0.397026002407074\n",
      "cnt: 0 - valLoss: 0.4086795747280121 - trainLoss: 0.39702075719833374\n",
      "cnt: 0 - valLoss: 0.40867704153060913 - trainLoss: 0.39701545238494873\n",
      "cnt: 0 - valLoss: 0.40867576003074646 - trainLoss: 0.3970101773738861\n",
      "cnt: 0 - valLoss: 0.40867260098457336 - trainLoss: 0.3970048725605011\n",
      "cnt: 0 - valLoss: 0.40867114067077637 - trainLoss: 0.3969995677471161\n",
      "cnt: 0 - valLoss: 0.40866827964782715 - trainLoss: 0.3969942629337311\n",
      "cnt: 0 - valLoss: 0.4086662530899048 - trainLoss: 0.39698895812034607\n",
      "cnt: 0 - valLoss: 0.4086637496948242 - trainLoss: 0.39698368310928345\n",
      "cnt: 0 - valLoss: 0.40866169333457947 - trainLoss: 0.39697837829589844\n",
      "cnt: 0 - valLoss: 0.4086590111255646 - trainLoss: 0.3969731032848358\n",
      "cnt: 0 - valLoss: 0.4086569547653198 - trainLoss: 0.3969677984714508\n",
      "cnt: 0 - valLoss: 0.4086545407772064 - trainLoss: 0.3969624936580658\n",
      "cnt: 0 - valLoss: 0.40865248441696167 - trainLoss: 0.3969571888446808\n",
      "cnt: 0 - valLoss: 0.40865057706832886 - trainLoss: 0.39695191383361816\n",
      "cnt: 0 - valLoss: 0.4086478054523468 - trainLoss: 0.39694663882255554\n",
      "cnt: 0 - valLoss: 0.4086456000804901 - trainLoss: 0.3969413638114929\n",
      "cnt: 0 - valLoss: 0.40864333510398865 - trainLoss: 0.3969360888004303\n",
      "cnt: 0 - valLoss: 0.4086412489414215 - trainLoss: 0.3969307541847229\n",
      "cnt: 0 - valLoss: 0.40863901376724243 - trainLoss: 0.3969254791736603\n",
      "cnt: 0 - valLoss: 0.40863627195358276 - trainLoss: 0.39692023396492004\n",
      "cnt: 0 - valLoss: 0.40863484144210815 - trainLoss: 0.39691492915153503\n",
      "cnt: 0 - valLoss: 0.4086321294307709 - trainLoss: 0.3969096541404724\n",
      "cnt: 0 - valLoss: 0.40863072872161865 - trainLoss: 0.3969043791294098\n",
      "cnt: 0 - valLoss: 0.408627450466156 - trainLoss: 0.39689910411834717\n",
      "cnt: 0 - valLoss: 0.4086252748966217 - trainLoss: 0.39689382910728455\n",
      "cnt: 0 - valLoss: 0.40862342715263367 - trainLoss: 0.39688852429389954\n",
      "cnt: 0 - valLoss: 0.4086206555366516 - trainLoss: 0.3968832492828369\n",
      "cnt: 0 - valLoss: 0.408619225025177 - trainLoss: 0.39687803387641907\n",
      "cnt: 0 - valLoss: 0.4086168110370636 - trainLoss: 0.39687272906303406\n",
      "cnt: 0 - valLoss: 0.4086152911186218 - trainLoss: 0.39686742424964905\n",
      "cnt: 0 - valLoss: 0.40861213207244873 - trainLoss: 0.3968622386455536\n",
      "cnt: 0 - valLoss: 0.40860962867736816 - trainLoss: 0.3968569338321686\n",
      "cnt: 0 - valLoss: 0.4086082875728607 - trainLoss: 0.39685168862342834\n",
      "cnt: 0 - valLoss: 0.40860551595687866 - trainLoss: 0.3968464136123657\n",
      "cnt: 0 - valLoss: 0.4086039364337921 - trainLoss: 0.3968411386013031\n",
      "cnt: 0 - valLoss: 0.4086012542247772 - trainLoss: 0.39683589339256287\n",
      "cnt: 0 - valLoss: 0.4085986018180847 - trainLoss: 0.39683061838150024\n",
      "cnt: 0 - valLoss: 0.4085974097251892 - trainLoss: 0.39682537317276\n",
      "cnt: 0 - valLoss: 0.4085940718650818 - trainLoss: 0.396820068359375\n",
      "cnt: 0 - valLoss: 0.40859293937683105 - trainLoss: 0.39681482315063477\n",
      "cnt: 0 - valLoss: 0.4085901379585266 - trainLoss: 0.39680954813957214\n",
      "cnt: 0 - valLoss: 0.4085884690284729 - trainLoss: 0.3968043327331543\n",
      "cnt: 0 - valLoss: 0.4085860848426819 - trainLoss: 0.39679908752441406\n",
      "cnt: 0 - valLoss: 0.40858373045921326 - trainLoss: 0.39679378271102905\n",
      "cnt: 0 - valLoss: 0.4085817039012909 - trainLoss: 0.3967885673046112\n",
      "cnt: 0 - valLoss: 0.4085790514945984 - trainLoss: 0.3967832922935486\n",
      "cnt: 0 - valLoss: 0.40857774019241333 - trainLoss: 0.39677804708480835\n",
      "cnt: 0 - valLoss: 0.40857499837875366 - trainLoss: 0.3967728316783905\n",
      "cnt: 0 - valLoss: 0.408573180437088 - trainLoss: 0.3967675566673279\n",
      "cnt: 0 - valLoss: 0.40857067704200745 - trainLoss: 0.39676231145858765\n",
      "cnt: 0 - valLoss: 0.4085688889026642 - trainLoss: 0.3967570662498474\n",
      "cnt: 0 - valLoss: 0.4085666537284851 - trainLoss: 0.39675185084342957\n",
      "cnt: 0 - valLoss: 0.40856388211250305 - trainLoss: 0.39674660563468933\n",
      "cnt: 0 - valLoss: 0.408562570810318 - trainLoss: 0.3967413902282715\n",
      "cnt: 0 - valLoss: 0.4085598587989807 - trainLoss: 0.39673611521720886\n",
      "cnt: 0 - valLoss: 0.4085579216480255 - trainLoss: 0.39673087000846863\n",
      "cnt: 0 - valLoss: 0.4085554778575897 - trainLoss: 0.3967256546020508\n",
      "cnt: 0 - valLoss: 0.4085538387298584 - trainLoss: 0.39672040939331055\n",
      "cnt: 0 - valLoss: 0.4085513949394226 - trainLoss: 0.3967151939868927\n",
      "cnt: 0 - valLoss: 0.4085487127304077 - trainLoss: 0.39670997858047485\n",
      "cnt: 0 - valLoss: 0.40854740142822266 - trainLoss: 0.3967047333717346\n",
      "cnt: 0 - valLoss: 0.4085448086261749 - trainLoss: 0.3966995179653168\n",
      "cnt: 0 - valLoss: 0.4085426330566406 - trainLoss: 0.39669427275657654\n",
      "cnt: 0 - valLoss: 0.40854018926620483 - trainLoss: 0.3966890573501587\n",
      "cnt: 0 - valLoss: 0.40853869915008545 - trainLoss: 0.39668381214141846\n",
      "cnt: 0 - valLoss: 0.408535897731781 - trainLoss: 0.396678626537323\n",
      "cnt: 0 - valLoss: 0.4085340201854706 - trainLoss: 0.39667338132858276\n",
      "cnt: 0 - valLoss: 0.40853139758110046 - trainLoss: 0.3966681659221649\n",
      "cnt: 0 - valLoss: 0.4085295498371124 - trainLoss: 0.39666295051574707\n",
      "cnt: 0 - valLoss: 0.40852680802345276 - trainLoss: 0.39665770530700684\n",
      "cnt: 0 - valLoss: 0.40852510929107666 - trainLoss: 0.3966525197029114\n",
      "cnt: 0 - valLoss: 0.4085227847099304 - trainLoss: 0.39664727449417114\n",
      "cnt: 0 - valLoss: 0.4085213243961334 - trainLoss: 0.3966420590877533\n",
      "cnt: 0 - valLoss: 0.40851831436157227 - trainLoss: 0.39663687348365784\n",
      "cnt: 0 - valLoss: 0.40851646661758423 - trainLoss: 0.3966316282749176\n",
      "cnt: 0 - valLoss: 0.4085140824317932 - trainLoss: 0.39662641286849976\n",
      "cnt: 0 - valLoss: 0.4085121750831604 - trainLoss: 0.3966212272644043\n",
      "cnt: 0 - valLoss: 0.4085097908973694 - trainLoss: 0.39661601185798645\n",
      "cnt: 0 - valLoss: 0.40850743651390076 - trainLoss: 0.3966107964515686\n",
      "cnt: 0 - valLoss: 0.40850579738616943 - trainLoss: 0.39660558104515076\n",
      "cnt: 0 - valLoss: 0.4085035026073456 - trainLoss: 0.3966003954410553\n",
      "cnt: 0 - valLoss: 0.4085010290145874 - trainLoss: 0.39659518003463745\n",
      "cnt: 0 - valLoss: 0.40849876403808594 - trainLoss: 0.3965899646282196\n",
      "cnt: 0 - valLoss: 0.408497154712677 - trainLoss: 0.39658474922180176\n",
      "cnt: 0 - valLoss: 0.4084944725036621 - trainLoss: 0.3965795636177063\n",
      "cnt: 0 - valLoss: 0.4084922671318054 - trainLoss: 0.39657434821128845\n",
      "cnt: 0 - valLoss: 0.4084910452365875 - trainLoss: 0.396569162607193\n",
      "cnt: 0 - valLoss: 0.40848803520202637 - trainLoss: 0.39656394720077515\n",
      "cnt: 0 - valLoss: 0.4084865152835846 - trainLoss: 0.3965587317943573\n",
      "cnt: 0 - valLoss: 0.40848401188850403 - trainLoss: 0.39655354619026184\n",
      "cnt: 0 - valLoss: 0.4084821939468384 - trainLoss: 0.3965483605861664\n",
      "cnt: 0 - valLoss: 0.4084799587726593 - trainLoss: 0.3965431749820709\n",
      "cnt: 0 - valLoss: 0.40847742557525635 - trainLoss: 0.39653798937797546\n",
      "cnt: 0 - valLoss: 0.40847617387771606 - trainLoss: 0.39653280377388\n",
      "cnt: 0 - valLoss: 0.4084739685058594 - trainLoss: 0.39652761816978455\n",
      "cnt: 0 - valLoss: 0.4084716737270355 - trainLoss: 0.3965224027633667\n",
      "cnt: 0 - valLoss: 0.4084698259830475 - trainLoss: 0.39651724696159363\n",
      "cnt: 0 - valLoss: 0.4084673821926117 - trainLoss: 0.39651206135749817\n",
      "cnt: 0 - valLoss: 0.4084661304950714 - trainLoss: 0.3965068459510803\n",
      "cnt: 0 - valLoss: 0.4084629714488983 - trainLoss: 0.39650169014930725\n",
      "cnt: 0 - valLoss: 0.40846186876296997 - trainLoss: 0.396496444940567\n",
      "cnt: 0 - valLoss: 0.4084596335887909 - trainLoss: 0.39649131894111633\n",
      "cnt: 0 - valLoss: 0.40845760703086853 - trainLoss: 0.3964861035346985\n",
      "cnt: 0 - valLoss: 0.4084553122520447 - trainLoss: 0.396480917930603\n",
      "cnt: 0 - valLoss: 0.40845274925231934 - trainLoss: 0.39647573232650757\n",
      "cnt: 0 - valLoss: 0.40845173597335815 - trainLoss: 0.3964705765247345\n",
      "cnt: 0 - valLoss: 0.40844884514808655 - trainLoss: 0.39646539092063904\n",
      "cnt: 0 - valLoss: 0.40844762325286865 - trainLoss: 0.3964602053165436\n",
      "cnt: 0 - valLoss: 0.4084450900554657 - trainLoss: 0.3964550197124481\n",
      "cnt: 0 - valLoss: 0.40844273567199707 - trainLoss: 0.39644983410835266\n",
      "cnt: 0 - valLoss: 0.4084411561489105 - trainLoss: 0.3964446783065796\n",
      "cnt: 0 - valLoss: 0.4084388315677643 - trainLoss: 0.39643949270248413\n",
      "cnt: 0 - valLoss: 0.4084375202655792 - trainLoss: 0.39643433690071106\n",
      "cnt: 0 - valLoss: 0.40843474864959717 - trainLoss: 0.3964291512966156\n",
      "cnt: 0 - valLoss: 0.4084334671497345 - trainLoss: 0.39642396569252014\n",
      "cnt: 0 - valLoss: 0.4084307551383972 - trainLoss: 0.3964187800884247\n",
      "cnt: 0 - valLoss: 0.40842869877815247 - trainLoss: 0.396413654088974\n",
      "cnt: 0 - valLoss: 0.4084271192550659 - trainLoss: 0.39640846848487854\n",
      "cnt: 0 - valLoss: 0.4084242284297943 - trainLoss: 0.39640331268310547\n",
      "cnt: 0 - valLoss: 0.40842291712760925 - trainLoss: 0.3963981568813324\n",
      "cnt: 0 - valLoss: 0.40842074155807495 - trainLoss: 0.39639297127723694\n",
      "cnt: 0 - valLoss: 0.40841907262802124 - trainLoss: 0.39638781547546387\n",
      "cnt: 0 - valLoss: 0.4084169566631317 - trainLoss: 0.3963826596736908\n",
      "cnt: 0 - valLoss: 0.4084146320819855 - trainLoss: 0.39637747406959534\n",
      "cnt: 0 - valLoss: 0.40841323137283325 - trainLoss: 0.39637231826782227\n",
      "cnt: 0 - valLoss: 0.4084104895591736 - trainLoss: 0.3963671624660492\n",
      "cnt: 0 - valLoss: 0.4084089696407318 - trainLoss: 0.3963620066642761\n",
      "cnt: 0 - valLoss: 0.40840673446655273 - trainLoss: 0.39635685086250305\n",
      "cnt: 0 - valLoss: 0.4084051847457886 - trainLoss: 0.39635169506073\n",
      "cnt: 0 - valLoss: 0.40840277075767517 - trainLoss: 0.3963465690612793\n",
      "cnt: 0 - valLoss: 0.4084009528160095 - trainLoss: 0.39634138345718384\n",
      "cnt: 0 - valLoss: 0.4083993136882782 - trainLoss: 0.39633622765541077\n",
      "cnt: 0 - valLoss: 0.40839672088623047 - trainLoss: 0.3963311016559601\n",
      "cnt: 0 - valLoss: 0.40839511156082153 - trainLoss: 0.39632588624954224\n",
      "cnt: 0 - valLoss: 0.40839269757270813 - trainLoss: 0.39632079005241394\n",
      "cnt: 0 - valLoss: 0.4083915948867798 - trainLoss: 0.3963156044483185\n",
      "cnt: 0 - valLoss: 0.40838858485221863 - trainLoss: 0.3963104784488678\n",
      "cnt: 0 - valLoss: 0.4083867371082306 - trainLoss: 0.39630529284477234\n",
      "cnt: 0 - valLoss: 0.4083855450153351 - trainLoss: 0.39630013704299927\n",
      "cnt: 0 - valLoss: 0.40838274359703064 - trainLoss: 0.39629507064819336\n",
      "cnt: 0 - valLoss: 0.4083804786205292 - trainLoss: 0.3962898552417755\n",
      "cnt: 0 - valLoss: 0.4083792269229889 - trainLoss: 0.39628472924232483\n",
      "cnt: 0 - valLoss: 0.40837687253952026 - trainLoss: 0.39627957344055176\n",
      "cnt: 0 - valLoss: 0.4083753526210785 - trainLoss: 0.39627447724342346\n",
      "cnt: 0 - valLoss: 0.4083731472492218 - trainLoss: 0.396269291639328\n",
      "cnt: 0 - valLoss: 0.40837159752845764 - trainLoss: 0.39626413583755493\n",
      "cnt: 0 - valLoss: 0.4083693027496338 - trainLoss: 0.39625903964042664\n",
      "cnt: 0 - valLoss: 0.408366858959198 - trainLoss: 0.39625388383865356\n",
      "cnt: 0 - valLoss: 0.4083651900291443 - trainLoss: 0.3962487578392029\n",
      "cnt: 0 - valLoss: 0.40836313366889954 - trainLoss: 0.3962436020374298\n",
      "cnt: 0 - valLoss: 0.4083612561225891 - trainLoss: 0.3962384760379791\n",
      "cnt: 0 - valLoss: 0.4083592891693115 - trainLoss: 0.39623335003852844\n",
      "cnt: 0 - valLoss: 0.40835776925086975 - trainLoss: 0.39622819423675537\n",
      "cnt: 0 - valLoss: 0.40835538506507874 - trainLoss: 0.3962230682373047\n",
      "cnt: 0 - valLoss: 0.4083535075187683 - trainLoss: 0.396217942237854\n",
      "cnt: 0 - valLoss: 0.40835168957710266 - trainLoss: 0.39621278643608093\n",
      "cnt: 0 - valLoss: 0.40834948420524597 - trainLoss: 0.39620769023895264\n",
      "cnt: 0 - valLoss: 0.40834760665893555 - trainLoss: 0.3962025046348572\n",
      "cnt: 0 - valLoss: 0.4083452522754669 - trainLoss: 0.3961973786354065\n",
      "cnt: 0 - valLoss: 0.4083428978919983 - trainLoss: 0.3961923122406006\n",
      "cnt: 0 - valLoss: 0.40834057331085205 - trainLoss: 0.3961871564388275\n",
      "cnt: 0 - valLoss: 0.4083385169506073 - trainLoss: 0.39618203043937683\n",
      "cnt: 0 - valLoss: 0.40833666920661926 - trainLoss: 0.3961769640445709\n",
      "cnt: 0 - valLoss: 0.40833422541618347 - trainLoss: 0.39617180824279785\n",
      "cnt: 0 - valLoss: 0.4083327651023865 - trainLoss: 0.39616668224334717\n",
      "cnt: 0 - valLoss: 0.4083302319049835 - trainLoss: 0.39616158604621887\n",
      "cnt: 0 - valLoss: 0.4083273708820343 - trainLoss: 0.3961564600467682\n",
      "cnt: 0 - valLoss: 0.40832582116127014 - trainLoss: 0.3961513638496399\n",
      "cnt: 0 - valLoss: 0.4083232283592224 - trainLoss: 0.3961462378501892\n",
      "cnt: 0 - valLoss: 0.40832191705703735 - trainLoss: 0.3961411416530609\n",
      "cnt: 0 - valLoss: 0.4083186686038971 - trainLoss: 0.3961360454559326\n",
      "cnt: 0 - valLoss: 0.40831682085990906 - trainLoss: 0.39613088965415955\n",
      "cnt: 0 - valLoss: 0.40831488370895386 - trainLoss: 0.39612579345703125\n",
      "cnt: 0 - valLoss: 0.4083121120929718 - trainLoss: 0.39612069725990295\n",
      "cnt: 0 - valLoss: 0.4083094894886017 - trainLoss: 0.39611563086509705\n",
      "cnt: 0 - valLoss: 0.4083068072795868 - trainLoss: 0.39611053466796875\n",
      "cnt: 0 - valLoss: 0.40830501914024353 - trainLoss: 0.39610546827316284\n",
      "cnt: 0 - valLoss: 0.4083015024662018 - trainLoss: 0.39610037207603455\n",
      "cnt: 0 - valLoss: 0.40829896926879883 - trainLoss: 0.396095335483551\n",
      "cnt: 0 - valLoss: 0.40829721093177795 - trainLoss: 0.39609020948410034\n",
      "cnt: 0 - valLoss: 0.4082944691181183 - trainLoss: 0.3960851728916168\n",
      "cnt: 0 - valLoss: 0.4082925021648407 - trainLoss: 0.3960800766944885\n",
      "cnt: 0 - valLoss: 0.4082895517349243 - trainLoss: 0.3960750102996826\n",
      "cnt: 0 - valLoss: 0.4082871079444885 - trainLoss: 0.3960699141025543\n",
      "cnt: 0 - valLoss: 0.40828534960746765 - trainLoss: 0.3960648477077484\n",
      "cnt: 0 - valLoss: 0.40828245878219604 - trainLoss: 0.3960597813129425\n",
      "cnt: 0 - valLoss: 0.40828096866607666 - trainLoss: 0.396054744720459\n",
      "cnt: 0 - valLoss: 0.4082777500152588 - trainLoss: 0.3960496485233307\n",
      "cnt: 0 - valLoss: 0.4082757532596588 - trainLoss: 0.3960445523262024\n",
      "cnt: 0 - valLoss: 0.40827304124832153 - trainLoss: 0.39603951573371887\n",
      "cnt: 0 - valLoss: 0.4082707464694977 - trainLoss: 0.39603444933891296\n",
      "cnt: 0 - valLoss: 0.40826892852783203 - trainLoss: 0.39602938294410706\n",
      "cnt: 0 - valLoss: 0.40826615691185 - trainLoss: 0.39602428674697876\n",
      "cnt: 0 - valLoss: 0.408264696598053 - trainLoss: 0.39601922035217285\n",
      "cnt: 0 - valLoss: 0.4082620441913605 - trainLoss: 0.39601415395736694\n",
      "cnt: 0 - valLoss: 0.40825942158699036 - trainLoss: 0.39600905776023865\n",
      "cnt: 0 - valLoss: 0.40825769305229187 - trainLoss: 0.3960040211677551\n",
      "cnt: 0 - valLoss: 0.4082552194595337 - trainLoss: 0.39599889516830444\n",
      "cnt: 0 - valLoss: 0.40825292468070984 - trainLoss: 0.39599382877349854\n",
      "cnt: 0 - valLoss: 0.40825167298316956 - trainLoss: 0.39598870277404785\n",
      "cnt: 0 - valLoss: 0.4082488417625427 - trainLoss: 0.39598360657691956\n",
      "cnt: 0 - valLoss: 0.4082476794719696 - trainLoss: 0.39597851037979126\n",
      "cnt: 0 - valLoss: 0.4082445502281189 - trainLoss: 0.39597344398498535\n",
      "cnt: 0 - valLoss: 0.408242404460907 - trainLoss: 0.39596837759017944\n",
      "cnt: 0 - valLoss: 0.408241331577301 - trainLoss: 0.39596325159072876\n",
      "cnt: 0 - valLoss: 0.40823838114738464 - trainLoss: 0.39595818519592285\n",
      "cnt: 0 - valLoss: 0.4082360565662384 - trainLoss: 0.39595311880111694\n",
      "cnt: 0 - valLoss: 0.40823450684547424 - trainLoss: 0.39594805240631104\n",
      "cnt: 0 - valLoss: 0.40823203325271606 - trainLoss: 0.39594295620918274\n",
      "cnt: 0 - valLoss: 0.4082307815551758 - trainLoss: 0.39593786001205444\n",
      "cnt: 0 - valLoss: 0.4082277715206146 - trainLoss: 0.3959328234195709\n",
      "cnt: 0 - valLoss: 0.4082256257534027 - trainLoss: 0.3959277272224426\n",
      "cnt: 0 - valLoss: 0.40822428464889526 - trainLoss: 0.39592263102531433\n",
      "cnt: 0 - valLoss: 0.4082217812538147 - trainLoss: 0.3959175646305084\n",
      "cnt: 0 - valLoss: 0.4082195460796356 - trainLoss: 0.3959124982357025\n",
      "cnt: 0 - valLoss: 0.40821775794029236 - trainLoss: 0.3959074914455414\n",
      "cnt: 0 - valLoss: 0.4082152843475342 - trainLoss: 0.3959024250507355\n",
      "cnt: 0 - valLoss: 0.408214271068573 - trainLoss: 0.39589738845825195\n",
      "cnt: 0 - valLoss: 0.4082113802433014 - trainLoss: 0.39589235186576843\n",
      "cnt: 0 - valLoss: 0.40820908546447754 - trainLoss: 0.3958873152732849\n",
      "cnt: 0 - valLoss: 0.4082072973251343 - trainLoss: 0.395882248878479\n",
      "cnt: 0 - valLoss: 0.4082050323486328 - trainLoss: 0.39587724208831787\n",
      "cnt: 0 - valLoss: 0.40820279717445374 - trainLoss: 0.39587217569351196\n",
      "cnt: 0 - valLoss: 0.4082013964653015 - trainLoss: 0.39586716890335083\n",
      "cnt: 0 - valLoss: 0.40819883346557617 - trainLoss: 0.3958621025085449\n",
      "cnt: 0 - valLoss: 0.40819743275642395 - trainLoss: 0.3958570659160614\n",
      "cnt: 0 - valLoss: 0.4081949293613434 - trainLoss: 0.39585208892822266\n",
      "cnt: 0 - valLoss: 0.40819260478019714 - trainLoss: 0.39584702253341675\n",
      "cnt: 0 - valLoss: 0.40819066762924194 - trainLoss: 0.3958419859409332\n",
      "cnt: 0 - valLoss: 0.40818849205970764 - trainLoss: 0.3958369493484497\n",
      "cnt: 0 - valLoss: 0.408185750246048 - trainLoss: 0.3958319127559662\n",
      "cnt: 0 - valLoss: 0.4081847369670868 - trainLoss: 0.39582690596580505\n",
      "cnt: 0 - valLoss: 0.40818196535110474 - trainLoss: 0.3958218991756439\n",
      "cnt: 0 - valLoss: 0.4081808030605316 - trainLoss: 0.3958168029785156\n",
      "cnt: 0 - valLoss: 0.4081779718399048 - trainLoss: 0.3958118259906769\n",
      "cnt: 0 - valLoss: 0.4081757366657257 - trainLoss: 0.39580678939819336\n",
      "cnt: 0 - valLoss: 0.40817466378211975 - trainLoss: 0.39580175280570984\n",
      "cnt: 0 - valLoss: 0.40817174315452576 - trainLoss: 0.3957967460155487\n",
      "cnt: 0 - valLoss: 0.40816906094551086 - trainLoss: 0.3957917094230652\n",
      "cnt: 0 - valLoss: 0.40816810727119446 - trainLoss: 0.39578673243522644\n",
      "cnt: 0 - valLoss: 0.4081653356552124 - trainLoss: 0.3957816958427429\n",
      "cnt: 0 - valLoss: 0.40816450119018555 - trainLoss: 0.395776629447937\n",
      "cnt: 0 - valLoss: 0.40816158056259155 - trainLoss: 0.39577165246009827\n",
      "cnt: 0 - valLoss: 0.4081593155860901 - trainLoss: 0.39576661586761475\n",
      "cnt: 0 - valLoss: 0.4081578552722931 - trainLoss: 0.395761638879776\n",
      "cnt: 0 - valLoss: 0.408155620098114 - trainLoss: 0.3957566022872925\n",
      "cnt: 0 - valLoss: 0.40815243124961853 - trainLoss: 0.39575162529945374\n",
      "cnt: 0 - valLoss: 0.40815162658691406 - trainLoss: 0.3957465589046478\n",
      "cnt: 0 - valLoss: 0.4081491529941559 - trainLoss: 0.3957415521144867\n",
      "cnt: 0 - valLoss: 0.40814799070358276 - trainLoss: 0.39573654532432556\n",
      "cnt: 0 - valLoss: 0.4081452786922455 - trainLoss: 0.39573153853416443\n",
      "cnt: 0 - valLoss: 0.4081423580646515 - trainLoss: 0.3957265317440033\n",
      "cnt: 0 - valLoss: 0.40814143419265747 - trainLoss: 0.39572152495384216\n",
      "cnt: 0 - valLoss: 0.40813907980918884 - trainLoss: 0.39571651816368103\n",
      "cnt: 0 - valLoss: 0.4081367254257202 - trainLoss: 0.3957115113735199\n",
      "cnt: 0 - valLoss: 0.4081355035305023 - trainLoss: 0.39570650458335876\n",
      "cnt: 0 - valLoss: 0.408132940530777 - trainLoss: 0.39570146799087524\n",
      "cnt: 0 - valLoss: 0.40813055634498596 - trainLoss: 0.3956964910030365\n",
      "cnt: 0 - valLoss: 0.4081296920776367 - trainLoss: 0.39569151401519775\n",
      "cnt: 0 - valLoss: 0.40812671184539795 - trainLoss: 0.3956865072250366\n",
      "cnt: 0 - valLoss: 0.408125638961792 - trainLoss: 0.3956814706325531\n",
      "cnt: 0 - valLoss: 0.408122718334198 - trainLoss: 0.39567652344703674\n",
      "cnt: 0 - valLoss: 0.40812110900878906 - trainLoss: 0.3956715166568756\n",
      "cnt: 0 - valLoss: 0.4081192910671234 - trainLoss: 0.3956665098667145\n",
      "cnt: 0 - valLoss: 0.40811705589294434 - trainLoss: 0.3956615626811981\n",
      "cnt: 0 - valLoss: 0.4081151485443115 - trainLoss: 0.3956565856933594\n",
      "cnt: 0 - valLoss: 0.408113032579422 - trainLoss: 0.39565154910087585\n",
      "cnt: 0 - valLoss: 0.4081105887889862 - trainLoss: 0.3956465721130371\n",
      "cnt: 0 - valLoss: 0.40810921788215637 - trainLoss: 0.39564162492752075\n",
      "cnt: 0 - valLoss: 0.4081067144870758 - trainLoss: 0.3956366181373596\n",
      "cnt: 0 - valLoss: 0.40810465812683105 - trainLoss: 0.3956316411495209\n",
      "cnt: 0 - valLoss: 0.4081030786037445 - trainLoss: 0.39562663435935974\n",
      "cnt: 0 - valLoss: 0.40810084342956543 - trainLoss: 0.395621657371521\n",
      "cnt: 0 - valLoss: 0.4080994725227356 - trainLoss: 0.39561668038368225\n",
      "cnt: 0 - valLoss: 0.4080972671508789 - trainLoss: 0.3956117331981659\n",
      "cnt: 0 - valLoss: 0.4080943167209625 - trainLoss: 0.39560672640800476\n",
      "cnt: 0 - valLoss: 0.4080934524536133 - trainLoss: 0.395601749420166\n",
      "cnt: 0 - valLoss: 0.4080906808376312 - trainLoss: 0.39559677243232727\n",
      "cnt: 0 - valLoss: 0.4080890417098999 - trainLoss: 0.3955917954444885\n",
      "cnt: 0 - valLoss: 0.4080865979194641 - trainLoss: 0.3955867886543274\n",
      "cnt: 0 - valLoss: 0.408084511756897 - trainLoss: 0.3955818712711334\n",
      "cnt: 0 - valLoss: 0.4080829322338104 - trainLoss: 0.3955768644809723\n",
      "cnt: 0 - valLoss: 0.40808117389678955 - trainLoss: 0.39557191729545593\n",
      "cnt: 0 - valLoss: 0.408078670501709 - trainLoss: 0.3955669403076172\n",
      "cnt: 0 - valLoss: 0.4080764949321747 - trainLoss: 0.39556196331977844\n",
      "cnt: 0 - valLoss: 0.40807515382766724 - trainLoss: 0.3955570161342621\n",
      "cnt: 0 - valLoss: 0.40807297825813293 - trainLoss: 0.39555200934410095\n",
      "cnt: 0 - valLoss: 0.4080710709095001 - trainLoss: 0.39554712176322937\n",
      "cnt: 0 - valLoss: 0.40806883573532104 - trainLoss: 0.39554211497306824\n",
      "cnt: 0 - valLoss: 0.4080665409564972 - trainLoss: 0.3955371677875519\n",
      "cnt: 0 - valLoss: 0.4080653488636017 - trainLoss: 0.39553219079971313\n",
      "cnt: 0 - valLoss: 0.4080623984336853 - trainLoss: 0.39552727341651917\n",
      "cnt: 0 - valLoss: 0.408060759305954 - trainLoss: 0.3955223262310028\n",
      "cnt: 0 - valLoss: 0.4080592393875122 - trainLoss: 0.39551734924316406\n",
      "cnt: 0 - valLoss: 0.40805670619010925 - trainLoss: 0.3955124616622925\n",
      "cnt: 0 - valLoss: 0.4080542027950287 - trainLoss: 0.3955075144767761\n",
      "cnt: 0 - valLoss: 0.4080527126789093 - trainLoss: 0.39550256729125977\n",
      "cnt: 0 - valLoss: 0.4080502688884735 - trainLoss: 0.3954976201057434\n",
      "cnt: 0 - valLoss: 0.4080488085746765 - trainLoss: 0.39549270272254944\n",
      "cnt: 0 - valLoss: 0.4080463647842407 - trainLoss: 0.3954877555370331\n",
      "cnt: 0 - valLoss: 0.4080442786216736 - trainLoss: 0.3954828083515167\n",
      "cnt: 0 - valLoss: 0.4080428183078766 - trainLoss: 0.39547789096832275\n",
      "cnt: 0 - valLoss: 0.4080406427383423 - trainLoss: 0.3954729437828064\n",
      "cnt: 0 - valLoss: 0.408037930727005 - trainLoss: 0.3954680562019348\n",
      "cnt: 0 - valLoss: 0.40803706645965576 - trainLoss: 0.39546310901641846\n",
      "cnt: 0 - valLoss: 0.40803414583206177 - trainLoss: 0.3954581618309021\n",
      "cnt: 0 - valLoss: 0.40803253650665283 - trainLoss: 0.39545324444770813\n",
      "cnt: 0 - valLoss: 0.40803053975105286 - trainLoss: 0.3954482972621918\n",
      "cnt: 0 - valLoss: 0.40802812576293945 - trainLoss: 0.3954434096813202\n",
      "cnt: 0 - valLoss: 0.40802693367004395 - trainLoss: 0.3954384922981262\n",
      "cnt: 0 - valLoss: 0.40802380442619324 - trainLoss: 0.39543357491493225\n",
      "cnt: 0 - valLoss: 0.40802201628685 - trainLoss: 0.3954286575317383\n",
      "cnt: 0 - valLoss: 0.40802058577537537 - trainLoss: 0.3954237103462219\n",
      "cnt: 0 - valLoss: 0.40801844000816345 - trainLoss: 0.39541879296302795\n",
      "cnt: 0 - valLoss: 0.40801605582237244 - trainLoss: 0.39541390538215637\n",
      "cnt: 0 - valLoss: 0.40801432728767395 - trainLoss: 0.39540895819664\n",
      "cnt: 0 - valLoss: 0.4080122411251068 - trainLoss: 0.39540407061576843\n",
      "cnt: 0 - valLoss: 0.4080105423927307 - trainLoss: 0.39539915323257446\n",
      "cnt: 0 - valLoss: 0.4080084562301636 - trainLoss: 0.3953942358493805\n",
      "cnt: 0 - valLoss: 0.40800511837005615 - trainLoss: 0.3953893482685089\n",
      "cnt: 0 - valLoss: 0.40800440311431885 - trainLoss: 0.39538443088531494\n",
      "cnt: 0 - valLoss: 0.40800222754478455 - trainLoss: 0.39537954330444336\n",
      "cnt: 0 - valLoss: 0.40800026059150696 - trainLoss: 0.395374596118927\n",
      "cnt: 0 - valLoss: 0.40799757838249207 - trainLoss: 0.3953697085380554\n",
      "cnt: 0 - valLoss: 0.4079957902431488 - trainLoss: 0.39536482095718384\n",
      "cnt: 0 - valLoss: 0.4079945385456085 - trainLoss: 0.3953598737716675\n",
      "cnt: 0 - valLoss: 0.407992422580719 - trainLoss: 0.3953549861907959\n",
      "cnt: 0 - valLoss: 0.40798985958099365 - trainLoss: 0.3953500986099243\n",
      "cnt: 0 - valLoss: 0.4079889953136444 - trainLoss: 0.39534521102905273\n",
      "cnt: 0 - valLoss: 0.4079861342906952 - trainLoss: 0.39534029364585876\n",
      "cnt: 0 - valLoss: 0.40798419713974 - trainLoss: 0.3953354060649872\n",
      "cnt: 0 - valLoss: 0.40798220038414 - trainLoss: 0.3953304886817932\n",
      "cnt: 0 - valLoss: 0.40798014402389526 - trainLoss: 0.39532560110092163\n",
      "cnt: 0 - valLoss: 0.40797773003578186 - trainLoss: 0.39532068371772766\n",
      "cnt: 0 - valLoss: 0.40797698497772217 - trainLoss: 0.3953157961368561\n",
      "cnt: 0 - valLoss: 0.40797320008277893 - trainLoss: 0.3953108787536621\n",
      "cnt: 0 - valLoss: 0.4079713523387909 - trainLoss: 0.3953060209751129\n",
      "cnt: 0 - valLoss: 0.4079703688621521 - trainLoss: 0.39530113339424133\n",
      "cnt: 0 - valLoss: 0.40796831250190735 - trainLoss: 0.39529624581336975\n",
      "cnt: 0 - valLoss: 0.40796616673469543 - trainLoss: 0.3952913284301758\n",
      "cnt: 0 - valLoss: 0.4079638719558716 - trainLoss: 0.395286500453949\n",
      "cnt: 0 - valLoss: 0.40796202421188354 - trainLoss: 0.3952815532684326\n",
      "cnt: 0 - valLoss: 0.4079609513282776 - trainLoss: 0.39527663588523865\n",
      "cnt: 0 - valLoss: 0.4079588055610657 - trainLoss: 0.39527177810668945\n",
      "cnt: 0 - valLoss: 0.40795576572418213 - trainLoss: 0.39526689052581787\n",
      "cnt: 0 - valLoss: 0.40795451402664185 - trainLoss: 0.3952620029449463\n",
      "cnt: 0 - valLoss: 0.4079522490501404 - trainLoss: 0.3952571153640747\n",
      "cnt: 0 - valLoss: 0.40795040130615234 - trainLoss: 0.3952522277832031\n",
      "cnt: 0 - valLoss: 0.40794798731803894 - trainLoss: 0.39524737000465393\n",
      "cnt: 0 - valLoss: 0.4079466760158539 - trainLoss: 0.39524248242378235\n",
      "cnt: 0 - valLoss: 0.4079444408416748 - trainLoss: 0.39523759484291077\n",
      "cnt: 0 - valLoss: 0.4079422950744629 - trainLoss: 0.3952327072620392\n",
      "cnt: 0 - valLoss: 0.4079405963420868 - trainLoss: 0.3952278196811676\n",
      "cnt: 0 - valLoss: 0.4079388380050659 - trainLoss: 0.395222932100296\n",
      "cnt: 0 - valLoss: 0.40793681144714355 - trainLoss: 0.3952181041240692\n",
      "cnt: 0 - valLoss: 0.4079344570636749 - trainLoss: 0.39521318674087524\n",
      "cnt: 0 - valLoss: 0.40793269872665405 - trainLoss: 0.39520829916000366\n",
      "cnt: 0 - valLoss: 0.4079307019710541 - trainLoss: 0.39520344138145447\n",
      "cnt: 0 - valLoss: 0.40792906284332275 - trainLoss: 0.3951985239982605\n",
      "cnt: 0 - valLoss: 0.40792664885520935 - trainLoss: 0.3951937258243561\n",
      "cnt: 0 - valLoss: 0.4079245328903198 - trainLoss: 0.3951888382434845\n",
      "cnt: 0 - valLoss: 0.40792348980903625 - trainLoss: 0.3951839506626129\n",
      "cnt: 0 - valLoss: 0.4079209268093109 - trainLoss: 0.3951791226863861\n",
      "cnt: 0 - valLoss: 0.40791910886764526 - trainLoss: 0.3951742351055145\n",
      "cnt: 0 - valLoss: 0.4079168736934662 - trainLoss: 0.39516937732696533\n",
      "cnt: 0 - valLoss: 0.4079151451587677 - trainLoss: 0.3951645493507385\n",
      "cnt: 0 - valLoss: 0.4079141616821289 - trainLoss: 0.39515966176986694\n",
      "cnt: 0 - valLoss: 0.4079117178916931 - trainLoss: 0.39515480399131775\n",
      "cnt: 0 - valLoss: 0.4079090356826782 - trainLoss: 0.39514997601509094\n",
      "cnt: 0 - valLoss: 0.4079076647758484 - trainLoss: 0.39514508843421936\n",
      "cnt: 0 - valLoss: 0.40790554881095886 - trainLoss: 0.39514023065567017\n",
      "cnt: 0 - valLoss: 0.4079034924507141 - trainLoss: 0.3951353430747986\n",
      "cnt: 0 - valLoss: 0.40790215134620667 - trainLoss: 0.3951305150985718\n",
      "cnt: 0 - valLoss: 0.4078994691371918 - trainLoss: 0.39512568712234497\n",
      "cnt: 0 - valLoss: 0.40789785981178284 - trainLoss: 0.39512085914611816\n",
      "cnt: 0 - valLoss: 0.40789613127708435 - trainLoss: 0.39511600136756897\n",
      "cnt: 0 - valLoss: 0.4078938961029053 - trainLoss: 0.39511117339134216\n",
      "cnt: 0 - valLoss: 0.4078916311264038 - trainLoss: 0.39510634541511536\n",
      "cnt: 0 - valLoss: 0.40789076685905457 - trainLoss: 0.39510151743888855\n",
      "cnt: 0 - valLoss: 0.40788763761520386 - trainLoss: 0.39509665966033936\n",
      "cnt: 0 - valLoss: 0.40788713097572327 - trainLoss: 0.39509183168411255\n",
      "cnt: 0 - valLoss: 0.40788447856903076 - trainLoss: 0.39508703351020813\n",
      "cnt: 0 - valLoss: 0.40788212418556213 - trainLoss: 0.3950822055339813\n",
      "cnt: 0 - valLoss: 0.40788108110427856 - trainLoss: 0.3950773775577545\n",
      "cnt: 0 - valLoss: 0.4078781306743622 - trainLoss: 0.3950726091861725\n",
      "cnt: 0 - valLoss: 0.40787598490715027 - trainLoss: 0.3950677216053009\n",
      "cnt: 0 - valLoss: 0.40787532925605774 - trainLoss: 0.3950628936290741\n",
      "cnt: 0 - valLoss: 0.4078730642795563 - trainLoss: 0.3950580656528473\n",
      "cnt: 0 - valLoss: 0.40787068009376526 - trainLoss: 0.3950532376766205\n",
      "cnt: 0 - valLoss: 0.4078689217567444 - trainLoss: 0.39504846930503845\n",
      "cnt: 0 - valLoss: 0.407866895198822 - trainLoss: 0.39504364132881165\n",
      "cnt: 0 - valLoss: 0.40786561369895935 - trainLoss: 0.39503881335258484\n",
      "cnt: 0 - valLoss: 0.40786296129226685 - trainLoss: 0.3950340151786804\n",
      "cnt: 0 - valLoss: 0.40786170959472656 - trainLoss: 0.3950291574001312\n",
      "cnt: 0 - valLoss: 0.40785959362983704 - trainLoss: 0.3950243592262268\n",
      "cnt: 0 - valLoss: 0.40785783529281616 - trainLoss: 0.3950195610523224\n",
      "cnt: 0 - valLoss: 0.40785524249076843 - trainLoss: 0.3950147330760956\n",
      "cnt: 0 - valLoss: 0.407853901386261 - trainLoss: 0.39500993490219116\n",
      "cnt: 0 - valLoss: 0.4078519940376282 - trainLoss: 0.39500513672828674\n",
      "cnt: 0 - valLoss: 0.4078505039215088 - trainLoss: 0.39500030875205994\n",
      "cnt: 0 - valLoss: 0.40784817934036255 - trainLoss: 0.3949955105781555\n",
      "cnt: 0 - valLoss: 0.4078463315963745 - trainLoss: 0.3949906826019287\n",
      "cnt: 0 - valLoss: 0.4078448712825775 - trainLoss: 0.3949858248233795\n",
      "cnt: 0 - valLoss: 0.4078425168991089 - trainLoss: 0.3949810564517975\n",
      "cnt: 0 - valLoss: 0.4078404903411865 - trainLoss: 0.39497625827789307\n",
      "cnt: 0 - valLoss: 0.40783971548080444 - trainLoss: 0.39497146010398865\n",
      "cnt: 0 - valLoss: 0.40783655643463135 - trainLoss: 0.39496666193008423\n",
      "cnt: 0 - valLoss: 0.40783414244651794 - trainLoss: 0.3949618637561798\n",
      "cnt: 0 - valLoss: 0.4078328013420105 - trainLoss: 0.3949570953845978\n",
      "cnt: 0 - valLoss: 0.40782982110977173 - trainLoss: 0.39495235681533813\n",
      "cnt: 0 - valLoss: 0.40782850980758667 - trainLoss: 0.3949475884437561\n",
      "cnt: 0 - valLoss: 0.407825767993927 - trainLoss: 0.39494284987449646\n",
      "cnt: 0 - valLoss: 0.40782395005226135 - trainLoss: 0.39493805170059204\n",
      "cnt: 0 - valLoss: 0.40782079100608826 - trainLoss: 0.3949333429336548\n",
      "cnt: 0 - valLoss: 0.40781939029693604 - trainLoss: 0.39492857456207275\n",
      "cnt: 0 - valLoss: 0.40781715512275696 - trainLoss: 0.3949238061904907\n",
      "cnt: 0 - valLoss: 0.4078153371810913 - trainLoss: 0.3949190378189087\n",
      "cnt: 0 - valLoss: 0.40781307220458984 - trainLoss: 0.39491429924964905\n",
      "cnt: 0 - valLoss: 0.40781038999557495 - trainLoss: 0.394909530878067\n",
      "cnt: 0 - valLoss: 0.4078091084957123 - trainLoss: 0.3949047923088074\n",
      "cnt: 0 - valLoss: 0.4078060984611511 - trainLoss: 0.39490005373954773\n",
      "cnt: 0 - valLoss: 0.4078042507171631 - trainLoss: 0.3948952853679657\n",
      "cnt: 0 - valLoss: 0.4078025221824646 - trainLoss: 0.39489051699638367\n",
      "cnt: 0 - valLoss: 0.40780016779899597 - trainLoss: 0.394885778427124\n",
      "cnt: 0 - valLoss: 0.40779703855514526 - trainLoss: 0.394881010055542\n",
      "cnt: 0 - valLoss: 0.40779560804367065 - trainLoss: 0.39487627148628235\n",
      "cnt: 0 - valLoss: 0.4077935516834259 - trainLoss: 0.3948715329170227\n",
      "cnt: 0 - valLoss: 0.40779200196266174 - trainLoss: 0.3948667347431183\n",
      "cnt: 0 - valLoss: 0.40778985619544983 - trainLoss: 0.39486199617385864\n",
      "cnt: 0 - valLoss: 0.4077865779399872 - trainLoss: 0.394857257604599\n",
      "cnt: 0 - valLoss: 0.40778541564941406 - trainLoss: 0.39485251903533936\n",
      "cnt: 0 - valLoss: 0.4077829122543335 - trainLoss: 0.3948477804660797\n",
      "cnt: 0 - valLoss: 0.4077802002429962 - trainLoss: 0.3948430120944977\n",
      "cnt: 0 - valLoss: 0.40777918696403503 - trainLoss: 0.39483827352523804\n",
      "cnt: 0 - valLoss: 0.4077771306037903 - trainLoss: 0.3948335349559784\n",
      "cnt: 0 - valLoss: 0.40777382254600525 - trainLoss: 0.39482879638671875\n",
      "cnt: 0 - valLoss: 0.4077727794647217 - trainLoss: 0.3948240578174591\n",
      "cnt: 0 - valLoss: 0.40777066349983215 - trainLoss: 0.39481931924819946\n",
      "cnt: 0 - valLoss: 0.4077683091163635 - trainLoss: 0.3948145806789398\n",
      "cnt: 0 - valLoss: 0.4077668786048889 - trainLoss: 0.3948098123073578\n",
      "cnt: 0 - valLoss: 0.4077644944190979 - trainLoss: 0.39480510354042053\n",
      "cnt: 0 - valLoss: 0.40776267647743225 - trainLoss: 0.3948003649711609\n",
      "cnt: 0 - valLoss: 0.4077610969543457 - trainLoss: 0.39479562640190125\n",
      "cnt: 0 - valLoss: 0.40775904059410095 - trainLoss: 0.3947908878326416\n",
      "cnt: 0 - valLoss: 0.4077565371990204 - trainLoss: 0.39478617906570435\n",
      "cnt: 0 - valLoss: 0.4077545702457428 - trainLoss: 0.3947814404964447\n",
      "cnt: 0 - valLoss: 0.40775415301322937 - trainLoss: 0.39477670192718506\n",
      "cnt: 0 - valLoss: 0.40775153040885925 - trainLoss: 0.3947720229625702\n",
      "cnt: 0 - valLoss: 0.40774980187416077 - trainLoss: 0.39476728439331055\n",
      "cnt: 0 - valLoss: 0.4077470302581787 - trainLoss: 0.3947625160217285\n",
      "cnt: 0 - valLoss: 0.40774625539779663 - trainLoss: 0.39475780725479126\n",
      "cnt: 0 - valLoss: 0.4077438712120056 - trainLoss: 0.394753098487854\n",
      "cnt: 0 - valLoss: 0.4077422618865967 - trainLoss: 0.39474838972091675\n",
      "cnt: 0 - valLoss: 0.40774044394493103 - trainLoss: 0.3947436511516571\n",
      "cnt: 0 - valLoss: 0.407737672328949 - trainLoss: 0.39473894238471985\n",
      "cnt: 0 - valLoss: 0.40773722529411316 - trainLoss: 0.3947342038154602\n",
      "cnt: 0 - valLoss: 0.40773478150367737 - trainLoss: 0.39472952485084534\n",
      "cnt: 0 - valLoss: 0.40773266553878784 - trainLoss: 0.3947247564792633\n",
      "cnt: 0 - valLoss: 0.40773046016693115 - trainLoss: 0.39472001791000366\n",
      "cnt: 0 - valLoss: 0.4077300429344177 - trainLoss: 0.3947153389453888\n",
      "cnt: 0 - valLoss: 0.4077267348766327 - trainLoss: 0.39471063017845154\n",
      "cnt: 0 - valLoss: 0.4077249765396118 - trainLoss: 0.39470595121383667\n",
      "cnt: 0 - valLoss: 0.40772342681884766 - trainLoss: 0.394701212644577\n",
      "cnt: 0 - valLoss: 0.40772104263305664 - trainLoss: 0.394696444272995\n",
      "cnt: 0 - valLoss: 0.4077204763889313 - trainLoss: 0.3946917951107025\n",
      "cnt: 0 - valLoss: 0.40771815180778503 - trainLoss: 0.39468705654144287\n",
      "cnt: 0 - valLoss: 0.40771612524986267 - trainLoss: 0.3946823477745056\n",
      "cnt: 0 - valLoss: 0.40771403908729553 - trainLoss: 0.39467766880989075\n",
      "cnt: 0 - valLoss: 0.4077128767967224 - trainLoss: 0.3946729898452759\n",
      "cnt: 0 - valLoss: 0.40770992636680603 - trainLoss: 0.39466825127601624\n",
      "cnt: 0 - valLoss: 0.40770843625068665 - trainLoss: 0.394663542509079\n",
      "cnt: 0 - valLoss: 0.4077070653438568 - trainLoss: 0.3946588337421417\n",
      "cnt: 0 - valLoss: 0.4077049493789673 - trainLoss: 0.39465415477752686\n",
      "cnt: 0 - valLoss: 0.4077031910419464 - trainLoss: 0.3946494460105896\n",
      "cnt: 0 - valLoss: 0.40770137310028076 - trainLoss: 0.39464476704597473\n",
      "cnt: 0 - valLoss: 0.4076998233795166 - trainLoss: 0.3946400582790375\n",
      "cnt: 0 - valLoss: 0.4076985716819763 - trainLoss: 0.394635409116745\n",
      "cnt: 0 - valLoss: 0.40769660472869873 - trainLoss: 0.39463067054748535\n",
      "cnt: 0 - valLoss: 0.4076942503452301 - trainLoss: 0.3946259915828705\n",
      "cnt: 0 - valLoss: 0.4076926112174988 - trainLoss: 0.3946213126182556\n",
      "cnt: 0 - valLoss: 0.4076903760433197 - trainLoss: 0.39461660385131836\n",
      "cnt: 0 - valLoss: 0.4076891839504242 - trainLoss: 0.3946118950843811\n",
      "cnt: 0 - valLoss: 0.4076870083808899 - trainLoss: 0.39460721611976624\n",
      "cnt: 0 - valLoss: 0.40768545866012573 - trainLoss: 0.394602507352829\n",
      "cnt: 0 - valLoss: 0.4076838195323944 - trainLoss: 0.3945978581905365\n",
      "cnt: 0 - valLoss: 0.4076817035675049 - trainLoss: 0.39459317922592163\n",
      "cnt: 0 - valLoss: 0.40767937898635864 - trainLoss: 0.3945884108543396\n",
      "cnt: 0 - valLoss: 0.40767791867256165 - trainLoss: 0.3945837616920471\n",
      "cnt: 0 - valLoss: 0.4076763391494751 - trainLoss: 0.39457908272743225\n",
      "cnt: 0 - valLoss: 0.40767431259155273 - trainLoss: 0.3945744037628174\n",
      "cnt: 0 - valLoss: 0.40767204761505127 - trainLoss: 0.3945697247982025\n",
      "cnt: 0 - valLoss: 0.407670795917511 - trainLoss: 0.39456507563591003\n",
      "cnt: 0 - valLoss: 0.40766894817352295 - trainLoss: 0.3945603370666504\n",
      "cnt: 0 - valLoss: 0.4076668620109558 - trainLoss: 0.3945556879043579\n",
      "cnt: 0 - valLoss: 0.407664954662323 - trainLoss: 0.39455100893974304\n",
      "cnt: 0 - valLoss: 0.40766364336013794 - trainLoss: 0.3945463299751282\n",
      "cnt: 0 - valLoss: 0.4076615273952484 - trainLoss: 0.3945416808128357\n",
      "cnt: 0 - valLoss: 0.4076595902442932 - trainLoss: 0.39453697204589844\n",
      "cnt: 0 - valLoss: 0.4076577425003052 - trainLoss: 0.39453229308128357\n",
      "cnt: 0 - valLoss: 0.407656192779541 - trainLoss: 0.3945276439189911\n",
      "cnt: 0 - valLoss: 0.40765446424484253 - trainLoss: 0.3945229947566986\n",
      "cnt: 0 - valLoss: 0.407652348279953 - trainLoss: 0.39451831579208374\n",
      "cnt: 0 - valLoss: 0.4076499044895172 - trainLoss: 0.3945136070251465\n",
      "cnt: 0 - valLoss: 0.4076487123966217 - trainLoss: 0.3945089876651764\n",
      "cnt: 0 - valLoss: 0.4076463282108307 - trainLoss: 0.3945043087005615\n",
      "cnt: 0 - valLoss: 0.407644659280777 - trainLoss: 0.39449965953826904\n",
      "cnt: 0 - valLoss: 0.40764319896698 - trainLoss: 0.3944949507713318\n",
      "cnt: 0 - valLoss: 0.4076407253742218 - trainLoss: 0.3944903016090393\n",
      "cnt: 0 - valLoss: 0.40763983130455017 - trainLoss: 0.39448562264442444\n",
      "cnt: 0 - valLoss: 0.40763741731643677 - trainLoss: 0.39448097348213196\n",
      "cnt: 0 - valLoss: 0.4076351821422577 - trainLoss: 0.3944762945175171\n",
      "cnt: 0 - valLoss: 0.4076330363750458 - trainLoss: 0.3944716453552246\n",
      "cnt: 0 - valLoss: 0.407631516456604 - trainLoss: 0.39446699619293213\n",
      "cnt: 0 - valLoss: 0.4076302647590637 - trainLoss: 0.39446231722831726\n",
      "cnt: 0 - valLoss: 0.4076274633407593 - trainLoss: 0.39445760846138\n",
      "cnt: 0 - valLoss: 0.4076263904571533 - trainLoss: 0.3944529891014099\n",
      "cnt: 0 - valLoss: 0.40762466192245483 - trainLoss: 0.39444833993911743\n",
      "cnt: 0 - valLoss: 0.40762248635292053 - trainLoss: 0.39444366097450256\n",
      "cnt: 0 - valLoss: 0.407620906829834 - trainLoss: 0.3944390118122101\n",
      "cnt: 0 - valLoss: 0.4076189398765564 - trainLoss: 0.3944344222545624\n",
      "cnt: 0 - valLoss: 0.4076171815395355 - trainLoss: 0.3944297134876251\n",
      "cnt: 0 - valLoss: 0.4076148271560669 - trainLoss: 0.39442506432533264\n",
      "cnt: 0 - valLoss: 0.40761351585388184 - trainLoss: 0.3944203853607178\n",
      "cnt: 0 - valLoss: 0.4076118767261505 - trainLoss: 0.3944157660007477\n",
      "cnt: 0 - valLoss: 0.40760910511016846 - trainLoss: 0.3944111168384552\n",
      "cnt: 0 - valLoss: 0.4076084494590759 - trainLoss: 0.3944064974784851\n",
      "cnt: 0 - valLoss: 0.40760618448257446 - trainLoss: 0.39440181851387024\n",
      "cnt: 0 - valLoss: 0.40760454535484314 - trainLoss: 0.39439713954925537\n",
      "cnt: 0 - valLoss: 0.4076020419597626 - trainLoss: 0.3943925201892853\n",
      "cnt: 0 - valLoss: 0.4076003432273865 - trainLoss: 0.3943879008293152\n",
      "cnt: 0 - valLoss: 0.40759938955307007 - trainLoss: 0.3943832218647003\n",
      "cnt: 0 - valLoss: 0.4075968563556671 - trainLoss: 0.3943786025047302\n",
      "cnt: 0 - valLoss: 0.4075959026813507 - trainLoss: 0.39437395334243774\n",
      "cnt: 0 - valLoss: 0.40759381651878357 - trainLoss: 0.39436930418014526\n",
      "cnt: 0 - valLoss: 0.4075915515422821 - trainLoss: 0.39436468482017517\n",
      "cnt: 0 - valLoss: 0.40759027004241943 - trainLoss: 0.3943600356578827\n",
      "cnt: 0 - valLoss: 0.40758779644966125 - trainLoss: 0.3943554162979126\n",
      "cnt: 0 - valLoss: 0.4075866639614105 - trainLoss: 0.39435073733329773\n",
      "cnt: 0 - valLoss: 0.40758422017097473 - trainLoss: 0.39434611797332764\n",
      "cnt: 0 - valLoss: 0.40758273005485535 - trainLoss: 0.39434149861335754\n",
      "cnt: 0 - valLoss: 0.4075811207294464 - trainLoss: 0.3943368196487427\n",
      "cnt: 0 - valLoss: 0.40757977962493896 - trainLoss: 0.3943322002887726\n",
      "cnt: 0 - valLoss: 0.4075777232646942 - trainLoss: 0.3943276107311249\n",
      "cnt: 0 - valLoss: 0.40757670998573303 - trainLoss: 0.3943229615688324\n",
      "cnt: 0 - valLoss: 0.4075748920440674 - trainLoss: 0.3943183124065399\n",
      "cnt: 0 - valLoss: 0.40757256746292114 - trainLoss: 0.3943136930465698\n",
      "cnt: 0 - valLoss: 0.40757110714912415 - trainLoss: 0.39430901408195496\n",
      "cnt: 0 - valLoss: 0.407569020986557 - trainLoss: 0.39430442452430725\n",
      "cnt: 0 - valLoss: 0.4075673818588257 - trainLoss: 0.3942997455596924\n",
      "cnt: 0 - valLoss: 0.4075656831264496 - trainLoss: 0.3942951261997223\n",
      "cnt: 0 - valLoss: 0.4075641632080078 - trainLoss: 0.3942905068397522\n",
      "cnt: 0 - valLoss: 0.40756210684776306 - trainLoss: 0.3942859172821045\n",
      "cnt: 0 - valLoss: 0.4075605273246765 - trainLoss: 0.394281268119812\n",
      "cnt: 0 - valLoss: 0.40755903720855713 - trainLoss: 0.3942766487598419\n",
      "cnt: 0 - valLoss: 0.4075571596622467 - trainLoss: 0.39427199959754944\n",
      "cnt: 0 - valLoss: 0.4075550436973572 - trainLoss: 0.39426741003990173\n",
      "cnt: 0 - valLoss: 0.40755394101142883 - trainLoss: 0.39426276087760925\n",
      "cnt: 0 - valLoss: 0.40755215287208557 - trainLoss: 0.39425814151763916\n",
      "cnt: 0 - valLoss: 0.4075508117675781 - trainLoss: 0.39425352215766907\n",
      "cnt: 0 - valLoss: 0.40754905343055725 - trainLoss: 0.394248902797699\n",
      "cnt: 0 - valLoss: 0.4075466990470886 - trainLoss: 0.3942442834377289\n",
      "cnt: 0 - valLoss: 0.4075466990470886 - trainLoss: 0.39423972368240356\n",
      "cnt: 0 - valLoss: 0.40754470229148865 - trainLoss: 0.39423513412475586\n",
      "cnt: 0 - valLoss: 0.40754279494285583 - trainLoss: 0.3942304849624634\n",
      "cnt: 0 - valLoss: 0.4075402021408081 - trainLoss: 0.3942258656024933\n",
      "cnt: 0 - valLoss: 0.407539427280426 - trainLoss: 0.3942212760448456\n",
      "cnt: 0 - valLoss: 0.40753814578056335 - trainLoss: 0.3942166864871979\n",
      "cnt: 0 - valLoss: 0.4075359106063843 - trainLoss: 0.39421209692955017\n",
      "cnt: 0 - valLoss: 0.4075353741645813 - trainLoss: 0.3942074775695801\n",
      "cnt: 0 - valLoss: 0.4075331687927246 - trainLoss: 0.39420285820961\n",
      "cnt: 0 - valLoss: 0.4075315594673157 - trainLoss: 0.3941982686519623\n",
      "cnt: 0 - valLoss: 0.4075298607349396 - trainLoss: 0.3941936492919922\n",
      "cnt: 0 - valLoss: 0.40752822160720825 - trainLoss: 0.39418908953666687\n",
      "cnt: 0 - valLoss: 0.40752658247947693 - trainLoss: 0.39418449997901917\n",
      "cnt: 0 - valLoss: 0.4075252115726471 - trainLoss: 0.3941798508167267\n",
      "cnt: 0 - valLoss: 0.40752339363098145 - trainLoss: 0.39417529106140137\n",
      "cnt: 0 - valLoss: 0.4075222611427307 - trainLoss: 0.3941706717014313\n",
      "cnt: 0 - valLoss: 0.40752044320106506 - trainLoss: 0.39416611194610596\n",
      "cnt: 0 - valLoss: 0.40751853585243225 - trainLoss: 0.39416149258613586\n",
      "cnt: 0 - valLoss: 0.4075184166431427 - trainLoss: 0.39415693283081055\n",
      "cnt: 0 - valLoss: 0.40751612186431885 - trainLoss: 0.39415231347084045\n",
      "cnt: 0 - valLoss: 0.4075143039226532 - trainLoss: 0.39414772391319275\n",
      "cnt: 0 - valLoss: 0.4075123369693756 - trainLoss: 0.39414316415786743\n",
      "cnt: 0 - valLoss: 0.40751123428344727 - trainLoss: 0.3941385746002197\n",
      "cnt: 0 - valLoss: 0.4075098931789398 - trainLoss: 0.39413395524024963\n",
      "cnt: 0 - valLoss: 0.4075081944465637 - trainLoss: 0.3941293954849243\n",
      "cnt: 0 - valLoss: 0.4075060486793518 - trainLoss: 0.39412474632263184\n",
      "cnt: 0 - valLoss: 0.40750473737716675 - trainLoss: 0.39412012696266174\n",
      "cnt: 0 - valLoss: 0.4075050354003906 - trainLoss: 0.39411550760269165\n",
      "cnt: 1 - valLoss: 0.40750226378440857 - trainLoss: 0.39411091804504395\n",
      "cnt: 0 - valLoss: 0.4075009226799011 - trainLoss: 0.39410626888275146\n",
      "cnt: 0 - valLoss: 0.40749913454055786 - trainLoss: 0.39410164952278137\n",
      "cnt: 0 - valLoss: 0.40749746561050415 - trainLoss: 0.3940970301628113\n",
      "cnt: 0 - valLoss: 0.4074976146221161 - trainLoss: 0.39409247040748596\n",
      "cnt: 1 - valLoss: 0.4074951112270355 - trainLoss: 0.39408785104751587\n",
      "cnt: 0 - valLoss: 0.4074934422969818 - trainLoss: 0.39408326148986816\n",
      "cnt: 0 - valLoss: 0.4074918329715729 - trainLoss: 0.3940785825252533\n",
      "cnt: 0 - valLoss: 0.40749117732048035 - trainLoss: 0.394074022769928\n",
      "cnt: 0 - valLoss: 0.40748855471611023 - trainLoss: 0.3940694332122803\n",
      "cnt: 0 - valLoss: 0.4074879586696625 - trainLoss: 0.3940647840499878\n",
      "cnt: 0 - valLoss: 0.4074862003326416 - trainLoss: 0.3940601944923401\n",
      "cnt: 0 - valLoss: 0.40748468041419983 - trainLoss: 0.39405557513237\n",
      "cnt: 0 - valLoss: 0.4074830114841461 - trainLoss: 0.3940509855747223\n",
      "cnt: 0 - valLoss: 0.4074825942516327 - trainLoss: 0.3940463960170746\n",
      "cnt: 0 - valLoss: 0.4074811339378357 - trainLoss: 0.3940418064594269\n",
      "cnt: 0 - valLoss: 0.4074784815311432 - trainLoss: 0.3940371870994568\n",
      "cnt: 0 - valLoss: 0.40747740864753723 - trainLoss: 0.3940325677394867\n",
      "cnt: 0 - valLoss: 0.40747568011283875 - trainLoss: 0.3940280079841614\n",
      "cnt: 0 - valLoss: 0.40747517347335815 - trainLoss: 0.3940233886241913\n",
      "cnt: 0 - valLoss: 0.4074728786945343 - trainLoss: 0.3940187692642212\n",
      "cnt: 0 - valLoss: 0.4074716567993164 - trainLoss: 0.3940142095088959\n",
      "cnt: 0 - valLoss: 0.4074704051017761 - trainLoss: 0.3940095901489258\n",
      "cnt: 0 - valLoss: 0.40746915340423584 - trainLoss: 0.3940049409866333\n",
      "cnt: 0 - valLoss: 0.40746745467185974 - trainLoss: 0.39400026202201843\n",
      "cnt: 0 - valLoss: 0.40746620297431946 - trainLoss: 0.39399564266204834\n",
      "cnt: 0 - valLoss: 0.4074653685092926 - trainLoss: 0.39399102330207825\n",
      "cnt: 0 - valLoss: 0.40746384859085083 - trainLoss: 0.39398637413978577\n",
      "cnt: 0 - valLoss: 0.40746158361434937 - trainLoss: 0.3939817249774933\n",
      "cnt: 0 - valLoss: 0.4074612259864807 - trainLoss: 0.3939771056175232\n",
      "cnt: 0 - valLoss: 0.4074594974517822 - trainLoss: 0.3939724564552307\n",
      "cnt: 0 - valLoss: 0.4074585735797882 - trainLoss: 0.39396780729293823\n",
      "cnt: 0 - valLoss: 0.4074571132659912 - trainLoss: 0.39396318793296814\n",
      "cnt: 0 - valLoss: 0.40745508670806885 - trainLoss: 0.39395853877067566\n",
      "cnt: 0 - valLoss: 0.4074547290802002 - trainLoss: 0.3939538896083832\n",
      "cnt: 0 - valLoss: 0.40745288133621216 - trainLoss: 0.3939492702484131\n",
      "cnt: 0 - valLoss: 0.4074512720108032 - trainLoss: 0.3939446210861206\n",
      "cnt: 0 - valLoss: 0.4074506163597107 - trainLoss: 0.3939400315284729\n",
      "cnt: 0 - valLoss: 0.4074486196041107 - trainLoss: 0.3939353823661804\n",
      "cnt: 0 - valLoss: 0.40744781494140625 - trainLoss: 0.39393073320388794\n",
      "cnt: 0 - valLoss: 0.40744611620903015 - trainLoss: 0.39392611384391785\n",
      "cnt: 0 - valLoss: 0.40744471549987793 - trainLoss: 0.39392146468162537\n",
      "cnt: 0 - valLoss: 0.4074432849884033 - trainLoss: 0.39391687512397766\n",
      "cnt: 0 - valLoss: 0.40744325518608093 - trainLoss: 0.3939122259616852\n",
      "cnt: 0 - valLoss: 0.40744149684906006 - trainLoss: 0.3939076066017151\n",
      "cnt: 0 - valLoss: 0.407439649105072 - trainLoss: 0.393902987241745\n",
      "cnt: 0 - valLoss: 0.40743786096572876 - trainLoss: 0.3938983380794525\n",
      "cnt: 0 - valLoss: 0.4074369966983795 - trainLoss: 0.39389368891716003\n",
      "cnt: 0 - valLoss: 0.40743541717529297 - trainLoss: 0.39388909935951233\n",
      "cnt: 0 - valLoss: 0.407434344291687 - trainLoss: 0.39388447999954224\n",
      "cnt: 0 - valLoss: 0.40743353962898254 - trainLoss: 0.39387986063957214\n",
      "cnt: 0 - valLoss: 0.4074322283267975 - trainLoss: 0.3938753306865692\n",
      "cnt: 0 - valLoss: 0.4074307084083557 - trainLoss: 0.39387065172195435\n",
      "cnt: 0 - valLoss: 0.4074289798736572 - trainLoss: 0.39386603236198425\n",
      "cnt: 0 - valLoss: 0.4074271619319916 - trainLoss: 0.39386144280433655\n",
      "cnt: 0 - valLoss: 0.40742722153663635 - trainLoss: 0.39385679364204407\n",
      "cnt: 1 - valLoss: 0.4074248671531677 - trainLoss: 0.39385223388671875\n",
      "cnt: 0 - valLoss: 0.4074241816997528 - trainLoss: 0.39384761452674866\n",
      "cnt: 0 - valLoss: 0.4074220359325409 - trainLoss: 0.39384302496910095\n",
      "cnt: 0 - valLoss: 0.4074217975139618 - trainLoss: 0.39383840560913086\n",
      "cnt: 0 - valLoss: 0.40741971135139465 - trainLoss: 0.39383384585380554\n",
      "cnt: 0 - valLoss: 0.407418817281723 - trainLoss: 0.39382925629615784\n",
      "cnt: 0 - valLoss: 0.407417356967926 - trainLoss: 0.39382463693618774\n",
      "cnt: 0 - valLoss: 0.40741589665412903 - trainLoss: 0.39382001757621765\n",
      "cnt: 0 - valLoss: 0.40741458535194397 - trainLoss: 0.39381542801856995\n",
      "cnt: 0 - valLoss: 0.4074134826660156 - trainLoss: 0.39381083846092224\n",
      "cnt: 0 - valLoss: 0.4074123203754425 - trainLoss: 0.39380624890327454\n",
      "cnt: 0 - valLoss: 0.4074113070964813 - trainLoss: 0.39380165934562683\n",
      "cnt: 0 - valLoss: 0.4074091613292694 - trainLoss: 0.3937970697879791\n",
      "cnt: 0 - valLoss: 0.40740907192230225 - trainLoss: 0.3937924802303314\n",
      "cnt: 0 - valLoss: 0.40740785002708435 - trainLoss: 0.3937879204750061\n",
      "cnt: 0 - valLoss: 0.4074059724807739 - trainLoss: 0.3937833309173584\n",
      "cnt: 0 - valLoss: 0.4074050486087799 - trainLoss: 0.3937787115573883\n",
      "cnt: 0 - valLoss: 0.4074041247367859 - trainLoss: 0.3937741219997406\n",
      "cnt: 0 - valLoss: 0.4074031412601471 - trainLoss: 0.3937695026397705\n",
      "cnt: 0 - valLoss: 0.40740126371383667 - trainLoss: 0.3937649726867676\n",
      "cnt: 0 - valLoss: 0.407400906085968 - trainLoss: 0.3937603533267975\n",
      "cnt: 0 - valLoss: 0.40739965438842773 - trainLoss: 0.3937557637691498\n",
      "cnt: 0 - valLoss: 0.4073982238769531 - trainLoss: 0.39375120401382446\n",
      "cnt: 0 - valLoss: 0.4073980152606964 - trainLoss: 0.39374661445617676\n",
      "cnt: 0 - valLoss: 0.40739601850509644 - trainLoss: 0.39374208450317383\n",
      "cnt: 0 - valLoss: 0.4073951840400696 - trainLoss: 0.39373743534088135\n",
      "cnt: 0 - valLoss: 0.40739384293556213 - trainLoss: 0.39373284578323364\n",
      "cnt: 0 - valLoss: 0.4073927402496338 - trainLoss: 0.39372822642326355\n",
      "cnt: 0 - valLoss: 0.40739181637763977 - trainLoss: 0.39372363686561584\n",
      "cnt: 0 - valLoss: 0.4073905050754547 - trainLoss: 0.3937190771102905\n",
      "cnt: 0 - valLoss: 0.40738892555236816 - trainLoss: 0.3937144875526428\n",
      "cnt: 0 - valLoss: 0.4073888063430786 - trainLoss: 0.3937099277973175\n",
      "cnt: 0 - valLoss: 0.407387375831604 - trainLoss: 0.3937053084373474\n",
      "cnt: 0 - valLoss: 0.40738561749458313 - trainLoss: 0.3937007188796997\n",
      "cnt: 0 - valLoss: 0.4073845148086548 - trainLoss: 0.3936961591243744\n",
      "cnt: 0 - valLoss: 0.40738314390182495 - trainLoss: 0.3936915695667267\n",
      "cnt: 0 - valLoss: 0.4073828458786011 - trainLoss: 0.39368706941604614\n",
      "cnt: 0 - valLoss: 0.40738096833229065 - trainLoss: 0.3936825394630432\n",
      "cnt: 0 - valLoss: 0.4073801040649414 - trainLoss: 0.39367803931236267\n",
      "cnt: 0 - valLoss: 0.40737852454185486 - trainLoss: 0.39367350935935974\n",
      "cnt: 0 - valLoss: 0.40737783908843994 - trainLoss: 0.39366891980171204\n",
      "cnt: 0 - valLoss: 0.4073760211467743 - trainLoss: 0.3936644196510315\n",
      "cnt: 0 - valLoss: 0.40737590193748474 - trainLoss: 0.39365988969802856\n",
      "cnt: 0 - valLoss: 0.40737393498420715 - trainLoss: 0.393655389547348\n",
      "cnt: 0 - valLoss: 0.40737277269363403 - trainLoss: 0.3936508297920227\n",
      "cnt: 0 - valLoss: 0.40737155079841614 - trainLoss: 0.3936462998390198\n",
      "cnt: 0 - valLoss: 0.40737032890319824 - trainLoss: 0.39364171028137207\n",
      "cnt: 0 - valLoss: 0.40736913681030273 - trainLoss: 0.39363712072372437\n",
      "cnt: 0 - valLoss: 0.40736836194992065 - trainLoss: 0.39363256096839905\n",
      "cnt: 0 - valLoss: 0.40736642479896545 - trainLoss: 0.39362800121307373\n",
      "cnt: 0 - valLoss: 0.4073655903339386 - trainLoss: 0.393623411655426\n",
      "cnt: 0 - valLoss: 0.4073648154735565 - trainLoss: 0.3936188519001007\n",
      "cnt: 0 - valLoss: 0.40736308693885803 - trainLoss: 0.3936142921447754\n",
      "cnt: 0 - valLoss: 0.4073626399040222 - trainLoss: 0.3936097323894501\n",
      "cnt: 0 - valLoss: 0.4073609411716461 - trainLoss: 0.39360517263412476\n",
      "cnt: 0 - valLoss: 0.40735915303230286 - trainLoss: 0.39360058307647705\n",
      "cnt: 0 - valLoss: 0.407358855009079 - trainLoss: 0.39359602332115173\n",
      "cnt: 0 - valLoss: 0.40735721588134766 - trainLoss: 0.3935914635658264\n",
      "cnt: 0 - valLoss: 0.4073562026023865 - trainLoss: 0.3935869634151459\n",
      "cnt: 0 - valLoss: 0.40735509991645813 - trainLoss: 0.39358243346214294\n",
      "cnt: 0 - valLoss: 0.4073537290096283 - trainLoss: 0.3935779333114624\n",
      "cnt: 0 - valLoss: 0.4073537290096283 - trainLoss: 0.3935734033584595\n",
      "cnt: 0 - valLoss: 0.4073524475097656 - trainLoss: 0.3935689330101013\n",
      "cnt: 0 - valLoss: 0.40735018253326416 - trainLoss: 0.393564373254776\n",
      "cnt: 0 - valLoss: 0.40734967589378357 - trainLoss: 0.39355987310409546\n",
      "cnt: 0 - valLoss: 0.4073482155799866 - trainLoss: 0.3935553729534149\n",
      "cnt: 0 - valLoss: 0.4073473811149597 - trainLoss: 0.3935508728027344\n",
      "cnt: 0 - valLoss: 0.4073461592197418 - trainLoss: 0.39354637265205383\n",
      "cnt: 0 - valLoss: 0.40734508633613586 - trainLoss: 0.3935418128967285\n",
      "cnt: 0 - valLoss: 0.40734410285949707 - trainLoss: 0.393537312746048\n",
      "cnt: 0 - valLoss: 0.407342791557312 - trainLoss: 0.39353281259536743\n",
      "cnt: 0 - valLoss: 0.4073418378829956 - trainLoss: 0.3935282528400421\n",
      "cnt: 0 - valLoss: 0.40734055638313293 - trainLoss: 0.3935237228870392\n",
      "cnt: 0 - valLoss: 0.4073391258716583 - trainLoss: 0.39351922273635864\n",
      "cnt: 0 - valLoss: 0.4073371887207031 - trainLoss: 0.3935146927833557\n",
      "cnt: 0 - valLoss: 0.40733635425567627 - trainLoss: 0.39351019263267517\n",
      "cnt: 0 - valLoss: 0.4073343276977539 - trainLoss: 0.393505722284317\n",
      "cnt: 0 - valLoss: 0.407332181930542 - trainLoss: 0.3935012221336365\n",
      "cnt: 0 - valLoss: 0.4073302149772644 - trainLoss: 0.39349669218063354\n",
      "cnt: 0 - valLoss: 0.40732958912849426 - trainLoss: 0.3934922218322754\n",
      "cnt: 0 - valLoss: 0.40732714533805847 - trainLoss: 0.39348769187927246\n",
      "cnt: 0 - valLoss: 0.40732675790786743 - trainLoss: 0.3934831917285919\n",
      "cnt: 0 - valLoss: 0.4073236882686615 - trainLoss: 0.39347872138023376\n",
      "cnt: 0 - valLoss: 0.40732327103614807 - trainLoss: 0.3934742212295532\n",
      "cnt: 0 - valLoss: 0.4073212444782257 - trainLoss: 0.39346975088119507\n",
      "cnt: 0 - valLoss: 0.40731948614120483 - trainLoss: 0.39346519112586975\n",
      "cnt: 0 - valLoss: 0.4073179364204407 - trainLoss: 0.3934606909751892\n",
      "cnt: 0 - valLoss: 0.40731629729270935 - trainLoss: 0.3934561610221863\n",
      "cnt: 0 - valLoss: 0.40731438994407654 - trainLoss: 0.39345163106918335\n",
      "cnt: 0 - valLoss: 0.40731361508369446 - trainLoss: 0.3934471905231476\n",
      "cnt: 0 - valLoss: 0.40731093287467957 - trainLoss: 0.39344263076782227\n",
      "cnt: 0 - valLoss: 0.4073098301887512 - trainLoss: 0.3934381306171417\n",
      "cnt: 0 - valLoss: 0.4073089361190796 - trainLoss: 0.3934336304664612\n",
      "cnt: 0 - valLoss: 0.4073070287704468 - trainLoss: 0.39342913031578064\n",
      "cnt: 0 - valLoss: 0.40730521082878113 - trainLoss: 0.3934246301651001\n",
      "cnt: 0 - valLoss: 0.4073037803173065 - trainLoss: 0.39342013001441956\n",
      "cnt: 0 - valLoss: 0.40730154514312744 - trainLoss: 0.393415629863739\n",
      "cnt: 0 - valLoss: 0.40730130672454834 - trainLoss: 0.39341115951538086\n",
      "cnt: 0 - valLoss: 0.4072989821434021 - trainLoss: 0.39340662956237793\n",
      "cnt: 0 - valLoss: 0.40729743242263794 - trainLoss: 0.3934021592140198\n",
      "cnt: 0 - valLoss: 0.4072957932949066 - trainLoss: 0.39339765906333923\n",
      "cnt: 0 - valLoss: 0.4072940945625305 - trainLoss: 0.3933931589126587\n",
      "cnt: 0 - valLoss: 0.4072933793067932 - trainLoss: 0.39338868856430054\n",
      "cnt: 0 - valLoss: 0.4072910249233246 - trainLoss: 0.39338418841362\n",
      "cnt: 0 - valLoss: 0.40728941559791565 - trainLoss: 0.39337974786758423\n",
      "cnt: 0 - valLoss: 0.40728822350502014 - trainLoss: 0.3933752477169037\n",
      "cnt: 0 - valLoss: 0.4072866141796112 - trainLoss: 0.39337074756622314\n",
      "cnt: 0 - valLoss: 0.4072854816913605 - trainLoss: 0.39336633682250977\n",
      "cnt: 0 - valLoss: 0.4072835147380829 - trainLoss: 0.3933618366718292\n",
      "cnt: 0 - valLoss: 0.4072822332382202 - trainLoss: 0.3933573365211487\n",
      "cnt: 0 - valLoss: 0.4072813391685486 - trainLoss: 0.3933528959751129\n",
      "cnt: 0 - valLoss: 0.40727943181991577 - trainLoss: 0.39334842562675476\n",
      "cnt: 0 - valLoss: 0.40727782249450684 - trainLoss: 0.3933439552783966\n",
      "cnt: 0 - valLoss: 0.40727588534355164 - trainLoss: 0.39333948493003845\n",
      "cnt: 0 - valLoss: 0.4072745740413666 - trainLoss: 0.3933350145816803\n",
      "cnt: 0 - valLoss: 0.4072737693786621 - trainLoss: 0.3933306038379669\n",
      "cnt: 0 - valLoss: 0.40727174282073975 - trainLoss: 0.39332613348960876\n",
      "cnt: 0 - valLoss: 0.4072701334953308 - trainLoss: 0.3933216631412506\n",
      "cnt: 0 - valLoss: 0.4072681963443756 - trainLoss: 0.39331719279289246\n",
      "cnt: 0 - valLoss: 0.40726661682128906 - trainLoss: 0.3933126926422119\n",
      "cnt: 0 - valLoss: 0.4072664976119995 - trainLoss: 0.39330828189849854\n",
      "cnt: 0 - valLoss: 0.4072642922401428 - trainLoss: 0.39330384135246277\n",
      "cnt: 0 - valLoss: 0.4072626233100891 - trainLoss: 0.3932993710041046\n",
      "cnt: 0 - valLoss: 0.4072612226009369 - trainLoss: 0.39329493045806885\n",
      "cnt: 0 - valLoss: 0.40726014971733093 - trainLoss: 0.3932904601097107\n",
      "cnt: 0 - valLoss: 0.4072583317756653 - trainLoss: 0.3932860493659973\n",
      "cnt: 0 - valLoss: 0.4072567820549011 - trainLoss: 0.39328157901763916\n",
      "cnt: 0 - valLoss: 0.40725529193878174 - trainLoss: 0.393277108669281\n",
      "cnt: 0 - valLoss: 0.40725356340408325 - trainLoss: 0.3932726979255676\n",
      "cnt: 0 - valLoss: 0.4072527289390564 - trainLoss: 0.3932682275772095\n",
      "cnt: 0 - valLoss: 0.4072505831718445 - trainLoss: 0.3932637870311737\n",
      "cnt: 0 - valLoss: 0.4072493612766266 - trainLoss: 0.39325934648513794\n",
      "cnt: 0 - valLoss: 0.4072480797767639 - trainLoss: 0.3932549059391022\n",
      "cnt: 0 - valLoss: 0.4072459042072296 - trainLoss: 0.393250435590744\n",
      "cnt: 0 - valLoss: 0.40724509954452515 - trainLoss: 0.39324599504470825\n",
      "cnt: 0 - valLoss: 0.407243937253952 - trainLoss: 0.3932415544986725\n",
      "cnt: 0 - valLoss: 0.4072420597076416 - trainLoss: 0.39323708415031433\n",
      "cnt: 0 - valLoss: 0.40724077820777893 - trainLoss: 0.39323264360427856\n",
      "cnt: 0 - valLoss: 0.4072394073009491 - trainLoss: 0.3932282030582428\n",
      "cnt: 0 - valLoss: 0.40723916888237 - trainLoss: 0.39322373270988464\n",
      "cnt: 0 - valLoss: 0.40723732113838196 - trainLoss: 0.39321932196617126\n",
      "cnt: 0 - valLoss: 0.4072355329990387 - trainLoss: 0.3932148516178131\n",
      "cnt: 0 - valLoss: 0.40723398327827454 - trainLoss: 0.3932104706764221\n",
      "cnt: 0 - valLoss: 0.4072330594062805 - trainLoss: 0.39320600032806396\n",
      "cnt: 0 - valLoss: 0.40723103284835815 - trainLoss: 0.3932015895843506\n",
      "cnt: 0 - valLoss: 0.4072303771972656 - trainLoss: 0.39319711923599243\n",
      "cnt: 0 - valLoss: 0.4072287380695343 - trainLoss: 0.39319267868995667\n",
      "cnt: 0 - valLoss: 0.4072269797325134 - trainLoss: 0.3931882679462433\n",
      "cnt: 0 - valLoss: 0.40722644329071045 - trainLoss: 0.3931838274002075\n",
      "cnt: 0 - valLoss: 0.4072245955467224 - trainLoss: 0.39317941665649414\n",
      "cnt: 0 - valLoss: 0.4072234630584717 - trainLoss: 0.39317500591278076\n",
      "cnt: 0 - valLoss: 0.4072226285934448 - trainLoss: 0.393170565366745\n",
      "cnt: 0 - valLoss: 0.4072205722332001 - trainLoss: 0.393166184425354\n",
      "cnt: 0 - valLoss: 0.4072195887565613 - trainLoss: 0.39316174387931824\n",
      "cnt: 0 - valLoss: 0.40721821784973145 - trainLoss: 0.39315730333328247\n",
      "cnt: 0 - valLoss: 0.4072161912918091 - trainLoss: 0.3931529223918915\n",
      "cnt: 0 - valLoss: 0.4072157144546509 - trainLoss: 0.3931484818458557\n",
      "cnt: 0 - valLoss: 0.40721437335014343 - trainLoss: 0.3931441307067871\n",
      "cnt: 0 - valLoss: 0.40721291303634644 - trainLoss: 0.39313966035842896\n",
      "cnt: 0 - valLoss: 0.40721166133880615 - trainLoss: 0.3931352496147156\n",
      "cnt: 0 - valLoss: 0.40720996260643005 - trainLoss: 0.3931308686733246\n",
      "cnt: 0 - valLoss: 0.4072086811065674 - trainLoss: 0.3931264281272888\n",
      "cnt: 0 - valLoss: 0.4072076082229614 - trainLoss: 0.3931220471858978\n",
      "cnt: 0 - valLoss: 0.40720605850219727 - trainLoss: 0.3931175768375397\n",
      "cnt: 0 - valLoss: 0.4072054922580719 - trainLoss: 0.3931131958961487\n",
      "cnt: 0 - valLoss: 0.40720313787460327 - trainLoss: 0.3931087851524353\n",
      "cnt: 0 - valLoss: 0.4072020351886749 - trainLoss: 0.39310431480407715\n",
      "cnt: 0 - valLoss: 0.4072018265724182 - trainLoss: 0.39309990406036377\n",
      "cnt: 0 - valLoss: 0.40720006823539734 - trainLoss: 0.3930954933166504\n",
      "cnt: 0 - valLoss: 0.4071982502937317 - trainLoss: 0.393091082572937\n",
      "cnt: 0 - valLoss: 0.40719717741012573 - trainLoss: 0.39308667182922363\n",
      "cnt: 0 - valLoss: 0.40719568729400635 - trainLoss: 0.39308226108551025\n",
      "cnt: 0 - valLoss: 0.4071940779685974 - trainLoss: 0.3930777907371521\n",
      "cnt: 0 - valLoss: 0.4071940779685974 - trainLoss: 0.3930734097957611\n",
      "cnt: 0 - valLoss: 0.40719205141067505 - trainLoss: 0.3930690288543701\n",
      "cnt: 0 - valLoss: 0.4071906805038452 - trainLoss: 0.39306455850601196\n",
      "cnt: 0 - valLoss: 0.4071892201900482 - trainLoss: 0.39306017756462097\n",
      "cnt: 0 - valLoss: 0.40718820691108704 - trainLoss: 0.3930557668209076\n",
      "cnt: 0 - valLoss: 0.4071873426437378 - trainLoss: 0.3930513560771942\n",
      "cnt: 0 - valLoss: 0.4071854054927826 - trainLoss: 0.39304694533348083\n",
      "cnt: 0 - valLoss: 0.4071846008300781 - trainLoss: 0.39304253458976746\n",
      "cnt: 0 - valLoss: 0.4071830213069916 - trainLoss: 0.3930381238460541\n",
      "cnt: 0 - valLoss: 0.40718135237693787 - trainLoss: 0.3930336833000183\n",
      "cnt: 0 - valLoss: 0.4071810245513916 - trainLoss: 0.39302927255630493\n",
      "cnt: 0 - valLoss: 0.40717893838882446 - trainLoss: 0.39302489161491394\n",
      "cnt: 0 - valLoss: 0.4071781635284424 - trainLoss: 0.3930204212665558\n",
      "cnt: 0 - valLoss: 0.40717607736587524 - trainLoss: 0.3930160105228424\n",
      "cnt: 0 - valLoss: 0.40717485547065735 - trainLoss: 0.39301156997680664\n",
      "cnt: 0 - valLoss: 0.4071735143661499 - trainLoss: 0.39300718903541565\n",
      "cnt: 0 - valLoss: 0.4071718156337738 - trainLoss: 0.3930027484893799\n",
      "cnt: 0 - valLoss: 0.40717193484306335 - trainLoss: 0.3929983675479889\n",
      "cnt: 1 - valLoss: 0.4071696996688843 - trainLoss: 0.3929939568042755\n",
      "cnt: 0 - valLoss: 0.4071686565876007 - trainLoss: 0.39298945665359497\n",
      "cnt: 0 - valLoss: 0.40716683864593506 - trainLoss: 0.3929850161075592\n",
      "cnt: 0 - valLoss: 0.4071652889251709 - trainLoss: 0.39298057556152344\n",
      "cnt: 0 - valLoss: 0.40716442465782166 - trainLoss: 0.39297613501548767\n",
      "cnt: 0 - valLoss: 0.4071630835533142 - trainLoss: 0.3929717540740967\n",
      "cnt: 0 - valLoss: 0.4071616530418396 - trainLoss: 0.3929672837257385\n",
      "cnt: 0 - valLoss: 0.4071601331233978 - trainLoss: 0.39296284317970276\n",
      "cnt: 0 - valLoss: 0.40715885162353516 - trainLoss: 0.3929584324359894\n",
      "cnt: 0 - valLoss: 0.40715688467025757 - trainLoss: 0.3929539918899536\n",
      "cnt: 0 - valLoss: 0.4071560204029083 - trainLoss: 0.39294952154159546\n",
      "cnt: 0 - valLoss: 0.4071558713912964 - trainLoss: 0.3929451107978821\n",
      "cnt: 0 - valLoss: 0.40715357661247253 - trainLoss: 0.3929407298564911\n",
      "cnt: 0 - valLoss: 0.40715211629867554 - trainLoss: 0.39293625950813293\n",
      "cnt: 0 - valLoss: 0.40715089440345764 - trainLoss: 0.39293184876441956\n",
      "cnt: 0 - valLoss: 0.4071495234966278 - trainLoss: 0.3929274082183838\n",
      "cnt: 0 - valLoss: 0.40714865922927856 - trainLoss: 0.3929229974746704\n",
      "cnt: 0 - valLoss: 0.40714743733406067 - trainLoss: 0.39291852712631226\n",
      "cnt: 0 - valLoss: 0.4071456789970398 - trainLoss: 0.3929141163825989\n",
      "cnt: 0 - valLoss: 0.40714478492736816 - trainLoss: 0.3929097056388855\n",
      "cnt: 0 - valLoss: 0.4071432650089264 - trainLoss: 0.3929052948951721\n",
      "cnt: 0 - valLoss: 0.40714171528816223 - trainLoss: 0.39290085434913635\n",
      "cnt: 0 - valLoss: 0.4071417450904846 - trainLoss: 0.392896443605423\n",
      "cnt: 1 - valLoss: 0.4071398675441742 - trainLoss: 0.3928920030593872\n",
      "cnt: 0 - valLoss: 0.4071381390094757 - trainLoss: 0.39288759231567383\n",
      "cnt: 0 - valLoss: 0.4071367681026459 - trainLoss: 0.39288315176963806\n",
      "cnt: 0 - valLoss: 0.4071359932422638 - trainLoss: 0.3928787410259247\n",
      "cnt: 0 - valLoss: 0.40713509917259216 - trainLoss: 0.3928743898868561\n",
      "cnt: 0 - valLoss: 0.4071332514286041 - trainLoss: 0.3928700089454651\n",
      "cnt: 0 - valLoss: 0.4071325659751892 - trainLoss: 0.3928655982017517\n",
      "cnt: 0 - valLoss: 0.40713149309158325 - trainLoss: 0.39286118745803833\n",
      "cnt: 0 - valLoss: 0.40713030099868774 - trainLoss: 0.39285680651664734\n",
      "cnt: 0 - valLoss: 0.4071289896965027 - trainLoss: 0.39285242557525635\n",
      "cnt: 0 - valLoss: 0.40712764859199524 - trainLoss: 0.39284804463386536\n",
      "cnt: 0 - valLoss: 0.4071269631385803 - trainLoss: 0.3928436040878296\n",
      "cnt: 0 - valLoss: 0.40712517499923706 - trainLoss: 0.3928392231464386\n",
      "cnt: 0 - valLoss: 0.4071248769760132 - trainLoss: 0.3928348422050476\n",
      "cnt: 0 - valLoss: 0.40712353587150574 - trainLoss: 0.392830491065979\n",
      "cnt: 0 - valLoss: 0.4071224331855774 - trainLoss: 0.3928260803222656\n",
      "cnt: 0 - valLoss: 0.407121866941452 - trainLoss: 0.392821729183197\n",
      "cnt: 0 - valLoss: 0.4071197807788849 - trainLoss: 0.39281728863716125\n",
      "cnt: 0 - valLoss: 0.4071195125579834 - trainLoss: 0.39281296730041504\n",
      "cnt: 0 - valLoss: 0.40711820125579834 - trainLoss: 0.39280855655670166\n",
      "cnt: 0 - valLoss: 0.4071168899536133 - trainLoss: 0.39280417561531067\n",
      "cnt: 0 - valLoss: 0.40711545944213867 - trainLoss: 0.39279982447624207\n",
      "cnt: 0 - valLoss: 0.4071142375469208 - trainLoss: 0.3927953839302063\n",
      "cnt: 0 - valLoss: 0.40711337327957153 - trainLoss: 0.3927910029888153\n",
      "cnt: 0 - valLoss: 0.4071120321750641 - trainLoss: 0.39278659224510193\n",
      "cnt: 0 - valLoss: 0.4071118235588074 - trainLoss: 0.39278218150138855\n",
      "cnt: 0 - valLoss: 0.40711039304733276 - trainLoss: 0.39277780055999756\n",
      "cnt: 0 - valLoss: 0.4071093201637268 - trainLoss: 0.3927733898162842\n",
      "cnt: 0 - valLoss: 0.40710797905921936 - trainLoss: 0.3927690088748932\n",
      "cnt: 0 - valLoss: 0.4071062505245209 - trainLoss: 0.3927645683288574\n",
      "cnt: 0 - valLoss: 0.40710610151290894 - trainLoss: 0.39276015758514404\n",
      "cnt: 0 - valLoss: 0.40710461139678955 - trainLoss: 0.3927558362483978\n",
      "cnt: 0 - valLoss: 0.4071042537689209 - trainLoss: 0.39275145530700684\n",
      "cnt: 0 - valLoss: 0.40710294246673584 - trainLoss: 0.39274710416793823\n",
      "cnt: 0 - valLoss: 0.40710124373435974 - trainLoss: 0.39274275302886963\n",
      "cnt: 0 - valLoss: 0.4071003496646881 - trainLoss: 0.3927384316921234\n",
      "cnt: 0 - valLoss: 0.4070993661880493 - trainLoss: 0.3927340507507324\n",
      "cnt: 0 - valLoss: 0.40709972381591797 - trainLoss: 0.3927296996116638\n",
      "cnt: 1 - valLoss: 0.40709739923477173 - trainLoss: 0.3927253782749176\n",
      "cnt: 0 - valLoss: 0.40709632635116577 - trainLoss: 0.3927209973335266\n",
      "cnt: 0 - valLoss: 0.4070950746536255 - trainLoss: 0.3927166759967804\n",
      "cnt: 0 - valLoss: 0.40709438920021057 - trainLoss: 0.3927122950553894\n",
      "cnt: 0 - valLoss: 0.4070931673049927 - trainLoss: 0.3927079737186432\n",
      "cnt: 0 - valLoss: 0.4070923328399658 - trainLoss: 0.3927035927772522\n",
      "cnt: 0 - valLoss: 0.40709149837493896 - trainLoss: 0.392699271440506\n",
      "cnt: 0 - valLoss: 0.40709033608436584 - trainLoss: 0.3926949203014374\n",
      "cnt: 0 - valLoss: 0.4070897102355957 - trainLoss: 0.39269059896469116\n",
      "cnt: 0 - valLoss: 0.407088041305542 - trainLoss: 0.39268624782562256\n",
      "cnt: 0 - valLoss: 0.4070874750614166 - trainLoss: 0.39268192648887634\n",
      "cnt: 0 - valLoss: 0.40708687901496887 - trainLoss: 0.39267754554748535\n",
      "cnt: 0 - valLoss: 0.40708598494529724 - trainLoss: 0.39267319440841675\n",
      "cnt: 0 - valLoss: 0.4070848822593689 - trainLoss: 0.39266884326934814\n",
      "cnt: 0 - valLoss: 0.40708327293395996 - trainLoss: 0.39266443252563477\n",
      "cnt: 0 - valLoss: 0.40708306431770325 - trainLoss: 0.39266008138656616\n",
      "cnt: 0 - valLoss: 0.4070812463760376 - trainLoss: 0.39265573024749756\n",
      "cnt: 0 - valLoss: 0.40707987546920776 - trainLoss: 0.3926512897014618\n",
      "cnt: 0 - valLoss: 0.4070795476436615 - trainLoss: 0.3926469385623932\n",
      "cnt: 0 - valLoss: 0.40707871317863464 - trainLoss: 0.3926425576210022\n",
      "cnt: 0 - valLoss: 0.4070771336555481 - trainLoss: 0.3926381766796112\n",
      "cnt: 0 - valLoss: 0.40707677602767944 - trainLoss: 0.3926337659358978\n",
      "cnt: 0 - valLoss: 0.4070754051208496 - trainLoss: 0.39262935519218445\n",
      "cnt: 0 - valLoss: 0.4070742726325989 - trainLoss: 0.3926249146461487\n",
      "cnt: 0 - valLoss: 0.40707293152809143 - trainLoss: 0.3926205337047577\n",
      "cnt: 0 - valLoss: 0.40707284212112427 - trainLoss: 0.3926161229610443\n",
      "cnt: 0 - valLoss: 0.407071590423584 - trainLoss: 0.39261171221733093\n",
      "cnt: 0 - valLoss: 0.40707018971443176 - trainLoss: 0.3926072418689728\n",
      "cnt: 0 - valLoss: 0.4070695638656616 - trainLoss: 0.3926028609275818\n",
      "cnt: 0 - valLoss: 0.40706780552864075 - trainLoss: 0.392598420381546\n",
      "cnt: 0 - valLoss: 0.4070676565170288 - trainLoss: 0.39259397983551025\n",
      "cnt: 0 - valLoss: 0.40706610679626465 - trainLoss: 0.3925895392894745\n",
      "cnt: 0 - valLoss: 0.4070656895637512 - trainLoss: 0.3925851285457611\n",
      "cnt: 0 - valLoss: 0.4070643186569214 - trainLoss: 0.39258065819740295\n",
      "cnt: 0 - valLoss: 0.40706366300582886 - trainLoss: 0.3925762176513672\n",
      "cnt: 0 - valLoss: 0.4070623517036438 - trainLoss: 0.3925717771053314\n",
      "cnt: 0 - valLoss: 0.4070611000061035 - trainLoss: 0.39256736636161804\n",
      "cnt: 0 - valLoss: 0.4070604741573334 - trainLoss: 0.3925628662109375\n",
      "cnt: 0 - valLoss: 0.4070601463317871 - trainLoss: 0.3925584554672241\n",
      "cnt: 0 - valLoss: 0.40705928206443787 - trainLoss: 0.39255401492118835\n",
      "cnt: 0 - valLoss: 0.4070577025413513 - trainLoss: 0.3925495147705078\n",
      "cnt: 0 - valLoss: 0.40705686807632446 - trainLoss: 0.39254504442214966\n",
      "cnt: 0 - valLoss: 0.40705615282058716 - trainLoss: 0.3925405740737915\n",
      "cnt: 0 - valLoss: 0.4070543050765991 - trainLoss: 0.39253610372543335\n",
      "cnt: 0 - valLoss: 0.4070543050765991 - trainLoss: 0.39253175258636475\n",
      "cnt: 0 - valLoss: 0.4070538580417633 - trainLoss: 0.39252737164497375\n",
      "cnt: 0 - valLoss: 0.40705248713493347 - trainLoss: 0.392522931098938\n",
      "cnt: 0 - valLoss: 0.40705129504203796 - trainLoss: 0.3925185203552246\n",
      "cnt: 0 - valLoss: 0.4070504307746887 - trainLoss: 0.392514169216156\n",
      "cnt: 0 - valLoss: 0.40704965591430664 - trainLoss: 0.3925097584724426\n",
      "cnt: 0 - valLoss: 0.4070495665073395 - trainLoss: 0.39250531792640686\n",
      "cnt: 0 - valLoss: 0.40704843401908875 - trainLoss: 0.39250093698501587\n",
      "cnt: 0 - valLoss: 0.4070470631122589 - trainLoss: 0.3924964964389801\n",
      "cnt: 0 - valLoss: 0.4070470631122589 - trainLoss: 0.3924920856952667\n",
      "cnt: 0 - valLoss: 0.4070454239845276 - trainLoss: 0.39248770475387573\n",
      "cnt: 0 - valLoss: 0.4070452153682709 - trainLoss: 0.39248326420783997\n",
      "cnt: 0 - valLoss: 0.40704378485679626 - trainLoss: 0.3924788534641266\n",
      "cnt: 0 - valLoss: 0.4070431590080261 - trainLoss: 0.39247438311576843\n",
      "cnt: 0 - valLoss: 0.4070432186126709 - trainLoss: 0.39246997237205505\n",
      "cnt: 1 - valLoss: 0.40704143047332764 - trainLoss: 0.3924655318260193\n",
      "cnt: 0 - valLoss: 0.4070410132408142 - trainLoss: 0.3924610912799835\n",
      "cnt: 0 - valLoss: 0.40703946352005005 - trainLoss: 0.39245665073394775\n",
      "cnt: 0 - valLoss: 0.4070386290550232 - trainLoss: 0.3924522399902344\n",
      "cnt: 0 - valLoss: 0.407039076089859 - trainLoss: 0.3924478590488434\n",
      "cnt: 1 - valLoss: 0.4070380926132202 - trainLoss: 0.3924435079097748\n",
      "cnt: 0 - valLoss: 0.4070363938808441 - trainLoss: 0.3924390971660614\n",
      "cnt: 0 - valLoss: 0.4070354998111725 - trainLoss: 0.39243465662002563\n",
      "cnt: 0 - valLoss: 0.40703442692756653 - trainLoss: 0.39243030548095703\n",
      "cnt: 0 - valLoss: 0.40703436732292175 - trainLoss: 0.39242589473724365\n",
      "cnt: 0 - valLoss: 0.40703317523002625 - trainLoss: 0.39242151379585266\n",
      "cnt: 0 - valLoss: 0.4070322811603546 - trainLoss: 0.39241713285446167\n",
      "cnt: 0 - valLoss: 0.4070318937301636 - trainLoss: 0.3924127519130707\n",
      "cnt: 0 - valLoss: 0.40703073143959045 - trainLoss: 0.3924083113670349\n",
      "cnt: 0 - valLoss: 0.40702947974205017 - trainLoss: 0.3924039304256439\n",
      "cnt: 0 - valLoss: 0.4070299565792084 - trainLoss: 0.3923995792865753\n",
      "cnt: 1 - valLoss: 0.40702834725379944 - trainLoss: 0.39239516854286194\n",
      "cnt: 0 - valLoss: 0.40702733397483826 - trainLoss: 0.39239075779914856\n",
      "cnt: 0 - valLoss: 0.40702784061431885 - trainLoss: 0.39238637685775757\n",
      "cnt: 1 - valLoss: 0.40702614188194275 - trainLoss: 0.3923819959163666\n",
      "cnt: 0 - valLoss: 0.40702521800994873 - trainLoss: 0.3923776149749756\n",
      "cnt: 0 - valLoss: 0.4070240259170532 - trainLoss: 0.3923732042312622\n",
      "cnt: 0 - valLoss: 0.40702393651008606 - trainLoss: 0.3923688232898712\n",
      "cnt: 0 - valLoss: 0.4070224165916443 - trainLoss: 0.3923644423484802\n",
      "cnt: 0 - valLoss: 0.4070219397544861 - trainLoss: 0.39236006140708923\n",
      "cnt: 0 - valLoss: 0.4070206582546234 - trainLoss: 0.3923557698726654\n",
      "cnt: 0 - valLoss: 0.40702059864997864 - trainLoss: 0.3923514485359192\n",
      "cnt: 0 - valLoss: 0.40701937675476074 - trainLoss: 0.39234715700149536\n",
      "cnt: 0 - valLoss: 0.40701884031295776 - trainLoss: 0.39234283566474915\n",
      "cnt: 0 - valLoss: 0.4070183038711548 - trainLoss: 0.3923385441303253\n",
      "cnt: 0 - valLoss: 0.4070177972316742 - trainLoss: 0.3923342525959015\n",
      "cnt: 0 - valLoss: 0.4070163667201996 - trainLoss: 0.3923299312591553\n",
      "cnt: 0 - valLoss: 0.40701574087142944 - trainLoss: 0.39232560992240906\n",
      "cnt: 0 - valLoss: 0.4070148169994354 - trainLoss: 0.39232137799263\n",
      "cnt: 0 - valLoss: 0.40701407194137573 - trainLoss: 0.3923170268535614\n",
      "cnt: 0 - valLoss: 0.4070131480693817 - trainLoss: 0.39231276512145996\n",
      "cnt: 0 - valLoss: 0.4070121943950653 - trainLoss: 0.39230844378471375\n",
      "cnt: 0 - valLoss: 0.40701228380203247 - trainLoss: 0.3923041522502899\n",
      "cnt: 1 - valLoss: 0.4070107340812683 - trainLoss: 0.3922998607158661\n",
      "cnt: 0 - valLoss: 0.40701061487197876 - trainLoss: 0.3922955393791199\n",
      "cnt: 0 - valLoss: 0.4070095121860504 - trainLoss: 0.39229127764701843\n",
      "cnt: 0 - valLoss: 0.40700915455818176 - trainLoss: 0.39228692650794983\n",
      "cnt: 0 - valLoss: 0.4070080816745758 - trainLoss: 0.3922826945781708\n",
      "cnt: 0 - valLoss: 0.40700772404670715 - trainLoss: 0.39227837324142456\n",
      "cnt: 0 - valLoss: 0.40700653195381165 - trainLoss: 0.39227408170700073\n",
      "cnt: 0 - valLoss: 0.4070057272911072 - trainLoss: 0.3922698199748993\n",
      "cnt: 0 - valLoss: 0.40700486302375793 - trainLoss: 0.39226552844047546\n",
      "cnt: 0 - valLoss: 0.407004714012146 - trainLoss: 0.39226123690605164\n",
      "cnt: 0 - valLoss: 0.4070030152797699 - trainLoss: 0.3922569453716278\n",
      "cnt: 0 - valLoss: 0.4070034623146057 - trainLoss: 0.392252653837204\n",
      "cnt: 1 - valLoss: 0.4070025384426117 - trainLoss: 0.39224836230278015\n",
      "cnt: 0 - valLoss: 0.40700122714042664 - trainLoss: 0.3922441005706787\n",
      "cnt: 0 - valLoss: 0.40700092911720276 - trainLoss: 0.3922397792339325\n",
      "cnt: 0 - valLoss: 0.4069995582103729 - trainLoss: 0.39223548769950867\n",
      "cnt: 0 - valLoss: 0.4069999158382416 - trainLoss: 0.3922312259674072\n",
      "cnt: 1 - valLoss: 0.4069986939430237 - trainLoss: 0.3922269344329834\n",
      "cnt: 0 - valLoss: 0.40699830651283264 - trainLoss: 0.3922226130962372\n",
      "cnt: 0 - valLoss: 0.4069969952106476 - trainLoss: 0.39221838116645813\n",
      "cnt: 0 - valLoss: 0.40699630975723267 - trainLoss: 0.3922140598297119\n",
      "cnt: 0 - valLoss: 0.40699514746665955 - trainLoss: 0.3922097682952881\n",
      "cnt: 0 - valLoss: 0.4069960117340088 - trainLoss: 0.39220550656318665\n",
      "cnt: 1 - valLoss: 0.40699514746665955 - trainLoss: 0.3922012448310852\n",
      "cnt: 0 - valLoss: 0.4069945514202118 - trainLoss: 0.3921969532966614\n",
      "cnt: 0 - valLoss: 0.4069932699203491 - trainLoss: 0.39219269156455994\n",
      "cnt: 0 - valLoss: 0.4069921374320984 - trainLoss: 0.3921884000301361\n",
      "cnt: 0 - valLoss: 0.4069916903972626 - trainLoss: 0.39218413829803467\n",
      "cnt: 0 - valLoss: 0.4069907069206238 - trainLoss: 0.39217984676361084\n",
      "cnt: 0 - valLoss: 0.40699028968811035 - trainLoss: 0.3921756148338318\n",
      "cnt: 0 - valLoss: 0.40698981285095215 - trainLoss: 0.39217132329940796\n",
      "cnt: 0 - valLoss: 0.4069889485836029 - trainLoss: 0.39216703176498413\n",
      "cnt: 0 - valLoss: 0.4069884717464447 - trainLoss: 0.3921627998352051\n",
      "cnt: 0 - valLoss: 0.4069879651069641 - trainLoss: 0.39215850830078125\n",
      "cnt: 0 - valLoss: 0.40698692202568054 - trainLoss: 0.3921542167663574\n",
      "cnt: 0 - valLoss: 0.40698596835136414 - trainLoss: 0.392149955034256\n",
      "cnt: 0 - valLoss: 0.40698567032814026 - trainLoss: 0.39214569330215454\n",
      "cnt: 0 - valLoss: 0.4069855213165283 - trainLoss: 0.3921414017677307\n",
      "cnt: 0 - valLoss: 0.4069843888282776 - trainLoss: 0.3921371400356293\n",
      "cnt: 0 - valLoss: 0.4069834351539612 - trainLoss: 0.39213281869888306\n",
      "cnt: 0 - valLoss: 0.40698254108428955 - trainLoss: 0.3921285569667816\n",
      "cnt: 0 - valLoss: 0.40698128938674927 - trainLoss: 0.3921242356300354\n",
      "cnt: 0 - valLoss: 0.4069812297821045 - trainLoss: 0.39211997389793396\n",
      "cnt: 0 - valLoss: 0.40698108077049255 - trainLoss: 0.3921157419681549\n",
      "cnt: 0 - valLoss: 0.4069806635379791 - trainLoss: 0.3921114504337311\n",
      "cnt: 0 - valLoss: 0.4069792926311493 - trainLoss: 0.39210718870162964\n",
      "cnt: 0 - valLoss: 0.4069787561893463 - trainLoss: 0.3921028673648834\n",
      "cnt: 0 - valLoss: 0.4069770872592926 - trainLoss: 0.392098605632782\n",
      "cnt: 0 - valLoss: 0.40697649121284485 - trainLoss: 0.39209431409835815\n",
      "cnt: 0 - valLoss: 0.40697604417800903 - trainLoss: 0.3920900821685791\n",
      "cnt: 0 - valLoss: 0.40697529911994934 - trainLoss: 0.3920857906341553\n",
      "cnt: 0 - valLoss: 0.40697550773620605 - trainLoss: 0.39208152890205383\n",
      "cnt: 1 - valLoss: 0.4069739878177643 - trainLoss: 0.3920772671699524\n",
      "cnt: 0 - valLoss: 0.40697336196899414 - trainLoss: 0.39207300543785095\n",
      "cnt: 0 - valLoss: 0.4069717824459076 - trainLoss: 0.3920687437057495\n",
      "cnt: 0 - valLoss: 0.4069715738296509 - trainLoss: 0.3920644223690033\n",
      "cnt: 0 - valLoss: 0.40697115659713745 - trainLoss: 0.39206022024154663\n",
      "cnt: 0 - valLoss: 0.4069701135158539 - trainLoss: 0.3920559585094452\n",
      "cnt: 0 - valLoss: 0.40696969628334045 - trainLoss: 0.392051637172699\n",
      "cnt: 0 - valLoss: 0.4069691598415375 - trainLoss: 0.3920474350452423\n",
      "cnt: 0 - valLoss: 0.4069684147834778 - trainLoss: 0.39204317331314087\n",
      "cnt: 0 - valLoss: 0.4069671332836151 - trainLoss: 0.39203888177871704\n",
      "cnt: 0 - valLoss: 0.4069663882255554 - trainLoss: 0.3920345902442932\n",
      "cnt: 0 - valLoss: 0.4069654941558838 - trainLoss: 0.3920303285121918\n",
      "cnt: 0 - valLoss: 0.4069654643535614 - trainLoss: 0.3920260965824127\n",
      "cnt: 0 - valLoss: 0.40696480870246887 - trainLoss: 0.3920218348503113\n",
      "cnt: 0 - valLoss: 0.406963974237442 - trainLoss: 0.39201757311820984\n",
      "cnt: 0 - valLoss: 0.4069625437259674 - trainLoss: 0.3920133113861084\n",
      "cnt: 0 - valLoss: 0.40696218609809875 - trainLoss: 0.39200904965400696\n",
      "cnt: 0 - valLoss: 0.4069611132144928 - trainLoss: 0.39200475811958313\n",
      "cnt: 0 - valLoss: 0.40696126222610474 - trainLoss: 0.3920005261898041\n",
      "cnt: 1 - valLoss: 0.40696069598197937 - trainLoss: 0.39199623465538025\n",
      "cnt: 0 - valLoss: 0.40695974230766296 - trainLoss: 0.3919919729232788\n",
      "cnt: 0 - valLoss: 0.40695905685424805 - trainLoss: 0.39198774099349976\n",
      "cnt: 0 - valLoss: 0.40695831179618835 - trainLoss: 0.3919834792613983\n",
      "cnt: 0 - valLoss: 0.40695756673812866 - trainLoss: 0.39197924733161926\n",
      "cnt: 0 - valLoss: 0.40695619583129883 - trainLoss: 0.3919750154018402\n",
      "cnt: 0 - valLoss: 0.4069567322731018 - trainLoss: 0.39197081327438354\n",
      "cnt: 1 - valLoss: 0.4069552719593048 - trainLoss: 0.39196664094924927\n",
      "cnt: 0 - valLoss: 0.40695521235466003 - trainLoss: 0.3919624388217926\n",
      "cnt: 0 - valLoss: 0.4069538414478302 - trainLoss: 0.39195823669433594\n",
      "cnt: 0 - valLoss: 0.40695345401763916 - trainLoss: 0.39195406436920166\n",
      "cnt: 0 - valLoss: 0.4069521427154541 - trainLoss: 0.391949862241745\n",
      "cnt: 0 - valLoss: 0.40695229172706604 - trainLoss: 0.39194566011428833\n",
      "cnt: 1 - valLoss: 0.40695157647132874 - trainLoss: 0.39194145798683167\n",
      "cnt: 0 - valLoss: 0.4069506824016571 - trainLoss: 0.3919373154640198\n",
      "cnt: 0 - valLoss: 0.4069499671459198 - trainLoss: 0.3919331133365631\n",
      "cnt: 0 - valLoss: 0.4069497287273407 - trainLoss: 0.39192891120910645\n",
      "cnt: 0 - valLoss: 0.406948983669281 - trainLoss: 0.3919247090816498\n",
      "cnt: 0 - valLoss: 0.40694916248321533 - trainLoss: 0.3919205367565155\n",
      "cnt: 1 - valLoss: 0.406948447227478 - trainLoss: 0.39191630482673645\n",
      "cnt: 0 - valLoss: 0.40694740414619446 - trainLoss: 0.3919120728969574\n",
      "cnt: 0 - valLoss: 0.4069466292858124 - trainLoss: 0.3919079005718231\n",
      "cnt: 0 - valLoss: 0.4069465398788452 - trainLoss: 0.39190366864204407\n",
      "cnt: 0 - valLoss: 0.40694549679756165 - trainLoss: 0.3918994665145874\n",
      "cnt: 0 - valLoss: 0.4069449007511139 - trainLoss: 0.39189526438713074\n",
      "cnt: 0 - valLoss: 0.406943678855896 - trainLoss: 0.39189109206199646\n",
      "cnt: 0 - valLoss: 0.4069436490535736 - trainLoss: 0.391886830329895\n",
      "cnt: 0 - valLoss: 0.4069424271583557 - trainLoss: 0.39188268780708313\n",
      "cnt: 0 - valLoss: 0.4069422483444214 - trainLoss: 0.39187851548194885\n",
      "cnt: 0 - valLoss: 0.40694186091423035 - trainLoss: 0.39187437295913696\n",
      "cnt: 0 - valLoss: 0.406940758228302 - trainLoss: 0.39187026023864746\n",
      "cnt: 0 - valLoss: 0.4069398045539856 - trainLoss: 0.3918660879135132\n",
      "cnt: 0 - valLoss: 0.40693971514701843 - trainLoss: 0.3918619453907013\n",
      "cnt: 0 - valLoss: 0.40693914890289307 - trainLoss: 0.3918578326702118\n",
      "cnt: 0 - valLoss: 0.4069378972053528 - trainLoss: 0.3918536901473999\n",
      "cnt: 0 - valLoss: 0.4069375991821289 - trainLoss: 0.3918495178222656\n",
      "cnt: 0 - valLoss: 0.40693724155426025 - trainLoss: 0.3918454051017761\n",
      "cnt: 0 - valLoss: 0.4069363474845886 - trainLoss: 0.39184126257896423\n",
      "cnt: 0 - valLoss: 0.406934916973114 - trainLoss: 0.39183714985847473\n",
      "cnt: 0 - valLoss: 0.40693509578704834 - trainLoss: 0.39183300733566284\n",
      "cnt: 1 - valLoss: 0.40693366527557373 - trainLoss: 0.39182889461517334\n",
      "cnt: 0 - valLoss: 0.4069332778453827 - trainLoss: 0.39182478189468384\n",
      "cnt: 0 - valLoss: 0.4069322943687439 - trainLoss: 0.39182060956954956\n",
      "cnt: 0 - valLoss: 0.40693217515945435 - trainLoss: 0.39181649684906006\n",
      "cnt: 0 - valLoss: 0.4069311022758484 - trainLoss: 0.39181235432624817\n",
      "cnt: 0 - valLoss: 0.40693026781082153 - trainLoss: 0.3918082118034363\n",
      "cnt: 0 - valLoss: 0.4069299101829529 - trainLoss: 0.3918040692806244\n",
      "cnt: 0 - valLoss: 0.4069288671016693 - trainLoss: 0.3917999565601349\n",
      "cnt: 0 - valLoss: 0.4069283604621887 - trainLoss: 0.3917958438396454\n",
      "cnt: 0 - valLoss: 0.4069274663925171 - trainLoss: 0.3917917013168335\n",
      "cnt: 0 - valLoss: 0.4069273769855499 - trainLoss: 0.391787588596344\n",
      "cnt: 0 - valLoss: 0.40692567825317383 - trainLoss: 0.3917835056781769\n",
      "cnt: 0 - valLoss: 0.4069247841835022 - trainLoss: 0.3917793929576874\n",
      "cnt: 0 - valLoss: 0.406924843788147 - trainLoss: 0.3917752802371979\n",
      "cnt: 1 - valLoss: 0.40692371129989624 - trainLoss: 0.3917711675167084\n",
      "cnt: 0 - valLoss: 0.4069226086139679 - trainLoss: 0.3917670249938965\n",
      "cnt: 0 - valLoss: 0.40692150592803955 - trainLoss: 0.39176294207572937\n",
      "cnt: 0 - valLoss: 0.40692201256752014 - trainLoss: 0.39175885915756226\n",
      "cnt: 1 - valLoss: 0.4069199562072754 - trainLoss: 0.39175477623939514\n",
      "cnt: 0 - valLoss: 0.40691903233528137 - trainLoss: 0.39175066351890564\n",
      "cnt: 0 - valLoss: 0.4069180190563202 - trainLoss: 0.3917465806007385\n",
      "cnt: 0 - valLoss: 0.4069172739982605 - trainLoss: 0.391742467880249\n",
      "cnt: 0 - valLoss: 0.4069156050682068 - trainLoss: 0.3917383849620819\n",
      "cnt: 0 - valLoss: 0.40691477060317993 - trainLoss: 0.3917343318462372\n",
      "cnt: 0 - valLoss: 0.4069141745567322 - trainLoss: 0.3917302191257477\n",
      "cnt: 0 - valLoss: 0.40691354870796204 - trainLoss: 0.39172613620758057\n",
      "cnt: 0 - valLoss: 0.40691182017326355 - trainLoss: 0.39172205328941345\n",
      "cnt: 0 - valLoss: 0.4069114029407501 - trainLoss: 0.39171797037124634\n",
      "cnt: 0 - valLoss: 0.4069095551967621 - trainLoss: 0.39171385765075684\n",
      "cnt: 0 - valLoss: 0.4069088101387024 - trainLoss: 0.3917098045349121\n",
      "cnt: 0 - valLoss: 0.4069083034992218 - trainLoss: 0.3917056918144226\n",
      "cnt: 0 - valLoss: 0.40690740942955017 - trainLoss: 0.3917016386985779\n",
      "cnt: 0 - valLoss: 0.40690621733665466 - trainLoss: 0.39169755578041077\n",
      "cnt: 0 - valLoss: 0.4069044589996338 - trainLoss: 0.39169347286224365\n",
      "cnt: 0 - valLoss: 0.40690356492996216 - trainLoss: 0.39168936014175415\n",
      "cnt: 0 - valLoss: 0.40690162777900696 - trainLoss: 0.39168527722358704\n",
      "cnt: 0 - valLoss: 0.40690189599990845 - trainLoss: 0.3916812241077423\n",
      "cnt: 1 - valLoss: 0.40689969062805176 - trainLoss: 0.3916771709918976\n",
      "cnt: 0 - valLoss: 0.406899094581604 - trainLoss: 0.3916730284690857\n",
      "cnt: 0 - valLoss: 0.40689817070961 - trainLoss: 0.39166897535324097\n",
      "cnt: 0 - valLoss: 0.40689706802368164 - trainLoss: 0.39166489243507385\n",
      "cnt: 0 - valLoss: 0.4068963825702667 - trainLoss: 0.3916608393192291\n",
      "cnt: 0 - valLoss: 0.4068949222564697 - trainLoss: 0.3916567862033844\n",
      "cnt: 0 - valLoss: 0.40689316391944885 - trainLoss: 0.3916527032852173\n",
      "cnt: 0 - valLoss: 0.40689313411712646 - trainLoss: 0.39164862036705017\n",
      "cnt: 0 - valLoss: 0.40689122676849365 - trainLoss: 0.39164453744888306\n",
      "cnt: 0 - valLoss: 0.40689173340797424 - trainLoss: 0.39164048433303833\n",
      "cnt: 1 - valLoss: 0.40688982605934143 - trainLoss: 0.3916364312171936\n",
      "cnt: 0 - valLoss: 0.40688851475715637 - trainLoss: 0.3916323482990265\n",
      "cnt: 0 - valLoss: 0.40688735246658325 - trainLoss: 0.3916282653808594\n",
      "cnt: 0 - valLoss: 0.4068869352340698 - trainLoss: 0.39162421226501465\n",
      "cnt: 0 - valLoss: 0.40688589215278625 - trainLoss: 0.3916201591491699\n",
      "cnt: 0 - valLoss: 0.4068843424320221 - trainLoss: 0.3916160762310028\n",
      "cnt: 0 - valLoss: 0.4068831503391266 - trainLoss: 0.3916120231151581\n",
      "cnt: 0 - valLoss: 0.40688323974609375 - trainLoss: 0.39160796999931335\n",
      "cnt: 1 - valLoss: 0.4068814218044281 - trainLoss: 0.39160391688346863\n",
      "cnt: 0 - valLoss: 0.40688014030456543 - trainLoss: 0.3915998637676239\n",
      "cnt: 0 - valLoss: 0.40687915682792664 - trainLoss: 0.3915957808494568\n",
      "cnt: 0 - valLoss: 0.40687835216522217 - trainLoss: 0.39159172773361206\n",
      "cnt: 0 - valLoss: 0.40687769651412964 - trainLoss: 0.3915877044200897\n",
      "cnt: 0 - valLoss: 0.40687644481658936 - trainLoss: 0.391583651304245\n",
      "cnt: 0 - valLoss: 0.4068753719329834 - trainLoss: 0.39157962799072266\n",
      "cnt: 0 - valLoss: 0.40687477588653564 - trainLoss: 0.3915756046772003\n",
      "cnt: 0 - valLoss: 0.4068731665611267 - trainLoss: 0.391571581363678\n",
      "cnt: 0 - valLoss: 0.40687280893325806 - trainLoss: 0.39156755805015564\n",
      "cnt: 0 - valLoss: 0.4068722426891327 - trainLoss: 0.3915635347366333\n",
      "cnt: 0 - valLoss: 0.40687069296836853 - trainLoss: 0.39155954122543335\n",
      "cnt: 0 - valLoss: 0.4068690240383148 - trainLoss: 0.39155545830726624\n",
      "cnt: 0 - valLoss: 0.4068685472011566 - trainLoss: 0.3915514647960663\n",
      "cnt: 0 - valLoss: 0.4068678021430969 - trainLoss: 0.39154741168022156\n",
      "cnt: 0 - valLoss: 0.4068673849105835 - trainLoss: 0.3915434181690216\n",
      "cnt: 0 - valLoss: 0.4068656265735626 - trainLoss: 0.39153939485549927\n",
      "cnt: 0 - valLoss: 0.40686526894569397 - trainLoss: 0.39153537154197693\n",
      "cnt: 0 - valLoss: 0.40686431527137756 - trainLoss: 0.391531378030777\n",
      "cnt: 0 - valLoss: 0.40686309337615967 - trainLoss: 0.391527384519577\n",
      "cnt: 0 - valLoss: 0.40686163306236267 - trainLoss: 0.3915233314037323\n",
      "cnt: 0 - valLoss: 0.40686121582984924 - trainLoss: 0.39151933789253235\n",
      "cnt: 0 - valLoss: 0.40686091780662537 - trainLoss: 0.3915153443813324\n",
      "cnt: 0 - valLoss: 0.40685972571372986 - trainLoss: 0.39151129126548767\n",
      "cnt: 0 - valLoss: 0.4068591892719269 - trainLoss: 0.3915073275566101\n",
      "cnt: 0 - valLoss: 0.40685778856277466 - trainLoss: 0.39150330424308777\n",
      "cnt: 0 - valLoss: 0.4068565368652344 - trainLoss: 0.39149925112724304\n",
      "cnt: 0 - valLoss: 0.4068557024002075 - trainLoss: 0.3914952278137207\n",
      "cnt: 0 - valLoss: 0.40685492753982544 - trainLoss: 0.39149123430252075\n",
      "cnt: 0 - valLoss: 0.40685388445854187 - trainLoss: 0.3914872407913208\n",
      "cnt: 0 - valLoss: 0.40685319900512695 - trainLoss: 0.39148324728012085\n",
      "cnt: 0 - valLoss: 0.4068529009819031 - trainLoss: 0.3914792537689209\n",
      "cnt: 0 - valLoss: 0.4068518280982971 - trainLoss: 0.39147523045539856\n",
      "cnt: 0 - valLoss: 0.40685102343559265 - trainLoss: 0.391471266746521\n",
      "cnt: 0 - valLoss: 0.4068504571914673 - trainLoss: 0.39146727323532104\n",
      "cnt: 0 - valLoss: 0.40684959292411804 - trainLoss: 0.3914632797241211\n",
      "cnt: 0 - valLoss: 0.4068484604358673 - trainLoss: 0.39145925641059875\n",
      "cnt: 0 - valLoss: 0.4068468511104584 - trainLoss: 0.3914552330970764\n",
      "cnt: 0 - valLoss: 0.4068472683429718 - trainLoss: 0.39145126938819885\n",
      "cnt: 1 - valLoss: 0.4068474769592285 - trainLoss: 0.3914472758769989\n",
      "cnt: 2 - valLoss: 0.40684619545936584 - trainLoss: 0.39144331216812134\n",
      "cnt: 0 - valLoss: 0.4068444073200226 - trainLoss: 0.391439288854599\n",
      "cnt: 0 - valLoss: 0.4068449139595032 - trainLoss: 0.39143529534339905\n",
      "cnt: 1 - valLoss: 0.4068433344364166 - trainLoss: 0.3914313316345215\n",
      "cnt: 0 - valLoss: 0.4068421721458435 - trainLoss: 0.39142730832099915\n",
      "cnt: 0 - valLoss: 0.40684106945991516 - trainLoss: 0.3914233148097992\n",
      "cnt: 0 - valLoss: 0.4068411588668823 - trainLoss: 0.39141935110092163\n",
      "cnt: 1 - valLoss: 0.4068394899368286 - trainLoss: 0.3914153575897217\n",
      "cnt: 0 - valLoss: 0.40683895349502563 - trainLoss: 0.39141136407852173\n",
      "cnt: 0 - valLoss: 0.4068380296230316 - trainLoss: 0.3914073407649994\n",
      "cnt: 0 - valLoss: 0.4068383276462555 - trainLoss: 0.3914034068584442\n",
      "cnt: 1 - valLoss: 0.4068370461463928 - trainLoss: 0.39139941334724426\n",
      "cnt: 0 - valLoss: 0.4068363308906555 - trainLoss: 0.3913953900337219\n",
      "cnt: 0 - valLoss: 0.40683460235595703 - trainLoss: 0.39139145612716675\n",
      "cnt: 0 - valLoss: 0.40683436393737793 - trainLoss: 0.3913874626159668\n",
      "cnt: 0 - valLoss: 0.4068332612514496 - trainLoss: 0.39138346910476685\n",
      "cnt: 0 - valLoss: 0.40683212876319885 - trainLoss: 0.3913794755935669\n",
      "cnt: 0 - valLoss: 0.40683314204216003 - trainLoss: 0.39137551188468933\n",
      "cnt: 1 - valLoss: 0.4068323075771332 - trainLoss: 0.3913715183734894\n",
      "cnt: 2 - valLoss: 0.4068308174610138 - trainLoss: 0.3913675844669342\n",
      "cnt: 0 - valLoss: 0.40683016180992126 - trainLoss: 0.39136362075805664\n",
      "cnt: 0 - valLoss: 0.4068288505077362 - trainLoss: 0.3913595974445343\n",
      "cnt: 0 - valLoss: 0.40682798624038696 - trainLoss: 0.39135560393333435\n",
      "cnt: 0 - valLoss: 0.4068276584148407 - trainLoss: 0.3913516402244568\n",
      "cnt: 0 - valLoss: 0.4068264365196228 - trainLoss: 0.3913476765155792\n",
      "cnt: 0 - valLoss: 0.4068250358104706 - trainLoss: 0.3913436830043793\n",
      "cnt: 0 - valLoss: 0.40682387351989746 - trainLoss: 0.3913397192955017\n",
      "cnt: 0 - valLoss: 0.40682411193847656 - trainLoss: 0.39133575558662415\n",
      "cnt: 1 - valLoss: 0.4068221151828766 - trainLoss: 0.3913317918777466\n",
      "cnt: 0 - valLoss: 0.4068218767642975 - trainLoss: 0.3913278579711914\n",
      "cnt: 0 - valLoss: 0.40682071447372437 - trainLoss: 0.39132386445999146\n",
      "cnt: 0 - valLoss: 0.40682074427604675 - trainLoss: 0.3913199305534363\n",
      "cnt: 1 - valLoss: 0.4068182110786438 - trainLoss: 0.3913159668445587\n",
      "cnt: 0 - valLoss: 0.4068182408809662 - trainLoss: 0.39131200313568115\n",
      "cnt: 1 - valLoss: 0.40681734681129456 - trainLoss: 0.3913080394268036\n",
      "cnt: 0 - valLoss: 0.4068168103694916 - trainLoss: 0.391304075717926\n",
      "cnt: 0 - valLoss: 0.40681466460227966 - trainLoss: 0.39130014181137085\n",
      "cnt: 0 - valLoss: 0.4068150222301483 - trainLoss: 0.3912961781024933\n",
      "cnt: 1 - valLoss: 0.40681391954421997 - trainLoss: 0.3912922441959381\n",
      "cnt: 0 - valLoss: 0.4068143367767334 - trainLoss: 0.39128822088241577\n",
      "cnt: 1 - valLoss: 0.40681296586990356 - trainLoss: 0.3912842571735382\n",
      "cnt: 0 - valLoss: 0.4068133234977722 - trainLoss: 0.39128023386001587\n",
      "cnt: 1 - valLoss: 0.40681228041648865 - trainLoss: 0.39127621054649353\n",
      "cnt: 0 - valLoss: 0.40681296586990356 - trainLoss: 0.3912722170352936\n",
      "cnt: 1 - valLoss: 0.4068114757537842 - trainLoss: 0.391268253326416\n",
      "cnt: 0 - valLoss: 0.4068119525909424 - trainLoss: 0.3912642300128937\n",
      "cnt: 1 - valLoss: 0.4068114757537842 - trainLoss: 0.3912602365016937\n",
      "cnt: 0 - valLoss: 0.4068100154399872 - trainLoss: 0.3912562429904938\n",
      "cnt: 0 - valLoss: 0.40681058168411255 - trainLoss: 0.3912522494792938\n",
      "cnt: 1 - valLoss: 0.4068096876144409 - trainLoss: 0.3912482261657715\n",
      "cnt: 0 - valLoss: 0.4068097472190857 - trainLoss: 0.3912442624568939\n",
      "cnt: 1 - valLoss: 0.4068085849285126 - trainLoss: 0.39124026894569397\n",
      "cnt: 0 - valLoss: 0.4068090319633484 - trainLoss: 0.39123624563217163\n",
      "cnt: 1 - valLoss: 0.4068075716495514 - trainLoss: 0.3912322223186493\n",
      "cnt: 0 - valLoss: 0.4068090319633484 - trainLoss: 0.39122822880744934\n",
      "cnt: 1 - valLoss: 0.4068073034286499 - trainLoss: 0.391224205493927\n",
      "cnt: 0 - valLoss: 0.406807005405426 - trainLoss: 0.39122018218040466\n",
      "cnt: 0 - valLoss: 0.4068056344985962 - trainLoss: 0.3912161886692047\n",
      "cnt: 0 - valLoss: 0.40680694580078125 - trainLoss: 0.3912121653556824\n",
      "cnt: 1 - valLoss: 0.4068056643009186 - trainLoss: 0.3912081718444824\n",
      "cnt: 2 - valLoss: 0.4068053364753723 - trainLoss: 0.3912041485309601\n",
      "cnt: 0 - valLoss: 0.4068046808242798 - trainLoss: 0.3912001848220825\n",
      "cnt: 0 - valLoss: 0.40680429339408875 - trainLoss: 0.39119619131088257\n",
      "cnt: 0 - valLoss: 0.4068034887313843 - trainLoss: 0.391192227602005\n",
      "cnt: 0 - valLoss: 0.40680328011512756 - trainLoss: 0.39118820428848267\n",
      "cnt: 0 - valLoss: 0.4068019986152649 - trainLoss: 0.3911842405796051\n",
      "cnt: 0 - valLoss: 0.40680280327796936 - trainLoss: 0.39118024706840515\n",
      "cnt: 1 - valLoss: 0.4068014919757843 - trainLoss: 0.3911762833595276\n",
      "cnt: 0 - valLoss: 0.4068010449409485 - trainLoss: 0.39117231965065\n",
      "cnt: 0 - valLoss: 0.40680068731307983 - trainLoss: 0.39116835594177246\n",
      "cnt: 0 - valLoss: 0.4068002700805664 - trainLoss: 0.3911643624305725\n",
      "cnt: 0 - valLoss: 0.4067993760108948 - trainLoss: 0.39116036891937256\n",
      "cnt: 0 - valLoss: 0.40679875016212463 - trainLoss: 0.391156405210495\n",
      "cnt: 0 - valLoss: 0.406798392534256 - trainLoss: 0.39115244150161743\n",
      "cnt: 0 - valLoss: 0.4067973792552948 - trainLoss: 0.39114850759506226\n",
      "cnt: 0 - valLoss: 0.40679624676704407 - trainLoss: 0.3911445736885071\n",
      "cnt: 0 - valLoss: 0.40679553151130676 - trainLoss: 0.3911406099796295\n",
      "cnt: 0 - valLoss: 0.4067952036857605 - trainLoss: 0.39113667607307434\n",
      "cnt: 0 - valLoss: 0.4067942500114441 - trainLoss: 0.39113277196884155\n",
      "cnt: 0 - valLoss: 0.40679293870925903 - trainLoss: 0.391128808259964\n",
      "cnt: 0 - valLoss: 0.40679290890693665 - trainLoss: 0.3911248743534088\n",
      "cnt: 0 - valLoss: 0.40679171681404114 - trainLoss: 0.39112094044685364\n",
      "cnt: 0 - valLoss: 0.40679070353507996 - trainLoss: 0.39111700654029846\n",
      "cnt: 0 - valLoss: 0.40679028630256653 - trainLoss: 0.3911130726337433\n",
      "cnt: 0 - valLoss: 0.40678930282592773 - trainLoss: 0.3911091387271881\n",
      "cnt: 0 - valLoss: 0.4067895710468292 - trainLoss: 0.39110520482063293\n",
      "cnt: 1 - valLoss: 0.4067874252796173 - trainLoss: 0.39110130071640015\n",
      "cnt: 0 - valLoss: 0.40678662061691284 - trainLoss: 0.39109736680984497\n",
      "cnt: 0 - valLoss: 0.40678566694259644 - trainLoss: 0.3910934329032898\n",
      "cnt: 0 - valLoss: 0.4067856967449188 - trainLoss: 0.3910894989967346\n",
      "cnt: 1 - valLoss: 0.4067842960357666 - trainLoss: 0.39108559489250183\n",
      "cnt: 0 - valLoss: 0.40678414702415466 - trainLoss: 0.39108166098594666\n",
      "cnt: 0 - valLoss: 0.40678298473358154 - trainLoss: 0.39107775688171387\n",
      "cnt: 0 - valLoss: 0.4067825376987457 - trainLoss: 0.3910738527774811\n",
      "cnt: 0 - valLoss: 0.40678128600120544 - trainLoss: 0.3910699188709259\n",
      "cnt: 0 - valLoss: 0.4067808985710144 - trainLoss: 0.3910660147666931\n",
      "cnt: 0 - valLoss: 0.4067801833152771 - trainLoss: 0.3910621106624603\n",
      "cnt: 0 - valLoss: 0.4067797362804413 - trainLoss: 0.39105817675590515\n",
      "cnt: 0 - valLoss: 0.406777948141098 - trainLoss: 0.39105424284935\n",
      "cnt: 0 - valLoss: 0.40677785873413086 - trainLoss: 0.3910503685474396\n",
      "cnt: 0 - valLoss: 0.4067767560482025 - trainLoss: 0.3910464644432068\n",
      "cnt: 0 - valLoss: 0.4067763388156891 - trainLoss: 0.3910425305366516\n",
      "cnt: 0 - valLoss: 0.40677472949028015 - trainLoss: 0.39103859663009644\n",
      "cnt: 0 - valLoss: 0.4067748486995697 - trainLoss: 0.39103472232818604\n",
      "cnt: 1 - valLoss: 0.4067736566066742 - trainLoss: 0.39103081822395325\n",
      "cnt: 0 - valLoss: 0.4067734181880951 - trainLoss: 0.39102691411972046\n",
      "cnt: 0 - valLoss: 0.4067719578742981 - trainLoss: 0.39102301001548767\n",
      "cnt: 0 - valLoss: 0.4067709147930145 - trainLoss: 0.3910190761089325\n",
      "cnt: 0 - valLoss: 0.4067707061767578 - trainLoss: 0.3910152018070221\n",
      "cnt: 0 - valLoss: 0.4067698121070862 - trainLoss: 0.3910112977027893\n",
      "cnt: 0 - valLoss: 0.40676936507225037 - trainLoss: 0.3910073935985565\n",
      "cnt: 0 - valLoss: 0.40676844120025635 - trainLoss: 0.39100348949432373\n",
      "cnt: 0 - valLoss: 0.40676742792129517 - trainLoss: 0.39099958539009094\n",
      "cnt: 0 - valLoss: 0.40676698088645935 - trainLoss: 0.39099568128585815\n",
      "cnt: 0 - valLoss: 0.4067654013633728 - trainLoss: 0.39099180698394775\n",
      "cnt: 0 - valLoss: 0.40676623582839966 - trainLoss: 0.3909878730773926\n",
      "cnt: 1 - valLoss: 0.4067640006542206 - trainLoss: 0.3909839987754822\n",
      "cnt: 0 - valLoss: 0.40676382184028625 - trainLoss: 0.3909800946712494\n",
      "cnt: 0 - valLoss: 0.4067627489566803 - trainLoss: 0.3909761905670166\n",
      "cnt: 0 - valLoss: 0.40676262974739075 - trainLoss: 0.3909723460674286\n",
      "cnt: 0 - valLoss: 0.4067612886428833 - trainLoss: 0.3909684419631958\n",
      "cnt: 0 - valLoss: 0.4067615866661072 - trainLoss: 0.3909645676612854\n",
      "cnt: 1 - valLoss: 0.4067595899105072 - trainLoss: 0.3909606337547302\n",
      "cnt: 0 - valLoss: 0.4067598581314087 - trainLoss: 0.39095672965049744\n",
      "cnt: 1 - valLoss: 0.4067581593990326 - trainLoss: 0.39095285534858704\n",
      "cnt: 0 - valLoss: 0.40675821900367737 - trainLoss: 0.3909490406513214\n",
      "cnt: 1 - valLoss: 0.4067564010620117 - trainLoss: 0.39094510674476624\n",
      "cnt: 0 - valLoss: 0.4067564010620117 - trainLoss: 0.39094120264053345\n",
      "cnt: 0 - valLoss: 0.406755268573761 - trainLoss: 0.39093732833862305\n",
      "cnt: 0 - valLoss: 0.40675482153892517 - trainLoss: 0.39093345403671265\n",
      "cnt: 0 - valLoss: 0.40675264596939087 - trainLoss: 0.39092954993247986\n",
      "cnt: 0 - valLoss: 0.4067539572715759 - trainLoss: 0.39092573523521423\n",
      "cnt: 1 - valLoss: 0.4067520797252655 - trainLoss: 0.39092183113098145\n",
      "cnt: 0 - valLoss: 0.4067516624927521 - trainLoss: 0.39091792702674866\n",
      "cnt: 0 - valLoss: 0.4067508578300476 - trainLoss: 0.39091405272483826\n",
      "cnt: 0 - valLoss: 0.4067499339580536 - trainLoss: 0.39091020822525024\n",
      "cnt: 0 - valLoss: 0.4067497253417969 - trainLoss: 0.39090633392333984\n",
      "cnt: 0 - valLoss: 0.40674808621406555 - trainLoss: 0.39090245962142944\n",
      "cnt: 0 - valLoss: 0.4067472219467163 - trainLoss: 0.39089858531951904\n",
      "cnt: 0 - valLoss: 0.40674707293510437 - trainLoss: 0.39089471101760864\n",
      "cnt: 0 - valLoss: 0.40674564242362976 - trainLoss: 0.39089086651802063\n",
      "cnt: 0 - valLoss: 0.4067443013191223 - trainLoss: 0.39088696241378784\n",
      "cnt: 0 - valLoss: 0.4067448675632477 - trainLoss: 0.39088311791419983\n",
      "cnt: 1 - valLoss: 0.40674301981925964 - trainLoss: 0.39087924361228943\n",
      "cnt: 0 - valLoss: 0.4067420959472656 - trainLoss: 0.39087536931037903\n",
      "cnt: 0 - valLoss: 0.4067424535751343 - trainLoss: 0.390871524810791\n",
      "cnt: 1 - valLoss: 0.4067404568195343 - trainLoss: 0.3908676505088806\n",
      "cnt: 0 - valLoss: 0.40674009919166565 - trainLoss: 0.3908637762069702\n",
      "cnt: 0 - valLoss: 0.4067392945289612 - trainLoss: 0.3908599317073822\n",
      "cnt: 0 - valLoss: 0.40673843026161194 - trainLoss: 0.3908560872077942\n",
      "cnt: 0 - valLoss: 0.4067373275756836 - trainLoss: 0.3908522129058838\n",
      "cnt: 0 - valLoss: 0.40673595666885376 - trainLoss: 0.3908483386039734\n",
      "cnt: 0 - valLoss: 0.40673667192459106 - trainLoss: 0.3908444941043854\n",
      "cnt: 1 - valLoss: 0.4067343771457672 - trainLoss: 0.390840619802475\n",
      "cnt: 0 - valLoss: 0.4067344665527344 - trainLoss: 0.39083677530288696\n",
      "cnt: 1 - valLoss: 0.4067324995994568 - trainLoss: 0.39083296060562134\n",
      "cnt: 0 - valLoss: 0.4067326486110687 - trainLoss: 0.39082905650138855\n",
      "cnt: 1 - valLoss: 0.406730979681015 - trainLoss: 0.39082521200180054\n",
      "cnt: 0 - valLoss: 0.406730592250824 - trainLoss: 0.3908213973045349\n",
      "cnt: 0 - valLoss: 0.406729131937027 - trainLoss: 0.3908175230026245\n",
      "cnt: 0 - valLoss: 0.40672874450683594 - trainLoss: 0.3908136785030365\n",
      "cnt: 0 - valLoss: 0.40672820806503296 - trainLoss: 0.3908098340034485\n",
      "cnt: 0 - valLoss: 0.4067270755767822 - trainLoss: 0.3908059895038605\n",
      "cnt: 0 - valLoss: 0.4067267179489136 - trainLoss: 0.39080214500427246\n",
      "cnt: 0 - valLoss: 0.40672567486763 - trainLoss: 0.39079833030700684\n",
      "cnt: 0 - valLoss: 0.4067249596118927 - trainLoss: 0.39079445600509644\n",
      "cnt: 0 - valLoss: 0.4067249894142151 - trainLoss: 0.3907906115055084\n",
      "cnt: 1 - valLoss: 0.40672311186790466 - trainLoss: 0.3907867670059204\n",
      "cnt: 0 - valLoss: 0.4067227840423584 - trainLoss: 0.3907829523086548\n",
      "cnt: 0 - valLoss: 0.40672236680984497 - trainLoss: 0.3907790780067444\n",
      "cnt: 0 - valLoss: 0.4067215323448181 - trainLoss: 0.39077526330947876\n",
      "cnt: 0 - valLoss: 0.4067205786705017 - trainLoss: 0.39077138900756836\n",
      "cnt: 0 - valLoss: 0.40672028064727783 - trainLoss: 0.39076757431030273\n",
      "cnt: 0 - valLoss: 0.40671902894973755 - trainLoss: 0.3907637894153595\n",
      "cnt: 0 - valLoss: 0.4067184627056122 - trainLoss: 0.3907599449157715\n",
      "cnt: 0 - valLoss: 0.4067167341709137 - trainLoss: 0.3907560706138611\n",
      "cnt: 0 - valLoss: 0.40671661496162415 - trainLoss: 0.39075225591659546\n",
      "cnt: 0 - valLoss: 0.40671560168266296 - trainLoss: 0.39074841141700745\n",
      "cnt: 0 - valLoss: 0.4067157208919525 - trainLoss: 0.3907445967197418\n",
      "cnt: 1 - valLoss: 0.4067141115665436 - trainLoss: 0.3907407522201538\n",
      "cnt: 0 - valLoss: 0.4067133367061615 - trainLoss: 0.3907369077205658\n",
      "cnt: 0 - valLoss: 0.40671342611312866 - trainLoss: 0.39073309302330017\n",
      "cnt: 1 - valLoss: 0.40671223402023315 - trainLoss: 0.39072927832603455\n",
      "cnt: 0 - valLoss: 0.4067118167877197 - trainLoss: 0.39072543382644653\n",
      "cnt: 0 - valLoss: 0.40671125054359436 - trainLoss: 0.3907216489315033\n",
      "cnt: 0 - valLoss: 0.4067097008228302 - trainLoss: 0.3907177746295929\n",
      "cnt: 0 - valLoss: 0.40670937299728394 - trainLoss: 0.39071395993232727\n",
      "cnt: 0 - valLoss: 0.4067079424858093 - trainLoss: 0.39071011543273926\n",
      "cnt: 0 - valLoss: 0.4067085087299347 - trainLoss: 0.3907063603401184\n",
      "cnt: 1 - valLoss: 0.40670695900917053 - trainLoss: 0.3907025158405304\n",
      "cnt: 0 - valLoss: 0.4067060053348541 - trainLoss: 0.39069864153862\n",
      "cnt: 0 - valLoss: 0.406705379486084 - trainLoss: 0.39069488644599915\n",
      "cnt: 0 - valLoss: 0.40670472383499146 - trainLoss: 0.39069104194641113\n",
      "cnt: 0 - valLoss: 0.4067045748233795 - trainLoss: 0.3906872272491455\n",
      "cnt: 0 - valLoss: 0.40670308470726013 - trainLoss: 0.3906834125518799\n",
      "cnt: 0 - valLoss: 0.40670278668403625 - trainLoss: 0.39067956805229187\n",
      "cnt: 0 - valLoss: 0.40670186281204224 - trainLoss: 0.39067575335502625\n",
      "cnt: 0 - valLoss: 0.4067019522190094 - trainLoss: 0.3906719386577606\n",
      "cnt: 1 - valLoss: 0.406700998544693 - trainLoss: 0.39066818356513977\n",
      "cnt: 0 - valLoss: 0.40669992566108704 - trainLoss: 0.39066436886787415\n",
      "cnt: 0 - valLoss: 0.40669891238212585 - trainLoss: 0.39066049456596375\n",
      "cnt: 0 - valLoss: 0.4066990911960602 - trainLoss: 0.3906567096710205\n",
      "cnt: 1 - valLoss: 0.4066970646381378 - trainLoss: 0.39065292477607727\n",
      "cnt: 0 - valLoss: 0.40669742226600647 - trainLoss: 0.39064911007881165\n",
      "cnt: 1 - valLoss: 0.4066956639289856 - trainLoss: 0.390645295381546\n",
      "cnt: 0 - valLoss: 0.4066953957080841 - trainLoss: 0.3906414806842804\n",
      "cnt: 0 - valLoss: 0.4066954553127289 - trainLoss: 0.39063769578933716\n",
      "cnt: 1 - valLoss: 0.4066939353942871 - trainLoss: 0.39063385128974915\n",
      "cnt: 0 - valLoss: 0.40669310092926025 - trainLoss: 0.3906300365924835\n",
      "cnt: 0 - valLoss: 0.4066932499408722 - trainLoss: 0.39062628149986267\n",
      "cnt: 1 - valLoss: 0.40669184923171997 - trainLoss: 0.39062243700027466\n",
      "cnt: 0 - valLoss: 0.4066915214061737 - trainLoss: 0.3906186521053314\n",
      "cnt: 0 - valLoss: 0.40668994188308716 - trainLoss: 0.3906148374080658\n",
      "cnt: 0 - valLoss: 0.4066900610923767 - trainLoss: 0.39061102271080017\n",
      "cnt: 1 - valLoss: 0.4066886901855469 - trainLoss: 0.39060723781585693\n",
      "cnt: 0 - valLoss: 0.40668877959251404 - trainLoss: 0.3906033933162689\n",
      "cnt: 1 - valLoss: 0.4066881537437439 - trainLoss: 0.3905996084213257\n",
      "cnt: 0 - valLoss: 0.40668725967407227 - trainLoss: 0.39059582352638245\n",
      "cnt: 0 - valLoss: 0.4066861569881439 - trainLoss: 0.3905920088291168\n",
      "cnt: 0 - valLoss: 0.4066862463951111 - trainLoss: 0.3905882239341736\n",
      "cnt: 1 - valLoss: 0.40668514370918274 - trainLoss: 0.39058440923690796\n",
      "cnt: 0 - valLoss: 0.40668419003486633 - trainLoss: 0.3905806243419647\n",
      "cnt: 0 - valLoss: 0.4066839814186096 - trainLoss: 0.3905768394470215\n",
      "cnt: 0 - valLoss: 0.40668246150016785 - trainLoss: 0.39057302474975586\n",
      "cnt: 0 - valLoss: 0.4066823422908783 - trainLoss: 0.39056921005249023\n",
      "cnt: 0 - valLoss: 0.40668216347694397 - trainLoss: 0.390565425157547\n",
      "cnt: 0 - valLoss: 0.406680166721344 - trainLoss: 0.39056161046028137\n",
      "cnt: 0 - valLoss: 0.40668028593063354 - trainLoss: 0.39055779576301575\n",
      "cnt: 1 - valLoss: 0.40667974948883057 - trainLoss: 0.39055395126342773\n",
      "cnt: 0 - valLoss: 0.4066791534423828 - trainLoss: 0.3905501663684845\n",
      "cnt: 0 - valLoss: 0.4066774249076843 - trainLoss: 0.3905463218688965\n",
      "cnt: 0 - valLoss: 0.40667733550071716 - trainLoss: 0.39054250717163086\n",
      "cnt: 0 - valLoss: 0.40667644143104553 - trainLoss: 0.39053869247436523\n",
      "cnt: 0 - valLoss: 0.40667518973350525 - trainLoss: 0.3905348479747772\n",
      "cnt: 0 - valLoss: 0.406675785779953 - trainLoss: 0.3905310332775116\n",
      "cnt: 1 - valLoss: 0.4066740572452545 - trainLoss: 0.39052721858024597\n",
      "cnt: 0 - valLoss: 0.40667369961738586 - trainLoss: 0.39052340388298035\n",
      "cnt: 0 - valLoss: 0.4066722095012665 - trainLoss: 0.3905196189880371\n",
      "cnt: 0 - valLoss: 0.4066726863384247 - trainLoss: 0.3905158042907715\n",
      "cnt: 1 - valLoss: 0.4066711962223053 - trainLoss: 0.39051198959350586\n",
      "cnt: 0 - valLoss: 0.40667057037353516 - trainLoss: 0.39050817489624023\n",
      "cnt: 0 - valLoss: 0.40667062997817993 - trainLoss: 0.3905043601989746\n",
      "cnt: 1 - valLoss: 0.40666964650154114 - trainLoss: 0.390500545501709\n",
      "cnt: 0 - valLoss: 0.40666863322257996 - trainLoss: 0.39049670100212097\n",
      "cnt: 0 - valLoss: 0.40666788816452026 - trainLoss: 0.39049285650253296\n",
      "cnt: 0 - valLoss: 0.4066673815250397 - trainLoss: 0.3904891014099121\n",
      "cnt: 0 - valLoss: 0.4066663384437561 - trainLoss: 0.3904852867126465\n",
      "cnt: 0 - valLoss: 0.4066663682460785 - trainLoss: 0.39048144221305847\n",
      "cnt: 1 - valLoss: 0.406665563583374 - trainLoss: 0.39047759771347046\n",
      "cnt: 0 - valLoss: 0.4066641926765442 - trainLoss: 0.3904738128185272\n",
      "cnt: 0 - valLoss: 0.40666431188583374 - trainLoss: 0.3904699981212616\n",
      "cnt: 1 - valLoss: 0.40666311979293823 - trainLoss: 0.39046618342399597\n",
      "cnt: 0 - valLoss: 0.4066622257232666 - trainLoss: 0.39046233892440796\n",
      "cnt: 0 - valLoss: 0.40666258335113525 - trainLoss: 0.3904585540294647\n",
      "cnt: 1 - valLoss: 0.40666091442108154 - trainLoss: 0.3904547691345215\n",
      "cnt: 0 - valLoss: 0.40666085481643677 - trainLoss: 0.39045095443725586\n",
      "cnt: 0 - valLoss: 0.4066598415374756 - trainLoss: 0.39044713973999023\n",
      "cnt: 0 - valLoss: 0.40665924549102783 - trainLoss: 0.3904432952404022\n",
      "cnt: 0 - valLoss: 0.40665900707244873 - trainLoss: 0.3904394507408142\n",
      "cnt: 0 - valLoss: 0.40665775537490845 - trainLoss: 0.39043569564819336\n",
      "cnt: 0 - valLoss: 0.4066581428050995 - trainLoss: 0.39043188095092773\n",
      "cnt: 1 - valLoss: 0.406656950712204 - trainLoss: 0.3904280662536621\n",
      "cnt: 0 - valLoss: 0.4066564440727234 - trainLoss: 0.3904242515563965\n",
      "cnt: 0 - valLoss: 0.40665581822395325 - trainLoss: 0.39042046666145325\n",
      "cnt: 0 - valLoss: 0.40665528178215027 - trainLoss: 0.39041662216186523\n",
      "cnt: 0 - valLoss: 0.40665432810783386 - trainLoss: 0.3904128074645996\n",
      "cnt: 0 - valLoss: 0.4066535234451294 - trainLoss: 0.390408992767334\n",
      "cnt: 0 - valLoss: 0.40665361285209656 - trainLoss: 0.39040517807006836\n",
      "cnt: 1 - valLoss: 0.40665265917778015 - trainLoss: 0.39040136337280273\n",
      "cnt: 0 - valLoss: 0.40665194392204285 - trainLoss: 0.3903975486755371\n",
      "cnt: 0 - valLoss: 0.40665075182914734 - trainLoss: 0.3903936743736267\n",
      "cnt: 0 - valLoss: 0.40665096044540405 - trainLoss: 0.39038988947868347\n",
      "cnt: 1 - valLoss: 0.40664952993392944 - trainLoss: 0.39038607478141785\n",
      "cnt: 0 - valLoss: 0.40664952993392944 - trainLoss: 0.3903822600841522\n",
      "cnt: 0 - valLoss: 0.4066491425037384 - trainLoss: 0.3903784155845642\n",
      "cnt: 0 - valLoss: 0.4066484570503235 - trainLoss: 0.3903746008872986\n",
      "cnt: 0 - valLoss: 0.4066469669342041 - trainLoss: 0.39037078619003296\n",
      "cnt: 0 - valLoss: 0.40664708614349365 - trainLoss: 0.39036697149276733\n",
      "cnt: 1 - valLoss: 0.4066459536552429 - trainLoss: 0.3903632164001465\n",
      "cnt: 0 - valLoss: 0.4066452980041504 - trainLoss: 0.39035937190055847\n",
      "cnt: 0 - valLoss: 0.4066438674926758 - trainLoss: 0.39035552740097046\n",
      "cnt: 0 - valLoss: 0.4066444933414459 - trainLoss: 0.3903517723083496\n",
      "cnt: 1 - valLoss: 0.40664273500442505 - trainLoss: 0.3903478980064392\n",
      "cnt: 0 - valLoss: 0.4066433608531952 - trainLoss: 0.39034411311149597\n",
      "cnt: 1 - valLoss: 0.40664276480674744 - trainLoss: 0.39034032821655273\n",
      "cnt: 2 - valLoss: 0.40664148330688477 - trainLoss: 0.3903364837169647\n",
      "cnt: 0 - valLoss: 0.4066410958766937 - trainLoss: 0.3903326988220215\n",
      "cnt: 0 - valLoss: 0.40664055943489075 - trainLoss: 0.39032888412475586\n",
      "cnt: 0 - valLoss: 0.40663984417915344 - trainLoss: 0.39032506942749023\n",
      "cnt: 0 - valLoss: 0.40663909912109375 - trainLoss: 0.390321284532547\n",
      "cnt: 0 - valLoss: 0.40663835406303406 - trainLoss: 0.39031746983528137\n",
      "cnt: 0 - valLoss: 0.40663811564445496 - trainLoss: 0.39031365513801575\n",
      "cnt: 0 - valLoss: 0.4066368639469147 - trainLoss: 0.3903099000453949\n",
      "cnt: 0 - valLoss: 0.40663591027259827 - trainLoss: 0.3903060257434845\n",
      "cnt: 0 - valLoss: 0.40663620829582214 - trainLoss: 0.39030224084854126\n",
      "cnt: 1 - valLoss: 0.4066348969936371 - trainLoss: 0.390298455953598\n",
      "cnt: 0 - valLoss: 0.4066345691680908 - trainLoss: 0.3902946412563324\n",
      "cnt: 0 - valLoss: 0.4066338837146759 - trainLoss: 0.3902908265590668\n",
      "cnt: 0 - valLoss: 0.4066331684589386 - trainLoss: 0.3902870714664459\n",
      "cnt: 0 - valLoss: 0.40663251280784607 - trainLoss: 0.3902832865715027\n",
      "cnt: 0 - valLoss: 0.4066315293312073 - trainLoss: 0.39027947187423706\n",
      "cnt: 0 - valLoss: 0.4066319167613983 - trainLoss: 0.3902757465839386\n",
      "cnt: 1 - valLoss: 0.4066303074359894 - trainLoss: 0.390271931886673\n",
      "cnt: 0 - valLoss: 0.4066295325756073 - trainLoss: 0.39026814699172974\n",
      "cnt: 0 - valLoss: 0.406629741191864 - trainLoss: 0.3902643620967865\n",
      "cnt: 1 - valLoss: 0.40662848949432373 - trainLoss: 0.39026060700416565\n",
      "cnt: 0 - valLoss: 0.40662798285484314 - trainLoss: 0.3902568221092224\n",
      "cnt: 0 - valLoss: 0.406627357006073 - trainLoss: 0.3902530074119568\n",
      "cnt: 0 - valLoss: 0.4066275656223297 - trainLoss: 0.39024919271469116\n",
      "cnt: 1 - valLoss: 0.4066261649131775 - trainLoss: 0.3902454674243927\n",
      "cnt: 0 - valLoss: 0.4066254794597626 - trainLoss: 0.3902416527271271\n",
      "cnt: 0 - valLoss: 0.4066248834133148 - trainLoss: 0.39023786783218384\n",
      "cnt: 0 - valLoss: 0.40662428736686707 - trainLoss: 0.3902340531349182\n",
      "cnt: 0 - valLoss: 0.4066234827041626 - trainLoss: 0.3902302384376526\n",
      "cnt: 0 - valLoss: 0.4066227674484253 - trainLoss: 0.39022645354270935\n",
      "cnt: 0 - valLoss: 0.4066227376461029 - trainLoss: 0.3902226984500885\n",
      "cnt: 0 - valLoss: 0.4066217541694641 - trainLoss: 0.3902188837528229\n",
      "cnt: 0 - valLoss: 0.4066212475299835 - trainLoss: 0.39021509885787964\n",
      "cnt: 0 - valLoss: 0.4066203236579895 - trainLoss: 0.3902112543582916\n",
      "cnt: 0 - valLoss: 0.4066200256347656 - trainLoss: 0.3902074992656708\n",
      "cnt: 0 - valLoss: 0.4066193699836731 - trainLoss: 0.39020371437072754\n",
      "cnt: 0 - valLoss: 0.4066186249256134 - trainLoss: 0.3901998996734619\n",
      "cnt: 0 - valLoss: 0.4066186547279358 - trainLoss: 0.39019614458084106\n",
      "cnt: 1 - valLoss: 0.4066171944141388 - trainLoss: 0.3901923596858978\n",
      "cnt: 0 - valLoss: 0.40661659836769104 - trainLoss: 0.3901885449886322\n",
      "cnt: 0 - valLoss: 0.4066162109375 - trainLoss: 0.39018478989601135\n",
      "cnt: 0 - valLoss: 0.4066157639026642 - trainLoss: 0.3901810050010681\n",
      "cnt: 0 - valLoss: 0.40661460161209106 - trainLoss: 0.39017724990844727\n",
      "cnt: 0 - valLoss: 0.40661439299583435 - trainLoss: 0.39017343521118164\n",
      "cnt: 0 - valLoss: 0.4066145420074463 - trainLoss: 0.3901696503162384\n",
      "cnt: 1 - valLoss: 0.4066135883331299 - trainLoss: 0.39016589522361755\n",
      "cnt: 0 - valLoss: 0.40661221742630005 - trainLoss: 0.3901621401309967\n",
      "cnt: 0 - valLoss: 0.40661221742630005 - trainLoss: 0.39015835523605347\n",
      "cnt: 0 - valLoss: 0.4066113531589508 - trainLoss: 0.3901546001434326\n",
      "cnt: 0 - valLoss: 0.4066110849380493 - trainLoss: 0.3901508152484894\n",
      "cnt: 0 - valLoss: 0.4066099226474762 - trainLoss: 0.39014700055122375\n",
      "cnt: 0 - valLoss: 0.40661031007766724 - trainLoss: 0.3901432454586029\n",
      "cnt: 1 - valLoss: 0.4066089391708374 - trainLoss: 0.39013952016830444\n",
      "cnt: 0 - valLoss: 0.40660881996154785 - trainLoss: 0.3901357054710388\n",
      "cnt: 0 - valLoss: 0.40660718083381653 - trainLoss: 0.3901319205760956\n",
      "cnt: 0 - valLoss: 0.406607449054718 - trainLoss: 0.3901281952857971\n",
      "cnt: 1 - valLoss: 0.4066065549850464 - trainLoss: 0.3901243805885315\n",
      "cnt: 0 - valLoss: 0.4066055715084076 - trainLoss: 0.39012062549591064\n",
      "cnt: 0 - valLoss: 0.4066055119037628 - trainLoss: 0.3901168704032898\n",
      "cnt: 0 - valLoss: 0.4066040515899658 - trainLoss: 0.39011311531066895\n",
      "cnt: 0 - valLoss: 0.40660330653190613 - trainLoss: 0.3901093304157257\n",
      "cnt: 0 - valLoss: 0.40660199522972107 - trainLoss: 0.39010560512542725\n",
      "cnt: 0 - valLoss: 0.4066018760204315 - trainLoss: 0.3901018500328064\n",
      "cnt: 0 - valLoss: 0.4066006541252136 - trainLoss: 0.39009812474250793\n",
      "cnt: 0 - valLoss: 0.406599760055542 - trainLoss: 0.3900943696498871\n",
      "cnt: 0 - valLoss: 0.4065985679626465 - trainLoss: 0.39009061455726624\n",
      "cnt: 0 - valLoss: 0.40659815073013306 - trainLoss: 0.3900868594646454\n",
      "cnt: 0 - valLoss: 0.40659722685813904 - trainLoss: 0.3900831341743469\n",
      "cnt: 0 - valLoss: 0.4065960645675659 - trainLoss: 0.39007940888404846\n",
      "cnt: 0 - valLoss: 0.4065958559513092 - trainLoss: 0.3900756537914276\n",
      "cnt: 0 - valLoss: 0.4065944254398346 - trainLoss: 0.39007192850112915\n",
      "cnt: 0 - valLoss: 0.40659379959106445 - trainLoss: 0.3900681734085083\n",
      "cnt: 0 - valLoss: 0.40659260749816895 - trainLoss: 0.39006444811820984\n",
      "cnt: 0 - valLoss: 0.4065922796726227 - trainLoss: 0.3900606632232666\n",
      "cnt: 0 - valLoss: 0.4065910577774048 - trainLoss: 0.39005693793296814\n",
      "cnt: 0 - valLoss: 0.40659013390541077 - trainLoss: 0.3900531828403473\n",
      "cnt: 0 - valLoss: 0.40658989548683167 - trainLoss: 0.39004945755004883\n",
      "cnt: 0 - valLoss: 0.4065885841846466 - trainLoss: 0.39004576206207275\n",
      "cnt: 0 - valLoss: 0.40658801794052124 - trainLoss: 0.3900420069694519\n",
      "cnt: 0 - valLoss: 0.4065873622894287 - trainLoss: 0.39003828167915344\n",
      "cnt: 0 - valLoss: 0.4065861999988556 - trainLoss: 0.390034556388855\n",
      "cnt: 0 - valLoss: 0.4065859317779541 - trainLoss: 0.39003077149391174\n",
      "cnt: 0 - valLoss: 0.4065842032432556 - trainLoss: 0.3900270164012909\n",
      "cnt: 0 - valLoss: 0.40658387541770935 - trainLoss: 0.3900233209133148\n",
      "cnt: 0 - valLoss: 0.40658289194107056 - trainLoss: 0.39001959562301636\n",
      "cnt: 0 - valLoss: 0.40658289194107056 - trainLoss: 0.3900158405303955\n",
      "cnt: 0 - valLoss: 0.40658119320869446 - trainLoss: 0.39001214504241943\n",
      "cnt: 0 - valLoss: 0.40658101439476013 - trainLoss: 0.3900083899497986\n",
      "cnt: 0 - valLoss: 0.40657946467399597 - trainLoss: 0.3900046646595001\n",
      "cnt: 0 - valLoss: 0.406579852104187 - trainLoss: 0.39000096917152405\n",
      "cnt: 1 - valLoss: 0.4065784215927124 - trainLoss: 0.3899972140789032\n",
      "cnt: 0 - valLoss: 0.4065774083137512 - trainLoss: 0.38999348878860474\n",
      "cnt: 0 - valLoss: 0.40657734870910645 - trainLoss: 0.3899897634983063\n",
      "cnt: 0 - valLoss: 0.40657666325569153 - trainLoss: 0.3899860680103302\n",
      "cnt: 0 - valLoss: 0.4065757393836975 - trainLoss: 0.3899823725223541\n",
      "cnt: 0 - valLoss: 0.4065757989883423 - trainLoss: 0.38997864723205566\n",
      "cnt: 1 - valLoss: 0.4065757095813751 - trainLoss: 0.3899748921394348\n",
      "cnt: 0 - valLoss: 0.4065755307674408 - trainLoss: 0.38997116684913635\n",
      "cnt: 0 - valLoss: 0.4065741002559662 - trainLoss: 0.3899674117565155\n",
      "cnt: 0 - valLoss: 0.4065740406513214 - trainLoss: 0.38996368646621704\n",
      "cnt: 0 - valLoss: 0.406573623418808 - trainLoss: 0.3899599611759186\n",
      "cnt: 0 - valLoss: 0.4065733551979065 - trainLoss: 0.3899562656879425\n",
      "cnt: 0 - valLoss: 0.40657272934913635 - trainLoss: 0.38995251059532166\n",
      "cnt: 0 - valLoss: 0.40657249093055725 - trainLoss: 0.3899487853050232\n",
      "cnt: 0 - valLoss: 0.40657299757003784 - trainLoss: 0.38994506001472473\n",
      "cnt: 1 - valLoss: 0.4065721035003662 - trainLoss: 0.38994133472442627\n",
      "cnt: 0 - valLoss: 0.4065718948841095 - trainLoss: 0.3899376094341278\n",
      "cnt: 0 - valLoss: 0.40657201409339905 - trainLoss: 0.38993391394615173\n",
      "cnt: 1 - valLoss: 0.40657150745391846 - trainLoss: 0.3899301588535309\n",
      "cnt: 0 - valLoss: 0.4065709412097931 - trainLoss: 0.3899264335632324\n",
      "cnt: 0 - valLoss: 0.4065706729888916 - trainLoss: 0.38992276787757874\n",
      "cnt: 0 - valLoss: 0.4065706729888916 - trainLoss: 0.3899190425872803\n",
      "cnt: 0 - valLoss: 0.406570702791214 - trainLoss: 0.38991525769233704\n",
      "cnt: 1 - valLoss: 0.406570166349411 - trainLoss: 0.38991159200668335\n",
      "cnt: 0 - valLoss: 0.4065695106983185 - trainLoss: 0.3899078667163849\n",
      "cnt: 0 - valLoss: 0.4065696895122528 - trainLoss: 0.3899041414260864\n",
      "cnt: 1 - valLoss: 0.4065696895122528 - trainLoss: 0.38990044593811035\n",
      "cnt: 2 - valLoss: 0.40656906366348267 - trainLoss: 0.3898967206478119\n",
      "cnt: 0 - valLoss: 0.40656861662864685 - trainLoss: 0.3898929953575134\n",
      "cnt: 0 - valLoss: 0.4065687954425812 - trainLoss: 0.38988927006721497\n",
      "cnt: 1 - valLoss: 0.40656813979148865 - trainLoss: 0.3898855745792389\n",
      "cnt: 0 - valLoss: 0.4065678119659424 - trainLoss: 0.3898819088935852\n",
      "cnt: 0 - valLoss: 0.4065675139427185 - trainLoss: 0.38987818360328674\n",
      "cnt: 0 - valLoss: 0.40656813979148865 - trainLoss: 0.3898744285106659\n",
      "cnt: 1 - valLoss: 0.4065675735473633 - trainLoss: 0.3898707330226898\n",
      "cnt: 2 - valLoss: 0.40656712651252747 - trainLoss: 0.38986703753471375\n",
      "cnt: 0 - valLoss: 0.4065665006637573 - trainLoss: 0.38986334204673767\n",
      "cnt: 0 - valLoss: 0.4065658152103424 - trainLoss: 0.3898596167564392\n",
      "cnt: 0 - valLoss: 0.4065663814544678 - trainLoss: 0.38985589146614075\n",
      "cnt: 1 - valLoss: 0.4065658450126648 - trainLoss: 0.3898521661758423\n",
      "cnt: 2 - valLoss: 0.4065648019313812 - trainLoss: 0.3898484706878662\n",
      "cnt: 0 - valLoss: 0.4065658748149872 - trainLoss: 0.38984474539756775\n",
      "cnt: 1 - valLoss: 0.4065648913383484 - trainLoss: 0.3898410499095917\n",
      "cnt: 2 - valLoss: 0.4065641462802887 - trainLoss: 0.3898373246192932\n",
      "cnt: 0 - valLoss: 0.4065646529197693 - trainLoss: 0.3898336589336395\n",
      "cnt: 1 - valLoss: 0.40656375885009766 - trainLoss: 0.38982993364334106\n",
      "cnt: 0 - valLoss: 0.40656280517578125 - trainLoss: 0.389826238155365\n",
      "cnt: 0 - valLoss: 0.40656381845474243 - trainLoss: 0.3898225426673889\n",
      "cnt: 1 - valLoss: 0.40656211972236633 - trainLoss: 0.38981884717941284\n",
      "cnt: 0 - valLoss: 0.40656232833862305 - trainLoss: 0.38981515169143677\n",
      "cnt: 1 - valLoss: 0.40656203031539917 - trainLoss: 0.3898114860057831\n",
      "cnt: 0 - valLoss: 0.40656208992004395 - trainLoss: 0.3898077607154846\n",
      "cnt: 1 - valLoss: 0.406561017036438 - trainLoss: 0.38980409502983093\n",
      "cnt: 0 - valLoss: 0.40656208992004395 - trainLoss: 0.38980039954185486\n",
      "cnt: 1 - valLoss: 0.4065607786178589 - trainLoss: 0.38979673385620117\n",
      "cnt: 0 - valLoss: 0.4065605401992798 - trainLoss: 0.3897930085659027\n",
      "cnt: 0 - valLoss: 0.40656086802482605 - trainLoss: 0.389789342880249\n",
      "cnt: 1 - valLoss: 0.4065600335597992 - trainLoss: 0.38978564739227295\n",
      "cnt: 0 - valLoss: 0.4065598249435425 - trainLoss: 0.3897819519042969\n",
      "cnt: 0 - valLoss: 0.40655890107154846 - trainLoss: 0.3897782862186432\n",
      "cnt: 0 - valLoss: 0.4065586030483246 - trainLoss: 0.3897746205329895\n",
      "cnt: 0 - valLoss: 0.4065589904785156 - trainLoss: 0.38977089524269104\n",
      "cnt: 1 - valLoss: 0.4065582752227783 - trainLoss: 0.38976722955703735\n",
      "cnt: 0 - valLoss: 0.4065583646297455 - trainLoss: 0.3897635340690613\n",
      "cnt: 1 - valLoss: 0.40655797719955444 - trainLoss: 0.3897598385810852\n",
      "cnt: 0 - valLoss: 0.4065574109554291 - trainLoss: 0.38975614309310913\n",
      "cnt: 0 - valLoss: 0.4065566658973694 - trainLoss: 0.38975247740745544\n",
      "cnt: 0 - valLoss: 0.40655702352523804 - trainLoss: 0.389748752117157\n",
      "cnt: 1 - valLoss: 0.40655604004859924 - trainLoss: 0.3897451162338257\n",
      "cnt: 0 - valLoss: 0.40655574202537537 - trainLoss: 0.3897413909435272\n",
      "cnt: 0 - valLoss: 0.40655604004859924 - trainLoss: 0.38973769545555115\n",
      "cnt: 1 - valLoss: 0.4065544307231903 - trainLoss: 0.3897339999675751\n",
      "cnt: 0 - valLoss: 0.4065554440021515 - trainLoss: 0.389730304479599\n",
      "cnt: 1 - valLoss: 0.40655460953712463 - trainLoss: 0.3897266685962677\n",
      "cnt: 2 - valLoss: 0.4065544903278351 - trainLoss: 0.3897230327129364\n",
      "cnt: 3 - valLoss: 0.40655404329299927 - trainLoss: 0.3897193968296051\n",
      "cnt: 0 - valLoss: 0.40655434131622314 - trainLoss: 0.3897157311439514\n",
      "cnt: 1 - valLoss: 0.4065532386302948 - trainLoss: 0.3897121250629425\n",
      "cnt: 0 - valLoss: 0.4065537452697754 - trainLoss: 0.3897084593772888\n",
      "cnt: 1 - valLoss: 0.40655240416526794 - trainLoss: 0.3897048234939575\n",
      "cnt: 0 - valLoss: 0.4065519869327545 - trainLoss: 0.3897012174129486\n",
      "cnt: 0 - valLoss: 0.406552255153656 - trainLoss: 0.3896975517272949\n",
      "cnt: 1 - valLoss: 0.4065512716770172 - trainLoss: 0.3896939158439636\n",
      "cnt: 0 - valLoss: 0.40655073523521423 - trainLoss: 0.3896903097629547\n",
      "cnt: 0 - valLoss: 0.4065513610839844 - trainLoss: 0.3896866738796234\n",
      "cnt: 1 - valLoss: 0.4065503180027008 - trainLoss: 0.3896830081939697\n",
      "cnt: 0 - valLoss: 0.40654996037483215 - trainLoss: 0.3896794021129608\n",
      "cnt: 0 - valLoss: 0.4065498113632202 - trainLoss: 0.3896758258342743\n",
      "cnt: 0 - valLoss: 0.40654876828193665 - trainLoss: 0.3896722197532654\n",
      "cnt: 0 - valLoss: 0.40654778480529785 - trainLoss: 0.38966864347457886\n",
      "cnt: 0 - valLoss: 0.406547874212265 - trainLoss: 0.38966506719589233\n",
      "cnt: 1 - valLoss: 0.4065466821193695 - trainLoss: 0.3896614909172058\n",
      "cnt: 0 - valLoss: 0.40654584765434265 - trainLoss: 0.3896578848361969\n",
      "cnt: 0 - valLoss: 0.40654581785202026 - trainLoss: 0.3896543085575104\n",
      "cnt: 0 - valLoss: 0.40654492378234863 - trainLoss: 0.38965073227882385\n",
      "cnt: 0 - valLoss: 0.4065438508987427 - trainLoss: 0.38964715600013733\n",
      "cnt: 0 - valLoss: 0.40654391050338745 - trainLoss: 0.3896435499191284\n",
      "cnt: 1 - valLoss: 0.406543105840683 - trainLoss: 0.3896399736404419\n",
      "cnt: 0 - valLoss: 0.40654289722442627 - trainLoss: 0.38963642716407776\n",
      "cnt: 0 - valLoss: 0.4065411686897278 - trainLoss: 0.38963285088539124\n",
      "cnt: 0 - valLoss: 0.406541109085083 - trainLoss: 0.3896292746067047\n",
      "cnt: 0 - valLoss: 0.4065406024456024 - trainLoss: 0.38962575793266296\n",
      "cnt: 0 - valLoss: 0.40653929114341736 - trainLoss: 0.38962218165397644\n",
      "cnt: 0 - valLoss: 0.4065392315387726 - trainLoss: 0.3896186649799347\n",
      "cnt: 0 - valLoss: 0.40653908252716064 - trainLoss: 0.3896150290966034\n",
      "cnt: 0 - valLoss: 0.40653830766677856 - trainLoss: 0.38961154222488403\n",
      "cnt: 0 - valLoss: 0.4065379202365875 - trainLoss: 0.3896079361438751\n",
      "cnt: 0 - valLoss: 0.4065370261669159 - trainLoss: 0.389604389667511\n",
      "cnt: 0 - valLoss: 0.40653717517852783 - trainLoss: 0.38960087299346924\n",
      "cnt: 1 - valLoss: 0.40653568506240845 - trainLoss: 0.3895973265171051\n",
      "cnt: 0 - valLoss: 0.4065355360507965 - trainLoss: 0.3895937502384186\n",
      "cnt: 0 - valLoss: 0.40653544664382935 - trainLoss: 0.38959020376205444\n",
      "cnt: 0 - valLoss: 0.4065340459346771 - trainLoss: 0.3895866572856903\n",
      "cnt: 0 - valLoss: 0.4065342843532562 - trainLoss: 0.38958311080932617\n",
      "cnt: 1 - valLoss: 0.4065331220626831 - trainLoss: 0.38957956433296204\n",
      "cnt: 0 - valLoss: 0.4065323770046234 - trainLoss: 0.3895760178565979\n",
      "cnt: 0 - valLoss: 0.406532883644104 - trainLoss: 0.3895724415779114\n",
      "cnt: 1 - valLoss: 0.40653130412101746 - trainLoss: 0.38956892490386963\n",
      "cnt: 0 - valLoss: 0.4065307378768921 - trainLoss: 0.3895654082298279\n",
      "cnt: 0 - valLoss: 0.40653088688850403 - trainLoss: 0.38956183195114136\n",
      "cnt: 1 - valLoss: 0.40652933716773987 - trainLoss: 0.3895582854747772\n",
      "cnt: 0 - valLoss: 0.40652912855148315 - trainLoss: 0.3895547389984131\n",
      "cnt: 0 - valLoss: 0.40652865171432495 - trainLoss: 0.38955119252204895\n",
      "cnt: 0 - valLoss: 0.4065280854701996 - trainLoss: 0.3895476758480072\n",
      "cnt: 0 - valLoss: 0.40652695298194885 - trainLoss: 0.38954412937164307\n",
      "cnt: 0 - valLoss: 0.4065277874469757 - trainLoss: 0.38954055309295654\n",
      "cnt: 1 - valLoss: 0.4065261781215668 - trainLoss: 0.3895370364189148\n",
      "cnt: 0 - valLoss: 0.40652570128440857 - trainLoss: 0.38953346014022827\n",
      "cnt: 0 - valLoss: 0.4065251648426056 - trainLoss: 0.3895299732685089\n",
      "cnt: 0 - valLoss: 0.4065249562263489 - trainLoss: 0.3895264267921448\n",
      "cnt: 0 - valLoss: 0.40652382373809814 - trainLoss: 0.38952288031578064\n",
      "cnt: 0 - valLoss: 0.40652361512184143 - trainLoss: 0.3895193040370941\n",
      "cnt: 0 - valLoss: 0.40652304887771606 - trainLoss: 0.38951581716537476\n",
      "cnt: 0 - valLoss: 0.40652191638946533 - trainLoss: 0.3895122706890106\n",
      "cnt: 0 - valLoss: 0.4065222442150116 - trainLoss: 0.38950875401496887\n",
      "cnt: 1 - valLoss: 0.4065209925174713 - trainLoss: 0.38950520753860474\n",
      "cnt: 0 - valLoss: 0.406520813703537 - trainLoss: 0.3895016312599182\n",
      "cnt: 0 - valLoss: 0.40652036666870117 - trainLoss: 0.38949811458587646\n",
      "cnt: 0 - valLoss: 0.4065195918083191 - trainLoss: 0.38949456810951233\n",
      "cnt: 0 - valLoss: 0.40651848912239075 - trainLoss: 0.3894910514354706\n",
      "cnt: 0 - valLoss: 0.40651896595954895 - trainLoss: 0.38948753476142883\n",
      "cnt: 1 - valLoss: 0.4065176844596863 - trainLoss: 0.3894840180873871\n",
      "cnt: 0 - valLoss: 0.4065176248550415 - trainLoss: 0.38948047161102295\n",
      "cnt: 0 - valLoss: 0.40651702880859375 - trainLoss: 0.3894769847393036\n",
      "cnt: 0 - valLoss: 0.40651649236679077 - trainLoss: 0.38947343826293945\n",
      "cnt: 0 - valLoss: 0.40651559829711914 - trainLoss: 0.3894698917865753\n",
      "cnt: 0 - valLoss: 0.40651601552963257 - trainLoss: 0.3894663453102112\n",
      "cnt: 1 - valLoss: 0.4065144658088684 - trainLoss: 0.38946279883384705\n",
      "cnt: 0 - valLoss: 0.40651410818099976 - trainLoss: 0.3894592821598053\n",
      "cnt: 0 - valLoss: 0.40651434659957886 - trainLoss: 0.38945579528808594\n",
      "cnt: 1 - valLoss: 0.406512588262558 - trainLoss: 0.3894522488117218\n",
      "cnt: 0 - valLoss: 0.40651294589042664 - trainLoss: 0.38944873213768005\n",
      "cnt: 1 - valLoss: 0.4065125584602356 - trainLoss: 0.3894451856613159\n",
      "cnt: 0 - valLoss: 0.406511515378952 - trainLoss: 0.38944169878959656\n",
      "cnt: 0 - valLoss: 0.40651121735572815 - trainLoss: 0.3894381821155548\n",
      "cnt: 0 - valLoss: 0.4065111577510834 - trainLoss: 0.3894346356391907\n",
      "cnt: 0 - valLoss: 0.406509667634964 - trainLoss: 0.38943108916282654\n",
      "cnt: 0 - valLoss: 0.40650996565818787 - trainLoss: 0.3894276022911072\n",
      "cnt: 1 - valLoss: 0.4065086543560028 - trainLoss: 0.38942408561706543\n",
      "cnt: 0 - valLoss: 0.40650883316993713 - trainLoss: 0.3894205689430237\n",
      "cnt: 1 - valLoss: 0.40650758147239685 - trainLoss: 0.38941705226898193\n",
      "cnt: 0 - valLoss: 0.4065072536468506 - trainLoss: 0.3894135057926178\n",
      "cnt: 0 - valLoss: 0.40650641918182373 - trainLoss: 0.38941001892089844\n",
      "cnt: 0 - valLoss: 0.4065062701702118 - trainLoss: 0.3894065022468567\n",
      "cnt: 0 - valLoss: 0.40650516748428345 - trainLoss: 0.38940298557281494\n",
      "cnt: 0 - valLoss: 0.4065054655075073 - trainLoss: 0.3893994688987732\n",
      "cnt: 1 - valLoss: 0.40650367736816406 - trainLoss: 0.38939598202705383\n",
      "cnt: 0 - valLoss: 0.40650320053100586 - trainLoss: 0.3893924355506897\n",
      "cnt: 0 - valLoss: 0.40650367736816406 - trainLoss: 0.38938894867897034\n",
      "cnt: 1 - valLoss: 0.4065016806125641 - trainLoss: 0.3893854320049286\n",
      "cnt: 0 - valLoss: 0.4065020978450775 - trainLoss: 0.38938191533088684\n",
      "cnt: 1 - valLoss: 0.4065004885196686 - trainLoss: 0.3893784284591675\n",
      "cnt: 0 - valLoss: 0.4065002501010895 - trainLoss: 0.38937491178512573\n",
      "cnt: 0 - valLoss: 0.40649911761283875 - trainLoss: 0.389371395111084\n",
      "cnt: 0 - valLoss: 0.40649956464767456 - trainLoss: 0.389367938041687\n",
      "cnt: 1 - valLoss: 0.4064975678920746 - trainLoss: 0.38936445116996765\n",
      "cnt: 0 - valLoss: 0.4064978063106537 - trainLoss: 0.3893609344959259\n",
      "cnt: 1 - valLoss: 0.4064968526363373 - trainLoss: 0.38935747742652893\n",
      "cnt: 0 - valLoss: 0.40649649500846863 - trainLoss: 0.3893539607524872\n",
      "cnt: 0 - valLoss: 0.40649500489234924 - trainLoss: 0.3893505036830902\n",
      "cnt: 0 - valLoss: 0.4064956605434418 - trainLoss: 0.38934701681137085\n",
      "cnt: 1 - valLoss: 0.40649378299713135 - trainLoss: 0.3893435299396515\n",
      "cnt: 0 - valLoss: 0.4064933955669403 - trainLoss: 0.38934004306793213\n",
      "cnt: 0 - valLoss: 0.40649324655532837 - trainLoss: 0.38933655619621277\n",
      "cnt: 0 - valLoss: 0.40649276971817017 - trainLoss: 0.3893330991268158\n",
      "cnt: 0 - valLoss: 0.40649184584617615 - trainLoss: 0.3893296420574188\n",
      "cnt: 0 - valLoss: 0.4064907729625702 - trainLoss: 0.3893260955810547\n",
      "cnt: 0 - valLoss: 0.4064914584159851 - trainLoss: 0.3893226087093353\n",
      "cnt: 1 - valLoss: 0.4064895212650299 - trainLoss: 0.38931921124458313\n",
      "cnt: 0 - valLoss: 0.406488835811615 - trainLoss: 0.38931572437286377\n",
      "cnt: 0 - valLoss: 0.40648823976516724 - trainLoss: 0.3893122673034668\n",
      "cnt: 0 - valLoss: 0.4064878821372986 - trainLoss: 0.38930878043174744\n",
      "cnt: 0 - valLoss: 0.406486839056015 - trainLoss: 0.38930532336235046\n",
      "cnt: 0 - valLoss: 0.4064866602420807 - trainLoss: 0.3893018364906311\n",
      "cnt: 0 - valLoss: 0.4064861834049225 - trainLoss: 0.38929837942123413\n",
      "cnt: 0 - valLoss: 0.4064852297306061 - trainLoss: 0.3892948627471924\n",
      "cnt: 0 - valLoss: 0.406484454870224 - trainLoss: 0.3892914354801178\n",
      "cnt: 0 - valLoss: 0.4064843952655792 - trainLoss: 0.38928794860839844\n",
      "cnt: 0 - valLoss: 0.4064837396144867 - trainLoss: 0.38928449153900146\n",
      "cnt: 0 - valLoss: 0.406482994556427 - trainLoss: 0.3892810642719269\n",
      "cnt: 0 - valLoss: 0.4064824879169464 - trainLoss: 0.38927754759788513\n",
      "cnt: 0 - valLoss: 0.4064815044403076 - trainLoss: 0.38927412033081055\n",
      "cnt: 0 - valLoss: 0.40648117661476135 - trainLoss: 0.3892706334590912\n",
      "cnt: 0 - valLoss: 0.4064806401729584 - trainLoss: 0.3892672061920166\n",
      "cnt: 0 - valLoss: 0.4064793884754181 - trainLoss: 0.38926371932029724\n",
      "cnt: 0 - valLoss: 0.4064787030220032 - trainLoss: 0.38926026225090027\n",
      "cnt: 0 - valLoss: 0.40647950768470764 - trainLoss: 0.3892568051815033\n",
      "cnt: 1 - valLoss: 0.40647825598716736 - trainLoss: 0.38925331830978394\n",
      "cnt: 0 - valLoss: 0.40647730231285095 - trainLoss: 0.38924989104270935\n",
      "cnt: 0 - valLoss: 0.40647658705711365 - trainLoss: 0.3892464339733124\n",
      "cnt: 0 - valLoss: 0.40647733211517334 - trainLoss: 0.389242947101593\n",
      "cnt: 1 - valLoss: 0.4064754247665405 - trainLoss: 0.38923951983451843\n",
      "cnt: 0 - valLoss: 0.40647417306900024 - trainLoss: 0.3892360031604767\n",
      "cnt: 0 - valLoss: 0.40647509694099426 - trainLoss: 0.3892326354980469\n",
      "cnt: 1 - valLoss: 0.40647372603416443 - trainLoss: 0.3892291486263275\n",
      "cnt: 0 - valLoss: 0.406472772359848 - trainLoss: 0.38922569155693054\n",
      "cnt: 0 - valLoss: 0.4064730405807495 - trainLoss: 0.38922223448753357\n",
      "cnt: 1 - valLoss: 0.40647172927856445 - trainLoss: 0.389218807220459\n",
      "cnt: 0 - valLoss: 0.4064719080924988 - trainLoss: 0.3892153799533844\n",
      "cnt: 1 - valLoss: 0.4064704179763794 - trainLoss: 0.38921189308166504\n",
      "cnt: 0 - valLoss: 0.4064704179763794 - trainLoss: 0.38920846581459045\n",
      "cnt: 0 - valLoss: 0.4064697325229645 - trainLoss: 0.3892050087451935\n",
      "cnt: 0 - valLoss: 0.4064689576625824 - trainLoss: 0.3892015516757965\n",
      "cnt: 0 - valLoss: 0.4064687192440033 - trainLoss: 0.38919809460639954\n",
      "cnt: 0 - valLoss: 0.4064677357673645 - trainLoss: 0.38919466733932495\n",
      "cnt: 0 - valLoss: 0.40646734833717346 - trainLoss: 0.389191210269928\n",
      "cnt: 0 - valLoss: 0.40646788477897644 - trainLoss: 0.3891877830028534\n",
      "cnt: 1 - valLoss: 0.4064662456512451 - trainLoss: 0.3891843557357788\n",
      "cnt: 0 - valLoss: 0.4064655303955078 - trainLoss: 0.38918089866638184\n",
      "cnt: 0 - valLoss: 0.4064647853374481 - trainLoss: 0.38917747139930725\n",
      "cnt: 0 - valLoss: 0.4064648747444153 - trainLoss: 0.3891739845275879\n",
      "cnt: 1 - valLoss: 0.4064633846282959 - trainLoss: 0.3891705870628357\n",
      "cnt: 0 - valLoss: 0.40646350383758545 - trainLoss: 0.3891671597957611\n",
      "cnt: 1 - valLoss: 0.4064633250236511 - trainLoss: 0.3891637325286865\n",
      "cnt: 0 - valLoss: 0.4064621925354004 - trainLoss: 0.38916024565696716\n",
      "cnt: 0 - valLoss: 0.4064617455005646 - trainLoss: 0.3891568183898926\n",
      "cnt: 0 - valLoss: 0.4064611792564392 - trainLoss: 0.3891533613204956\n",
      "cnt: 0 - valLoss: 0.4064609706401825 - trainLoss: 0.38914990425109863\n",
      "cnt: 0 - valLoss: 0.4064595103263855 - trainLoss: 0.3891465365886688\n",
      "cnt: 0 - valLoss: 0.4064595103263855 - trainLoss: 0.38914304971694946\n",
      "cnt: 0 - valLoss: 0.4064587950706482 - trainLoss: 0.38913965225219727\n",
      "cnt: 0 - valLoss: 0.40645813941955566 - trainLoss: 0.3891362249851227\n",
      "cnt: 0 - valLoss: 0.4064575135707855 - trainLoss: 0.3891327679157257\n",
      "cnt: 0 - valLoss: 0.4064578115940094 - trainLoss: 0.3891293406486511\n",
      "cnt: 1 - valLoss: 0.4064570665359497 - trainLoss: 0.3891259431838989\n",
      "cnt: 0 - valLoss: 0.40645602345466614 - trainLoss: 0.38912245631217957\n",
      "cnt: 0 - valLoss: 0.40645572543144226 - trainLoss: 0.389119029045105\n",
      "cnt: 0 - valLoss: 0.40645459294319153 - trainLoss: 0.38911566138267517\n",
      "cnt: 0 - valLoss: 0.40645456314086914 - trainLoss: 0.3891121745109558\n",
      "cnt: 0 - valLoss: 0.40645405650138855 - trainLoss: 0.3891087472438812\n",
      "cnt: 0 - valLoss: 0.40645352005958557 - trainLoss: 0.38910531997680664\n",
      "cnt: 0 - valLoss: 0.40645289421081543 - trainLoss: 0.38910192251205444\n",
      "cnt: 0 - valLoss: 0.4064521789550781 - trainLoss: 0.38909849524497986\n",
      "cnt: 0 - valLoss: 0.4064520299434662 - trainLoss: 0.3890950679779053\n",
      "cnt: 0 - valLoss: 0.40645086765289307 - trainLoss: 0.3890916407108307\n",
      "cnt: 0 - valLoss: 0.4064503014087677 - trainLoss: 0.3890881836414337\n",
      "cnt: 0 - valLoss: 0.40644997358322144 - trainLoss: 0.38908475637435913\n",
      "cnt: 0 - valLoss: 0.40645086765289307 - trainLoss: 0.38908135890960693\n",
      "cnt: 1 - valLoss: 0.4064490497112274 - trainLoss: 0.38907793164253235\n",
      "cnt: 0 - valLoss: 0.406448096036911 - trainLoss: 0.38907450437545776\n",
      "cnt: 0 - valLoss: 0.4064483940601349 - trainLoss: 0.3890710771083832\n",
      "cnt: 1 - valLoss: 0.40644773840904236 - trainLoss: 0.389067679643631\n",
      "cnt: 0 - valLoss: 0.40644654631614685 - trainLoss: 0.3890642523765564\n",
      "cnt: 0 - valLoss: 0.4064466953277588 - trainLoss: 0.3890608251094818\n",
      "cnt: 1 - valLoss: 0.4064464271068573 - trainLoss: 0.389057457447052\n",
      "cnt: 0 - valLoss: 0.40644457936286926 - trainLoss: 0.38905397057533264\n",
      "cnt: 0 - valLoss: 0.40644577145576477 - trainLoss: 0.38905060291290283\n",
      "cnt: 1 - valLoss: 0.4064445495605469 - trainLoss: 0.38904717564582825\n",
      "cnt: 0 - valLoss: 0.406445175409317 - trainLoss: 0.38904374837875366\n",
      "cnt: 1 - valLoss: 0.40644359588623047 - trainLoss: 0.38904038071632385\n",
      "cnt: 0 - valLoss: 0.4064428508281708 - trainLoss: 0.3890369236469269\n",
      "cnt: 0 - valLoss: 0.40644386410713196 - trainLoss: 0.38903355598449707\n",
      "cnt: 1 - valLoss: 0.40644383430480957 - trainLoss: 0.3890301585197449\n",
      "cnt: 2 - valLoss: 0.40644195675849915 - trainLoss: 0.3890267312526703\n",
      "cnt: 0 - valLoss: 0.40644147992134094 - trainLoss: 0.3890233039855957\n",
      "cnt: 0 - valLoss: 0.40644189715385437 - trainLoss: 0.3890199065208435\n",
      "cnt: 1 - valLoss: 0.4064406752586365 - trainLoss: 0.3890164792537689\n",
      "cnt: 0 - valLoss: 0.40644022822380066 - trainLoss: 0.3890131115913391\n",
      "cnt: 0 - valLoss: 0.406440794467926 - trainLoss: 0.3890097141265869\n",
      "cnt: 1 - valLoss: 0.40644052624702454 - trainLoss: 0.38900628685951233\n",
      "cnt: 2 - valLoss: 0.40643900632858276 - trainLoss: 0.38900285959243774\n",
      "cnt: 0 - valLoss: 0.4064383804798126 - trainLoss: 0.38899949193000793\n",
      "cnt: 0 - valLoss: 0.40643760561943054 - trainLoss: 0.38899609446525574\n",
      "cnt: 0 - valLoss: 0.4064386487007141 - trainLoss: 0.38899266719818115\n",
      "cnt: 1 - valLoss: 0.4064379036426544 - trainLoss: 0.38898932933807373\n",
      "cnt: 2 - valLoss: 0.40643709897994995 - trainLoss: 0.38898587226867676\n",
      "cnt: 0 - valLoss: 0.4064360558986664 - trainLoss: 0.38898253440856934\n",
      "cnt: 0 - valLoss: 0.4064355492591858 - trainLoss: 0.38897910714149475\n",
      "cnt: 0 - valLoss: 0.4064359664916992 - trainLoss: 0.38897573947906494\n",
      "cnt: 1 - valLoss: 0.4064355194568634 - trainLoss: 0.38897231221199036\n",
      "cnt: 0 - valLoss: 0.40643438696861267 - trainLoss: 0.38896891474723816\n",
      "cnt: 0 - valLoss: 0.40643423795700073 - trainLoss: 0.38896554708480835\n",
      "cnt: 0 - valLoss: 0.40643391013145447 - trainLoss: 0.38896211981773376\n",
      "cnt: 0 - valLoss: 0.4064328670501709 - trainLoss: 0.38895872235298157\n",
      "cnt: 0 - valLoss: 0.4064328670501709 - trainLoss: 0.38895532488822937\n",
      "cnt: 0 - valLoss: 0.406432181596756 - trainLoss: 0.38895201683044434\n",
      "cnt: 0 - valLoss: 0.40643200278282166 - trainLoss: 0.38894855976104736\n",
      "cnt: 0 - valLoss: 0.40643274784088135 - trainLoss: 0.38894516229629517\n",
      "cnt: 1 - valLoss: 0.4064306914806366 - trainLoss: 0.38894176483154297\n",
      "cnt: 0 - valLoss: 0.4064309895038605 - trainLoss: 0.38893839716911316\n",
      "cnt: 1 - valLoss: 0.406430184841156 - trainLoss: 0.38893499970436096\n",
      "cnt: 0 - valLoss: 0.4064299464225769 - trainLoss: 0.38893160223960876\n",
      "cnt: 0 - valLoss: 0.40642908215522766 - trainLoss: 0.38892820477485657\n",
      "cnt: 0 - valLoss: 0.4064299166202545 - trainLoss: 0.38892480731010437\n",
      "cnt: 1 - valLoss: 0.40642863512039185 - trainLoss: 0.38892149925231934\n",
      "cnt: 0 - valLoss: 0.4064280688762665 - trainLoss: 0.38891804218292236\n",
      "cnt: 0 - valLoss: 0.40642794966697693 - trainLoss: 0.38891473412513733\n",
      "cnt: 0 - valLoss: 0.40642687678337097 - trainLoss: 0.38891130685806274\n",
      "cnt: 0 - valLoss: 0.40642672777175903 - trainLoss: 0.38890790939331055\n",
      "cnt: 0 - valLoss: 0.4064265191555023 - trainLoss: 0.38890451192855835\n",
      "cnt: 0 - valLoss: 0.4064260423183441 - trainLoss: 0.38890114426612854\n",
      "cnt: 0 - valLoss: 0.4064257740974426 - trainLoss: 0.38889777660369873\n",
      "cnt: 0 - valLoss: 0.4064250588417053 - trainLoss: 0.3888944089412689\n",
      "cnt: 0 - valLoss: 0.40642404556274414 - trainLoss: 0.3888910114765167\n",
      "cnt: 0 - valLoss: 0.4064251482486725 - trainLoss: 0.3888876438140869\n",
      "cnt: 1 - valLoss: 0.40642350912094116 - trainLoss: 0.3888842761516571\n",
      "cnt: 0 - valLoss: 0.4064238965511322 - trainLoss: 0.3888809084892273\n",
      "cnt: 1 - valLoss: 0.40642204880714417 - trainLoss: 0.3888775408267975\n",
      "cnt: 0 - valLoss: 0.40642377734184265 - trainLoss: 0.3888741433620453\n",
      "cnt: 1 - valLoss: 0.40642333030700684 - trainLoss: 0.3888707756996155\n",
      "cnt: 2 - valLoss: 0.4064214825630188 - trainLoss: 0.38886740803718567\n",
      "cnt: 0 - valLoss: 0.40642157196998596 - trainLoss: 0.38886401057243347\n",
      "cnt: 1 - valLoss: 0.40642020106315613 - trainLoss: 0.38886064291000366\n",
      "cnt: 0 - valLoss: 0.40642157196998596 - trainLoss: 0.38885727524757385\n",
      "cnt: 1 - valLoss: 0.4064207673072815 - trainLoss: 0.38885390758514404\n",
      "cnt: 2 - valLoss: 0.406419962644577 - trainLoss: 0.38885053992271423\n",
      "cnt: 0 - valLoss: 0.40641918778419495 - trainLoss: 0.38884714245796204\n",
      "cnt: 0 - valLoss: 0.40641921758651733 - trainLoss: 0.3888437747955322\n",
      "cnt: 1 - valLoss: 0.40641841292381287 - trainLoss: 0.3888404369354248\n",
      "cnt: 0 - valLoss: 0.40641799569129944 - trainLoss: 0.388837069272995\n",
      "cnt: 0 - valLoss: 0.4064180850982666 - trainLoss: 0.3888337016105652\n",
      "cnt: 1 - valLoss: 0.40641698241233826 - trainLoss: 0.388830304145813\n",
      "cnt: 0 - valLoss: 0.4064178764820099 - trainLoss: 0.38882696628570557\n",
      "cnt: 1 - valLoss: 0.4064168334007263 - trainLoss: 0.38882359862327576\n",
      "cnt: 0 - valLoss: 0.4064160883426666 - trainLoss: 0.38882023096084595\n",
      "cnt: 0 - valLoss: 0.40641525387763977 - trainLoss: 0.38881686329841614\n",
      "cnt: 0 - valLoss: 0.4064149856567383 - trainLoss: 0.38881349563598633\n",
      "cnt: 0 - valLoss: 0.40641507506370544 - trainLoss: 0.3888101577758789\n",
      "cnt: 1 - valLoss: 0.40641486644744873 - trainLoss: 0.3888067901134491\n",
      "cnt: 0 - valLoss: 0.40641406178474426 - trainLoss: 0.3888034224510193\n",
      "cnt: 0 - valLoss: 0.406412810087204 - trainLoss: 0.3888000547885895\n",
      "cnt: 0 - valLoss: 0.4064147472381592 - trainLoss: 0.38879668712615967\n",
      "cnt: 1 - valLoss: 0.40641316771507263 - trainLoss: 0.38879334926605225\n",
      "cnt: 2 - valLoss: 0.40641269087791443 - trainLoss: 0.3887900412082672\n",
      "cnt: 0 - valLoss: 0.4064112901687622 - trainLoss: 0.3887866139411926\n",
      "cnt: 0 - valLoss: 0.4064129889011383 - trainLoss: 0.3887832462787628\n",
      "cnt: 1 - valLoss: 0.4064112603664398 - trainLoss: 0.3887799084186554\n",
      "cnt: 0 - valLoss: 0.4064112603664398 - trainLoss: 0.388776570558548\n",
      "cnt: 0 - valLoss: 0.40640994906425476 - trainLoss: 0.38877320289611816\n",
      "cnt: 0 - valLoss: 0.40640994906425476 - trainLoss: 0.38876983523368835\n",
      "cnt: 0 - valLoss: 0.40640994906425476 - trainLoss: 0.38876649737358093\n",
      "cnt: 0 - valLoss: 0.4064098000526428 - trainLoss: 0.3887631893157959\n",
      "cnt: 0 - valLoss: 0.4064088463783264 - trainLoss: 0.3887598216533661\n",
      "cnt: 0 - valLoss: 0.40640830993652344 - trainLoss: 0.3887564241886139\n",
      "cnt: 0 - valLoss: 0.40640953183174133 - trainLoss: 0.38875311613082886\n",
      "cnt: 1 - valLoss: 0.40640851855278015 - trainLoss: 0.38874974846839905\n",
      "cnt: 2 - valLoss: 0.40640756487846375 - trainLoss: 0.388746440410614\n",
      "cnt: 0 - valLoss: 0.40640684962272644 - trainLoss: 0.3887430429458618\n",
      "cnt: 0 - valLoss: 0.40640750527381897 - trainLoss: 0.3887397050857544\n",
      "cnt: 1 - valLoss: 0.406406044960022 - trainLoss: 0.3887363374233246\n",
      "cnt: 0 - valLoss: 0.40640607476234436 - trainLoss: 0.38873299956321716\n",
      "cnt: 1 - valLoss: 0.4064062237739563 - trainLoss: 0.38872969150543213\n",
      "cnt: 2 - valLoss: 0.4064048230648041 - trainLoss: 0.38872629404067993\n",
      "cnt: 0 - valLoss: 0.406406432390213 - trainLoss: 0.3887229561805725\n",
      "cnt: 1 - valLoss: 0.4064047634601593 - trainLoss: 0.3887196481227875\n",
      "cnt: 0 - valLoss: 0.40640488266944885 - trainLoss: 0.38871631026268005\n",
      "cnt: 1 - valLoss: 0.40640345215797424 - trainLoss: 0.38871294260025024\n",
      "cnt: 0 - valLoss: 0.4064040184020996 - trainLoss: 0.3887096047401428\n",
      "cnt: 1 - valLoss: 0.4064030945301056 - trainLoss: 0.3887062668800354\n",
      "cnt: 0 - valLoss: 0.4064023792743683 - trainLoss: 0.3887028992176056\n",
      "cnt: 0 - valLoss: 0.40640363097190857 - trainLoss: 0.38869959115982056\n",
      "cnt: 1 - valLoss: 0.4064023494720459 - trainLoss: 0.38869622349739075\n",
      "cnt: 0 - valLoss: 0.4064018428325653 - trainLoss: 0.3886929154396057\n",
      "cnt: 0 - valLoss: 0.4064011573791504 - trainLoss: 0.3886895477771759\n",
      "cnt: 0 - valLoss: 0.4064019024372101 - trainLoss: 0.38868623971939087\n",
      "cnt: 1 - valLoss: 0.406400591135025 - trainLoss: 0.38868290185928345\n",
      "cnt: 0 - valLoss: 0.40640029311180115 - trainLoss: 0.388679563999176\n",
      "cnt: 0 - valLoss: 0.40639984607696533 - trainLoss: 0.3886762261390686\n",
      "cnt: 0 - valLoss: 0.4064001142978668 - trainLoss: 0.38867294788360596\n",
      "cnt: 1 - valLoss: 0.40640053153038025 - trainLoss: 0.38866958022117615\n",
      "cnt: 2 - valLoss: 0.40640008449554443 - trainLoss: 0.3886662423610687\n",
      "cnt: 3 - valLoss: 0.4063982367515564 - trainLoss: 0.3886629343032837\n",
      "cnt: 0 - valLoss: 0.406398743391037 - trainLoss: 0.3886595666408539\n",
      "cnt: 1 - valLoss: 0.40639835596084595 - trainLoss: 0.38865628838539124\n",
      "cnt: 2 - valLoss: 0.40639790892601013 - trainLoss: 0.3886529207229614\n",
      "cnt: 0 - valLoss: 0.40639686584472656 - trainLoss: 0.3886496126651764\n",
      "cnt: 0 - valLoss: 0.4063982367515564 - trainLoss: 0.38864630460739136\n",
      "cnt: 1 - valLoss: 0.40639790892601013 - trainLoss: 0.38864296674728394\n",
      "cnt: 2 - valLoss: 0.4063960015773773 - trainLoss: 0.3886396586894989\n",
      "cnt: 0 - valLoss: 0.4063960015773773 - trainLoss: 0.3886363208293915\n",
      "cnt: 0 - valLoss: 0.4063960015773773 - trainLoss: 0.38863298296928406\n",
      "cnt: 0 - valLoss: 0.4063953757286072 - trainLoss: 0.388629674911499\n",
      "cnt: 0 - valLoss: 0.4063943028450012 - trainLoss: 0.3886263966560364\n",
      "cnt: 0 - valLoss: 0.4063955247402191 - trainLoss: 0.38862308859825134\n",
      "cnt: 1 - valLoss: 0.40639492869377136 - trainLoss: 0.3886197507381439\n",
      "cnt: 2 - valLoss: 0.40639448165893555 - trainLoss: 0.3886164128780365\n",
      "cnt: 3 - valLoss: 0.40639248490333557 - trainLoss: 0.3886130750179291\n",
      "cnt: 0 - valLoss: 0.40639397501945496 - trainLoss: 0.3886098265647888\n",
      "cnt: 1 - valLoss: 0.4063926637172699 - trainLoss: 0.3886064887046814\n",
      "cnt: 2 - valLoss: 0.4063933789730072 - trainLoss: 0.388603150844574\n",
      "cnt: 3 - valLoss: 0.4063926935195923 - trainLoss: 0.38859987258911133\n",
      "cnt: 4 - valLoss: 0.40639156103134155 - trainLoss: 0.3885965645313263\n",
      "cnt: 0 - valLoss: 0.4063924551010132 - trainLoss: 0.38859322667121887\n",
      "cnt: 1 - valLoss: 0.4063916504383087 - trainLoss: 0.38858991861343384\n",
      "cnt: 2 - valLoss: 0.40639054775238037 - trainLoss: 0.3885865807533264\n",
      "cnt: 0 - valLoss: 0.4063923954963684 - trainLoss: 0.38858330249786377\n",
      "cnt: 1 - valLoss: 0.406390905380249 - trainLoss: 0.3885800242424011\n",
      "cnt: 2 - valLoss: 0.40639054775238037 - trainLoss: 0.3885766863822937\n",
      "cnt: 0 - valLoss: 0.4063892662525177 - trainLoss: 0.38857337832450867\n",
      "cnt: 0 - valLoss: 0.40639010071754456 - trainLoss: 0.38857007026672363\n",
      "cnt: 1 - valLoss: 0.4063897728919983 - trainLoss: 0.3885667622089386\n",
      "cnt: 2 - valLoss: 0.4063892662525177 - trainLoss: 0.38856348395347595\n",
      "cnt: 0 - valLoss: 0.40638864040374756 - trainLoss: 0.38856014609336853\n",
      "cnt: 0 - valLoss: 0.4063892662525177 - trainLoss: 0.3885568082332611\n",
      "cnt: 1 - valLoss: 0.40638938546180725 - trainLoss: 0.38855358958244324\n",
      "cnt: 2 - valLoss: 0.40638822317123413 - trainLoss: 0.3885502815246582\n",
      "cnt: 0 - valLoss: 0.4063875675201416 - trainLoss: 0.38854697346687317\n",
      "cnt: 0 - valLoss: 0.4063865542411804 - trainLoss: 0.38854360580444336\n",
      "cnt: 0 - valLoss: 0.40638789534568787 - trainLoss: 0.3885403573513031\n",
      "cnt: 1 - valLoss: 0.40638601779937744 - trainLoss: 0.38853707909584045\n",
      "cnt: 0 - valLoss: 0.40638697147369385 - trainLoss: 0.38853371143341064\n",
      "cnt: 1 - valLoss: 0.4063854515552521 - trainLoss: 0.388530433177948\n",
      "cnt: 0 - valLoss: 0.4063854217529297 - trainLoss: 0.38852712512016296\n",
      "cnt: 0 - valLoss: 0.4063863754272461 - trainLoss: 0.3885238468647003\n",
      "cnt: 1 - valLoss: 0.40638551115989685 - trainLoss: 0.38852056860923767\n",
      "cnt: 2 - valLoss: 0.40638497471809387 - trainLoss: 0.38851723074913025\n",
      "cnt: 0 - valLoss: 0.4063841998577118 - trainLoss: 0.3885139226913452\n",
      "cnt: 0 - valLoss: 0.4063849151134491 - trainLoss: 0.38851064443588257\n",
      "cnt: 1 - valLoss: 0.40638312697410583 - trainLoss: 0.3885073661804199\n",
      "cnt: 0 - valLoss: 0.40638306736946106 - trainLoss: 0.3885040283203125\n",
      "cnt: 0 - valLoss: 0.40638411045074463 - trainLoss: 0.38850075006484985\n",
      "cnt: 1 - valLoss: 0.40638402104377747 - trainLoss: 0.3884974718093872\n",
      "cnt: 2 - valLoss: 0.4063824713230133 - trainLoss: 0.3884941637516022\n",
      "cnt: 0 - valLoss: 0.40638166666030884 - trainLoss: 0.3884908854961395\n",
      "cnt: 0 - valLoss: 0.40638303756713867 - trainLoss: 0.3884875476360321\n",
      "cnt: 1 - valLoss: 0.40638160705566406 - trainLoss: 0.38848432898521423\n",
      "cnt: 0 - valLoss: 0.4063812792301178 - trainLoss: 0.3884809911251068\n",
      "cnt: 0 - valLoss: 0.40638163685798645 - trainLoss: 0.38847771286964417\n",
      "cnt: 1 - valLoss: 0.4063813388347626 - trainLoss: 0.3884744346141815\n",
      "cnt: 2 - valLoss: 0.4063809812068939 - trainLoss: 0.3884711265563965\n",
      "cnt: 0 - valLoss: 0.40637990832328796 - trainLoss: 0.38846784830093384\n",
      "cnt: 0 - valLoss: 0.40638047456741333 - trainLoss: 0.3884645402431488\n",
      "cnt: 1 - valLoss: 0.4063794016838074 - trainLoss: 0.38846126198768616\n",
      "cnt: 0 - valLoss: 0.4063786566257477 - trainLoss: 0.3884579539299011\n",
      "cnt: 0 - valLoss: 0.40637966990470886 - trainLoss: 0.3884546756744385\n",
      "cnt: 1 - valLoss: 0.40637847781181335 - trainLoss: 0.38845139741897583\n",
      "cnt: 0 - valLoss: 0.40637853741645813 - trainLoss: 0.3884481191635132\n",
      "cnt: 1 - valLoss: 0.4063780605792999 - trainLoss: 0.38844484090805054\n",
      "cnt: 0 - valLoss: 0.40637797117233276 - trainLoss: 0.3884415328502655\n",
      "cnt: 0 - valLoss: 0.40637728571891785 - trainLoss: 0.38843828439712524\n",
      "cnt: 0 - valLoss: 0.4063781201839447 - trainLoss: 0.3884349763393402\n",
      "cnt: 1 - valLoss: 0.4063780605792999 - trainLoss: 0.38843169808387756\n",
      "cnt: 2 - valLoss: 0.40637680888175964 - trainLoss: 0.3884284496307373\n",
      "cnt: 0 - valLoss: 0.40637636184692383 - trainLoss: 0.38842514157295227\n",
      "cnt: 0 - valLoss: 0.40637531876564026 - trainLoss: 0.38842180371284485\n",
      "cnt: 0 - valLoss: 0.4063766300678253 - trainLoss: 0.3884185254573822\n",
      "cnt: 1 - valLoss: 0.4063749313354492 - trainLoss: 0.38841527700424194\n",
      "cnt: 0 - valLoss: 0.40637585520744324 - trainLoss: 0.3884120285511017\n",
      "cnt: 1 - valLoss: 0.40637481212615967 - trainLoss: 0.38840875029563904\n",
      "cnt: 0 - valLoss: 0.406374454498291 - trainLoss: 0.388405442237854\n",
      "cnt: 0 - valLoss: 0.4063758850097656 - trainLoss: 0.38840219378471375\n",
      "cnt: 1 - valLoss: 0.40637457370758057 - trainLoss: 0.3883989155292511\n",
      "cnt: 2 - valLoss: 0.406374454498291 - trainLoss: 0.38839563727378845\n",
      "cnt: 0 - valLoss: 0.40637314319610596 - trainLoss: 0.3883923888206482\n",
      "cnt: 0 - valLoss: 0.40637391805648804 - trainLoss: 0.38838905096054077\n",
      "cnt: 1 - valLoss: 0.4063727855682373 - trainLoss: 0.3883858323097229\n",
      "cnt: 0 - valLoss: 0.40637245774269104 - trainLoss: 0.38838255405426025\n",
      "cnt: 0 - valLoss: 0.40637272596359253 - trainLoss: 0.3883792757987976\n",
      "cnt: 1 - valLoss: 0.4063727557659149 - trainLoss: 0.38837602734565735\n",
      "cnt: 2 - valLoss: 0.40637150406837463 - trainLoss: 0.3883727490901947\n",
      "cnt: 0 - valLoss: 0.4063730537891388 - trainLoss: 0.38836947083473206\n",
      "cnt: 1 - valLoss: 0.40637168288230896 - trainLoss: 0.3883662223815918\n",
      "cnt: 2 - valLoss: 0.4063718318939209 - trainLoss: 0.38836294412612915\n",
      "cnt: 3 - valLoss: 0.40637117624282837 - trainLoss: 0.3883596658706665\n",
      "cnt: 0 - valLoss: 0.4063706398010254 - trainLoss: 0.38835638761520386\n",
      "cnt: 0 - valLoss: 0.40636977553367615 - trainLoss: 0.3883531391620636\n",
      "cnt: 0 - valLoss: 0.40637093782424927 - trainLoss: 0.38834986090660095\n",
      "cnt: 1 - valLoss: 0.4063701629638672 - trainLoss: 0.3883466124534607\n",
      "cnt: 2 - valLoss: 0.4063688814640045 - trainLoss: 0.38834333419799805\n",
      "cnt: 0 - valLoss: 0.4063705801963806 - trainLoss: 0.3883401155471802\n",
      "cnt: 1 - valLoss: 0.40636909008026123 - trainLoss: 0.38833683729171753\n",
      "cnt: 2 - valLoss: 0.4063693881034851 - trainLoss: 0.3883335590362549\n",
      "cnt: 3 - valLoss: 0.40636828541755676 - trainLoss: 0.38833028078079224\n",
      "cnt: 0 - valLoss: 0.4063692092895508 - trainLoss: 0.3883269727230072\n",
      "cnt: 1 - valLoss: 0.4063678979873657 - trainLoss: 0.3883237838745117\n",
      "cnt: 0 - valLoss: 0.40636759996414185 - trainLoss: 0.3883204758167267\n",
      "cnt: 0 - valLoss: 0.40636807680130005 - trainLoss: 0.3883172571659088\n",
      "cnt: 1 - valLoss: 0.40636682510375977 - trainLoss: 0.38831400871276855\n",
      "cnt: 0 - valLoss: 0.4063670039176941 - trainLoss: 0.3883107602596283\n",
      "cnt: 1 - valLoss: 0.40636715292930603 - trainLoss: 0.38830748200416565\n",
      "cnt: 2 - valLoss: 0.40636658668518066 - trainLoss: 0.3883042335510254\n",
      "cnt: 0 - valLoss: 0.40636569261550903 - trainLoss: 0.38830095529556274\n",
      "cnt: 0 - valLoss: 0.40636616945266724 - trainLoss: 0.3882977366447449\n",
      "cnt: 1 - valLoss: 0.40636610984802246 - trainLoss: 0.3882944583892822\n",
      "cnt: 2 - valLoss: 0.4063652455806732 - trainLoss: 0.3882911801338196\n",
      "cnt: 0 - valLoss: 0.40636587142944336 - trainLoss: 0.3882879614830017\n",
      "cnt: 1 - valLoss: 0.406364768743515 - trainLoss: 0.38828474283218384\n",
      "cnt: 0 - valLoss: 0.4063641428947449 - trainLoss: 0.3882814645767212\n",
      "cnt: 0 - valLoss: 0.4063648581504822 - trainLoss: 0.38827821612358093\n",
      "cnt: 1 - valLoss: 0.4063640236854553 - trainLoss: 0.3882749378681183\n",
      "cnt: 0 - valLoss: 0.40636348724365234 - trainLoss: 0.3882717192173004\n",
      "cnt: 0 - valLoss: 0.4063641130924225 - trainLoss: 0.38826844096183777\n",
      "cnt: 1 - valLoss: 0.4063641130924225 - trainLoss: 0.3882651925086975\n",
      "cnt: 2 - valLoss: 0.40636253356933594 - trainLoss: 0.38826197385787964\n",
      "cnt: 0 - valLoss: 0.40636295080184937 - trainLoss: 0.388258695602417\n",
      "cnt: 1 - valLoss: 0.4063625931739807 - trainLoss: 0.3882554769515991\n",
      "cnt: 2 - valLoss: 0.40636253356933594 - trainLoss: 0.3882521986961365\n",
      "cnt: 0 - valLoss: 0.40636271238327026 - trainLoss: 0.3882489800453186\n",
      "cnt: 1 - valLoss: 0.4063623547554016 - trainLoss: 0.3882457911968231\n",
      "cnt: 0 - valLoss: 0.4063606858253479 - trainLoss: 0.3882425129413605\n",
      "cnt: 0 - valLoss: 0.4063618779182434 - trainLoss: 0.3882392346858978\n",
      "cnt: 1 - valLoss: 0.4063601791858673 - trainLoss: 0.38823601603507996\n",
      "cnt: 0 - valLoss: 0.40636229515075684 - trainLoss: 0.3882327973842621\n",
      "cnt: 1 - valLoss: 0.4063606262207031 - trainLoss: 0.3882296085357666\n",
      "cnt: 2 - valLoss: 0.40636032819747925 - trainLoss: 0.38822633028030396\n",
      "cnt: 3 - valLoss: 0.4063614308834076 - trainLoss: 0.3882230818271637\n",
      "cnt: 4 - valLoss: 0.40636056661605835 - trainLoss: 0.38821983337402344\n",
      "cnt: 5 - valLoss: 0.4063604474067688 - trainLoss: 0.38821664452552795\n",
      "cnt: 6 - valLoss: 0.40635937452316284 - trainLoss: 0.3882134258747101\n",
      "cnt: 0 - valLoss: 0.40635916590690613 - trainLoss: 0.38821014761924744\n",
      "cnt: 0 - valLoss: 0.4063595235347748 - trainLoss: 0.38820695877075195\n",
      "cnt: 1 - valLoss: 0.40635985136032104 - trainLoss: 0.3882037103176117\n",
      "cnt: 2 - valLoss: 0.406358927488327 - trainLoss: 0.3882005512714386\n",
      "cnt: 0 - valLoss: 0.40635886788368225 - trainLoss: 0.38819727301597595\n",
      "cnt: 0 - valLoss: 0.40635809302330017 - trainLoss: 0.3881940245628357\n",
      "cnt: 0 - valLoss: 0.4063582420349121 - trainLoss: 0.38819077610969543\n",
      "cnt: 1 - valLoss: 0.40635719895362854 - trainLoss: 0.38818755745887756\n",
      "cnt: 0 - valLoss: 0.40635931491851807 - trainLoss: 0.3881843686103821\n",
      "cnt: 1 - valLoss: 0.4063582420349121 - trainLoss: 0.3881811797618866\n",
      "cnt: 2 - valLoss: 0.4063567519187927 - trainLoss: 0.38817793130874634\n",
      "cnt: 0 - valLoss: 0.4063570499420166 - trainLoss: 0.3881746530532837\n",
      "cnt: 1 - valLoss: 0.4063558578491211 - trainLoss: 0.3881714642047882\n",
      "cnt: 0 - valLoss: 0.4063583314418793 - trainLoss: 0.3881682753562927\n",
      "cnt: 1 - valLoss: 0.40635693073272705 - trainLoss: 0.38816505670547485\n",
      "cnt: 2 - valLoss: 0.4063558280467987 - trainLoss: 0.3881618082523346\n",
      "cnt: 0 - valLoss: 0.4063566327095032 - trainLoss: 0.3881585896015167\n",
      "cnt: 1 - valLoss: 0.40635719895362854 - trainLoss: 0.38815537095069885\n",
      "cnt: 2 - valLoss: 0.4063557982444763 - trainLoss: 0.38815218210220337\n",
      "cnt: 0 - valLoss: 0.4063551127910614 - trainLoss: 0.3881489634513855\n",
      "cnt: 0 - valLoss: 0.40635567903518677 - trainLoss: 0.38814577460289\n",
      "cnt: 1 - valLoss: 0.4063550531864166 - trainLoss: 0.38814258575439453\n",
      "cnt: 0 - valLoss: 0.4063558876514435 - trainLoss: 0.3881393373012543\n",
      "cnt: 1 - valLoss: 0.40635544061660767 - trainLoss: 0.3881361782550812\n",
      "cnt: 2 - valLoss: 0.40635448694229126 - trainLoss: 0.3881329596042633\n",
      "cnt: 0 - valLoss: 0.40635430812835693 - trainLoss: 0.38812974095344543\n",
      "cnt: 0 - valLoss: 0.4063541293144226 - trainLoss: 0.38812655210494995\n",
      "cnt: 0 - valLoss: 0.4063544273376465 - trainLoss: 0.3881233334541321\n",
      "cnt: 1 - valLoss: 0.40635427832603455 - trainLoss: 0.38812020421028137\n",
      "cnt: 2 - valLoss: 0.4063531458377838 - trainLoss: 0.3881169855594635\n",
      "cnt: 0 - valLoss: 0.40635496377944946 - trainLoss: 0.388113796710968\n",
      "cnt: 1 - valLoss: 0.4063539206981659 - trainLoss: 0.38811060786247253\n",
      "cnt: 2 - valLoss: 0.4063531756401062 - trainLoss: 0.3881073594093323\n",
      "cnt: 3 - valLoss: 0.4063529372215271 - trainLoss: 0.3881042003631592\n",
      "cnt: 0 - valLoss: 0.40635254979133606 - trainLoss: 0.3881009817123413\n",
      "cnt: 0 - valLoss: 0.4063522517681122 - trainLoss: 0.3880978226661682\n",
      "cnt: 0 - valLoss: 0.40635257959365845 - trainLoss: 0.38809457421302795\n",
      "cnt: 1 - valLoss: 0.4063510596752167 - trainLoss: 0.38809141516685486\n",
      "cnt: 0 - valLoss: 0.4063527584075928 - trainLoss: 0.3880882263183594\n",
      "cnt: 1 - valLoss: 0.4063514471054077 - trainLoss: 0.3880850374698639\n",
      "cnt: 2 - valLoss: 0.4063515067100525 - trainLoss: 0.3880818486213684\n",
      "cnt: 3 - valLoss: 0.4063520133495331 - trainLoss: 0.3880786597728729\n",
      "cnt: 4 - valLoss: 0.4063505232334137 - trainLoss: 0.38807547092437744\n",
      "cnt: 0 - valLoss: 0.4063510000705719 - trainLoss: 0.38807228207588196\n",
      "cnt: 1 - valLoss: 0.40635058283805847 - trainLoss: 0.3880690932273865\n",
      "cnt: 2 - valLoss: 0.4063504934310913 - trainLoss: 0.3880659341812134\n",
      "cnt: 0 - valLoss: 0.4063507318496704 - trainLoss: 0.3880626857280731\n",
      "cnt: 1 - valLoss: 0.4063502848148346 - trainLoss: 0.3880595266819\n",
      "cnt: 0 - valLoss: 0.40634965896606445 - trainLoss: 0.38805636763572693\n",
      "cnt: 0 - valLoss: 0.4063500761985779 - trainLoss: 0.38805317878723145\n",
      "cnt: 1 - valLoss: 0.4063495099544525 - trainLoss: 0.38805001974105835\n",
      "cnt: 0 - valLoss: 0.4063494801521301 - trainLoss: 0.3880468010902405\n",
      "cnt: 0 - valLoss: 0.4063483774662018 - trainLoss: 0.388043612241745\n",
      "cnt: 0 - valLoss: 0.4063483476638794 - trainLoss: 0.3880404531955719\n",
      "cnt: 0 - valLoss: 0.40634843707084656 - trainLoss: 0.38803720474243164\n",
      "cnt: 1 - valLoss: 0.4063487946987152 - trainLoss: 0.38803407549858093\n",
      "cnt: 2 - valLoss: 0.4063471853733063 - trainLoss: 0.38803091645240784\n",
      "cnt: 0 - valLoss: 0.4063478708267212 - trainLoss: 0.38802769780158997\n",
      "cnt: 1 - valLoss: 0.4063478410243988 - trainLoss: 0.38802453875541687\n",
      "cnt: 2 - valLoss: 0.4063468277454376 - trainLoss: 0.3880213499069214\n",
      "cnt: 0 - valLoss: 0.40634602308273315 - trainLoss: 0.38801810145378113\n",
      "cnt: 0 - valLoss: 0.4063461422920227 - trainLoss: 0.38801488280296326\n",
      "cnt: 1 - valLoss: 0.40634581446647644 - trainLoss: 0.3880116641521454\n",
      "cnt: 0 - valLoss: 0.4063446521759033 - trainLoss: 0.3880084455013275\n",
      "cnt: 0 - valLoss: 0.40634578466415405 - trainLoss: 0.38800522685050964\n",
      "cnt: 1 - valLoss: 0.40634679794311523 - trainLoss: 0.38800206780433655\n",
      "cnt: 2 - valLoss: 0.40634945034980774 - trainLoss: 0.38799867033958435\n",
      "cnt: 3 - valLoss: 0.4063505530357361 - trainLoss: 0.38799527287483215\n",
      "cnt: 4 - valLoss: 0.40635159611701965 - trainLoss: 0.38799187541007996\n",
      "cnt: 5 - valLoss: 0.4063534438610077 - trainLoss: 0.38798853754997253\n",
      "cnt: 6 - valLoss: 0.40635567903518677 - trainLoss: 0.38798514008522034\n",
      "cnt: 7 - valLoss: 0.4063565731048584 - trainLoss: 0.3879818022251129\n",
      "cnt: 8 - valLoss: 0.40635836124420166 - trainLoss: 0.3879784047603607\n",
      "cnt: 9 - valLoss: 0.40635934472084045 - trainLoss: 0.3879750370979309\n",
      "cnt: 10 - valLoss: 0.40636229515075684 - trainLoss: 0.3879717290401459\n",
      "cnt: 11 - valLoss: 0.40636271238327026 - trainLoss: 0.38796836137771606\n",
      "cnt: 12 - valLoss: 0.40636488795280457 - trainLoss: 0.38796499371528625\n",
      "cnt: 13 - valLoss: 0.4063654839992523 - trainLoss: 0.38796159625053406\n",
      "cnt: 14 - valLoss: 0.40636757016181946 - trainLoss: 0.38795822858810425\n",
      "cnt: 15 - valLoss: 0.4063689708709717 - trainLoss: 0.3879549205303192\n",
      "cnt: 16 - valLoss: 0.4063703417778015 - trainLoss: 0.387951523065567\n",
      "cnt: 17 - valLoss: 0.4063718020915985 - trainLoss: 0.3879481554031372\n",
      "cnt: 18 - valLoss: 0.40637311339378357 - trainLoss: 0.3879448175430298\n",
      "cnt: 19 - valLoss: 0.4063746929168701 - trainLoss: 0.38794147968292236\n",
      "cnt: 20 - valLoss: 0.40637657046318054 - trainLoss: 0.38793814182281494\n",
      "cnt: 21 - valLoss: 0.40637755393981934 - trainLoss: 0.3879348039627075\n",
      "cnt: 22 - valLoss: 0.4063788056373596 - trainLoss: 0.3879314363002777\n",
      "cnt: 23 - valLoss: 0.40637990832328796 - trainLoss: 0.3879280686378479\n",
      "cnt: 24 - valLoss: 0.40638211369514465 - trainLoss: 0.3879246711730957\n",
      "cnt: 25 - valLoss: 0.40638306736946106 - trainLoss: 0.38792139291763306\n",
      "cnt: 26 - valLoss: 0.40638476610183716 - trainLoss: 0.38791802525520325\n",
      "cnt: 27 - valLoss: 0.4063847064971924 - trainLoss: 0.38791465759277344\n",
      "cnt: 28 - valLoss: 0.40638816356658936 - trainLoss: 0.387911319732666\n",
      "cnt: 29 - valLoss: 0.40638843178749084 - trainLoss: 0.38790804147720337\n",
      "cnt: 30 - valLoss: 0.4063895642757416 - trainLoss: 0.38790464401245117\n",
      "cnt: 31 - valLoss: 0.40639087557792664 - trainLoss: 0.38790133595466614\n",
      "cnt: 32 - valLoss: 0.40639275312423706 - trainLoss: 0.38789796829223633\n",
      "cnt: 33 - valLoss: 0.40639492869377136 - trainLoss: 0.3878946602344513\n",
      "cnt: 34 - valLoss: 0.4063950479030609 - trainLoss: 0.3878912925720215\n",
      "cnt: 35 - valLoss: 0.406396746635437 - trainLoss: 0.38788801431655884\n",
      "cnt: 36 - valLoss: 0.40639790892601013 - trainLoss: 0.38788464665412903\n",
      "cnt: 37 - valLoss: 0.40639954805374146 - trainLoss: 0.387881338596344\n",
      "cnt: 38 - valLoss: 0.40640050172805786 - trainLoss: 0.3878779709339142\n",
      "cnt: 39 - valLoss: 0.406402051448822 - trainLoss: 0.38787466287612915\n",
      "cnt: 40 - valLoss: 0.4064033329486847 - trainLoss: 0.3878713846206665\n",
      "cnt: 41 - valLoss: 0.4064052104949951 - trainLoss: 0.3878680467605591\n",
      "cnt: 42 - valLoss: 0.40640637278556824 - trainLoss: 0.38786473870277405\n",
      "cnt: 43 - valLoss: 0.4064069986343384 - trainLoss: 0.3878614008426666\n",
      "cnt: 44 - valLoss: 0.40640968084335327 - trainLoss: 0.387858122587204\n",
      "cnt: 45 - valLoss: 0.40641117095947266 - trainLoss: 0.38785478472709656\n",
      "cnt: 46 - valLoss: 0.40641191601753235 - trainLoss: 0.3878514766693115\n",
      "cnt: 47 - valLoss: 0.40641194581985474 - trainLoss: 0.3878481686115265\n",
      "cnt: 48 - valLoss: 0.406413733959198 - trainLoss: 0.38784489035606384\n",
      "cnt: 49 - valLoss: 0.40641337633132935 - trainLoss: 0.3878415822982788\n",
      "cnt: 50 - valLoss: 0.40641459822654724 - trainLoss: 0.38783833384513855\n",
      "cnt: 51 - valLoss: 0.40641459822654724 - trainLoss: 0.3878350555896759\n",
      "cnt: 52 - valLoss: 0.40641629695892334 - trainLoss: 0.38783180713653564\n",
      "cnt: 53 - valLoss: 0.4064162075519562 - trainLoss: 0.387828528881073\n",
      "cnt: 54 - valLoss: 0.4064168632030487 - trainLoss: 0.38782525062561035\n",
      "cnt: 55 - valLoss: 0.40641695261001587 - trainLoss: 0.3878220021724701\n",
      "cnt: 56 - valLoss: 0.4064178466796875 - trainLoss: 0.38781869411468506\n",
      "cnt: 57 - valLoss: 0.40641841292381287 - trainLoss: 0.3878154158592224\n",
      "cnt: 58 - valLoss: 0.4064190089702606 - trainLoss: 0.38781213760375977\n",
      "cnt: 59 - valLoss: 0.4064200520515442 - trainLoss: 0.3878089189529419\n",
      "cnt: 60 - valLoss: 0.4064198136329651 - trainLoss: 0.38780561089515686\n",
      "cnt: 61 - valLoss: 0.4064209759235382 - trainLoss: 0.3878023624420166\n",
      "cnt: 62 - valLoss: 0.4064209461212158 - trainLoss: 0.38779908418655396\n",
      "cnt: 63 - valLoss: 0.40642231702804565 - trainLoss: 0.3877958059310913\n",
      "cnt: 64 - valLoss: 0.4064224660396576 - trainLoss: 0.38779258728027344\n",
      "cnt: 65 - valLoss: 0.4064229428768158 - trainLoss: 0.3877893090248108\n",
      "cnt: 66 - valLoss: 0.40642377734184265 - trainLoss: 0.38778603076934814\n",
      "cnt: 67 - valLoss: 0.40642449259757996 - trainLoss: 0.3877827823162079\n",
      "cnt: 68 - valLoss: 0.4064246416091919 - trainLoss: 0.3877795338630676\n",
      "cnt: 69 - valLoss: 0.4064250588417053 - trainLoss: 0.38777631521224976\n",
      "cnt: 70 - valLoss: 0.40642550587654114 - trainLoss: 0.3877730071544647\n",
      "cnt: 71 - valLoss: 0.40642666816711426 - trainLoss: 0.38776975870132446\n",
      "cnt: 72 - valLoss: 0.4064267873764038 - trainLoss: 0.3877664804458618\n",
      "cnt: 73 - valLoss: 0.40642741322517395 - trainLoss: 0.38776323199272156\n",
      "cnt: 74 - valLoss: 0.40642765164375305 - trainLoss: 0.3877599537372589\n",
      "cnt: 75 - valLoss: 0.40642842650413513 - trainLoss: 0.38775667548179626\n",
      "cnt: 76 - valLoss: 0.40642860531806946 - trainLoss: 0.3877534568309784\n",
      "cnt: 77 - valLoss: 0.40642938017845154 - trainLoss: 0.38775017857551575\n",
      "cnt: 78 - valLoss: 0.406430184841156 - trainLoss: 0.3877469599246979\n",
      "cnt: 79 - valLoss: 0.40643051266670227 - trainLoss: 0.3877437114715576\n",
      "cnt: 80 - valLoss: 0.4064312279224396 - trainLoss: 0.38774043321609497\n",
      "cnt: 81 - valLoss: 0.4064316749572754 - trainLoss: 0.3877371847629547\n",
      "cnt: 82 - valLoss: 0.40643206238746643 - trainLoss: 0.38773393630981445\n",
      "cnt: 83 - valLoss: 0.4064328372478485 - trainLoss: 0.3877306580543518\n",
      "cnt: 84 - valLoss: 0.4064329266548157 - trainLoss: 0.38772743940353394\n",
      "cnt: 85 - valLoss: 0.4064335525035858 - trainLoss: 0.3877241611480713\n",
      "cnt: 86 - valLoss: 0.40643438696861267 - trainLoss: 0.3877209424972534\n",
      "cnt: 87 - valLoss: 0.40643420815467834 - trainLoss: 0.38771772384643555\n",
      "cnt: 88 - valLoss: 0.40643516182899475 - trainLoss: 0.3877144455909729\n",
      "cnt: 89 - valLoss: 0.4064353406429291 - trainLoss: 0.38771119713783264\n",
      "cnt: 90 - valLoss: 0.4064365327358246 - trainLoss: 0.38770791888237\n",
      "cnt: 91 - valLoss: 0.40643635392189026 - trainLoss: 0.3877047300338745\n",
      "cnt: 92 - valLoss: 0.4064371585845947 - trainLoss: 0.38770148158073425\n",
      "cnt: 93 - valLoss: 0.40643760561943054 - trainLoss: 0.3876981735229492\n",
      "cnt: 94 - valLoss: 0.40643808245658875 - trainLoss: 0.38769498467445374\n",
      "cnt: 95 - valLoss: 0.4064382016658783 - trainLoss: 0.3876917064189911\n",
      "cnt: 96 - valLoss: 0.4064388871192932 - trainLoss: 0.3876884877681732\n",
      "cnt: 97 - valLoss: 0.4064392149448395 - trainLoss: 0.38768523931503296\n",
      "cnt: 98 - valLoss: 0.40643933415412903 - trainLoss: 0.3876819908618927\n",
      "cnt: 99 - valLoss: 0.4064404368400574 - trainLoss: 0.38767877221107483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x159dd4a30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def trainAI(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for X, y,_ in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return trainLoss\n",
    "\n",
    "def valAI(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    valLoss =0\n",
    "    with torch.no_grad():\n",
    "        for X ,y,_ in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            # for i in loss:\n",
    "            valLoss+= loss.item()\n",
    "    return valLoss\n",
    "\n",
    "trainLoss=[]\n",
    "valLoss=[]\n",
    "bestModel = model\n",
    "bestLoss = float('inf')\n",
    "cnt=0\n",
    "\n",
    "while(cnt<100):\n",
    "    trainLoss.append(trainAI(trainLoader, model, loss_fn, optimizer))\n",
    "    valLoss.append(valAI(valLoader, model, loss_fn))\n",
    "\n",
    "    print(f'cnt: {cnt} - valLoss: {valLoss[-1]} - trainLoss: {trainLoss[-1]}')\n",
    "    if bestLoss<valLoss[-1]:\n",
    "        cnt+=1\n",
    "    else:\n",
    "        cnt = 0\n",
    "        bestLoss = valLoss[-1]\n",
    "        bestModel = model\n",
    "\n",
    "plt.plot(trainLoss,label='trainLoss')\n",
    "plt.plot(valLoss,label='valLoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8277153558052435"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracyAI(dataloader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, _  in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            for idx, i in enumerate(pred):\n",
    "                if torch.argmax(i)== torch.argmax(y[idx]):\n",
    "                    correct +=1 \n",
    "                total+=1\n",
    "    return correct / total\n",
    "\n",
    "accuracyAI(valLoader, bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     passengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for X ,_ , id in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            for idx, i in enumerate(pred):\n",
    "                result.append([id[idx],torch.argmax(i).item()])\n",
    "    return result\n",
    "\n",
    "result = testAI(testLoader, bestModel)\n",
    "result = pd.DataFrame(result)\n",
    "result = result.astype(int)\n",
    "result.columns=['passengerId','Survived']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
