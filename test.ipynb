{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoyt, Mr. William Fisher</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17600</td>\n",
       "      <td>30.6958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>479</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Karlsson, Mr. Nils August</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350060</td>\n",
       "      <td>7.5208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Jussila, Miss. Katriina</td>\n",
       "      <td>female</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4136</td>\n",
       "      <td>9.8250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                       Name     Sex   Age  \\\n",
       "793          794         0       1   Hoyt, Mr. William Fisher    male   NaN   \n",
       "478          479         0       3  Karlsson, Mr. Nils August    male  22.0   \n",
       "113          114         0       3    Jussila, Miss. Katriina  female  20.0   \n",
       "\n",
       "     SibSp  Parch    Ticket     Fare Cabin Embarked  \n",
       "793      0      0  PC 17600  30.6958   NaN        C  \n",
       "478      0      0    350060   7.5208   NaN        S  \n",
       "113      1      0      4136   9.8250   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "print(trainDF.info())\n",
    "trainDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1137</td>\n",
       "      <td>1</td>\n",
       "      <td>Kenyon, Mr. Frederick R</td>\n",
       "      <td>male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17464</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>D21</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>974</td>\n",
       "      <td>1</td>\n",
       "      <td>Case, Mr. Howard Brown</td>\n",
       "      <td>male</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19924</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>951</td>\n",
       "      <td>1</td>\n",
       "      <td>Chaudanson, Miss. Victorine</td>\n",
       "      <td>female</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17608</td>\n",
       "      <td>262.3750</td>\n",
       "      <td>B61</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                         Name     Sex   Age  SibSp  \\\n",
       "245         1137       1      Kenyon, Mr. Frederick R    male  41.0      1   \n",
       "82           974       1       Case, Mr. Howard Brown    male  49.0      0   \n",
       "59           951       1  Chaudanson, Miss. Victorine  female  36.0      0   \n",
       "\n",
       "     Parch    Ticket      Fare Cabin Embarked  \n",
       "245      0     17464   51.8625   D21        S  \n",
       "82       0     19924   26.0000   NaN        S  \n",
       "59       0  PC 17608  262.3750   B61        C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "print(testDF.info())\n",
    "testDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.257307</td>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.155660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>0.096335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.075972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>0.075198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>0.073258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.257307</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>-0.172683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_female</th>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_male</th>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_C</th>\n",
       "      <td>0.168240</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>-0.775441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.489874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>-0.155660</td>\n",
       "      <td>0.096335</td>\n",
       "      <td>-0.075972</td>\n",
       "      <td>0.075198</td>\n",
       "      <td>0.073258</td>\n",
       "      <td>-0.172683</td>\n",
       "      <td>-0.119504</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>-0.775441</td>\n",
       "      <td>-0.489874</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Survived    Pclass       Age     SibSp     Parch      Fare  \\\n",
       "Survived    1.000000 -0.338481 -0.077221 -0.035322  0.081629  0.257307   \n",
       "Pclass     -0.338481  1.000000 -0.408106  0.060832  0.018322 -0.558629   \n",
       "Age        -0.077221 -0.408106  1.000000 -0.243699 -0.150917  0.178740   \n",
       "SibSp      -0.035322  0.060832 -0.243699  1.000000  0.373587  0.160238   \n",
       "Parch       0.081629  0.018322 -0.150917  0.373587  1.000000  0.221539   \n",
       "Fare        0.257307 -0.558629  0.178740  0.160238  0.221539  1.000000   \n",
       "Sex_female  0.543351 -0.124617 -0.063645  0.109609  0.213125  0.185523   \n",
       "Sex_male   -0.543351  0.124617  0.063645 -0.109609 -0.213125 -0.185523   \n",
       "Embarked_C  0.168240 -0.269658  0.085777 -0.048396 -0.008635  0.286269   \n",
       "Embarked_Q  0.003650  0.230491 -0.019458 -0.048678 -0.100943 -0.130059   \n",
       "Embarked_S -0.155660  0.096335 -0.075972  0.075198  0.073258 -0.172683   \n",
       "\n",
       "            Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
       "Survived      0.543351 -0.543351    0.168240    0.003650   -0.155660  \n",
       "Pclass       -0.124617  0.124617   -0.269658    0.230491    0.096335  \n",
       "Age          -0.063645  0.063645    0.085777   -0.019458   -0.075972  \n",
       "SibSp         0.109609 -0.109609   -0.048396   -0.048678    0.075198  \n",
       "Parch         0.213125 -0.213125   -0.008635   -0.100943    0.073258  \n",
       "Fare          0.185523 -0.185523    0.286269   -0.130059   -0.172683  \n",
       "Sex_female    1.000000 -1.000000    0.066564    0.088651   -0.119504  \n",
       "Sex_male     -1.000000  1.000000   -0.066564   -0.088651    0.119504  \n",
       "Embarked_C    0.066564 -0.066564    1.000000   -0.164166   -0.775441  \n",
       "Embarked_Q    0.088651 -0.088651   -0.164166    1.000000   -0.489874  \n",
       "Embarked_S   -0.119504  0.119504   -0.775441   -0.489874    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF = concatDF.reset_index(drop=True)\n",
    "pd.get_dummies(concatDF,columns=['Sex','Embarked']).drop(columns=['PassengerId','Name','Ticket','Cabin']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredDF = pd.read_csv('train4.csv')\n",
    "# featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "# featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "# featuredDF['SibSp'] = featuredDF['SibSp']/featuredDF['SibSp'].max()\n",
    "# featuredDF['Parch'] = featuredDF['Parch']/featuredDF['Parch'].max()\n",
    "# featuredDF['Ticket'] = featuredDF['Ticket']/featuredDF['Ticket'].max()\n",
    "# featuredDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredDF = pd.read_csv('train5.csv')\n",
    "# featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "# featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "# featuredDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Name_ the Countess</th>\n",
       "      <th>Cabin_A</th>\n",
       "      <th>Cabin_B</th>\n",
       "      <th>Cabin_C</th>\n",
       "      <th>Cabin_D</th>\n",
       "      <th>Cabin_E</th>\n",
       "      <th>Cabin_F</th>\n",
       "      <th>Cabin_G</th>\n",
       "      <th>Cabin_M</th>\n",
       "      <th>Cabin_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27500</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.32500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>1305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>1306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.212559</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>1308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PassengerId  Survived      Age  SibSp     Parch    Ticket      Fare  \\\n",
       "0               1       0.0  0.27500  0.125  0.000000  0.090909  0.014151   \n",
       "1               2       1.0  0.47500  0.125  0.000000  0.181818  0.139136   \n",
       "2               3       1.0  0.32500  0.000  0.000000  0.090909  0.015469   \n",
       "3               4       1.0  0.43750  0.125  0.000000  0.181818  0.103644   \n",
       "4               5       0.0  0.43750  0.000  0.000000  0.090909  0.015713   \n",
       "...           ...       ...      ...    ...       ...       ...       ...   \n",
       "1304         1305       NaN  0.33750  0.000  0.000000  0.090909  0.015713   \n",
       "1305         1306       NaN  0.48750  0.000  0.000000  0.272727  0.212559   \n",
       "1306         1307       NaN  0.48125  0.000  0.000000  0.090909  0.014151   \n",
       "1307         1308       NaN  0.33750  0.000  0.000000  0.090909  0.015713   \n",
       "1308         1309       NaN  0.30000  0.125  0.111111  0.272727  0.043640   \n",
       "\n",
       "      Pclass_1  Pclass_2  Pclass_3  ...  Name_ the Countess  Cabin_A  Cabin_B  \\\n",
       "0        False     False      True  ...               False    False    False   \n",
       "1         True     False     False  ...               False    False    False   \n",
       "2        False     False      True  ...               False    False    False   \n",
       "3         True     False     False  ...               False    False    False   \n",
       "4        False     False      True  ...               False    False    False   \n",
       "...        ...       ...       ...  ...                 ...      ...      ...   \n",
       "1304     False     False      True  ...               False    False    False   \n",
       "1305      True     False     False  ...               False    False    False   \n",
       "1306     False     False      True  ...               False    False    False   \n",
       "1307     False     False      True  ...               False    False    False   \n",
       "1308     False     False      True  ...               False    False    False   \n",
       "\n",
       "      Cabin_C  Cabin_D  Cabin_E  Cabin_F  Cabin_G  Cabin_M  Cabin_T  \n",
       "0       False    False    False    False    False     True    False  \n",
       "1        True    False    False    False    False    False    False  \n",
       "2       False    False    False    False    False     True    False  \n",
       "3        True    False    False    False    False    False    False  \n",
       "4       False    False    False    False    False     True    False  \n",
       "...       ...      ...      ...      ...      ...      ...      ...  \n",
       "1304    False    False    False    False    False     True    False  \n",
       "1305     True    False    False    False    False    False    False  \n",
       "1306    False    False    False    False    False     True    False  \n",
       "1307    False    False    False    False    False     True    False  \n",
       "1308    False    False    False    False    False     True    False  \n",
       "\n",
       "[1309 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuredDF = pd.read_csv('train6.csv')\n",
    "featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "featuredDF['SibSp'] = featuredDF['SibSp']/featuredDF['SibSp'].max()\n",
    "featuredDF['Parch'] = featuredDF['Parch']/featuredDF['Parch'].max()\n",
    "featuredDF['Ticket'] = featuredDF['Ticket']/featuredDF['Ticket'].max()\n",
    "featuredDF= pd.get_dummies(featuredDF,columns=['Pclass','Sex','Embarked','Name','Cabin'])\n",
    "featuredDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4313, 0.0000, 0.0000, 0.0909, 0.0153, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 1.0000, 0.0000], device='mps:0'),\n",
       " tensor([], device='mps:0'),\n",
       " 892)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF:pd.DataFrame):\n",
    "        self.PassengerId= DF['PassengerId'].values\n",
    "        self.Servived = pd.get_dummies(DF['Survived']).values\n",
    "        DF = DF.drop(columns=['PassengerId','Survived'])\n",
    "        self.data = DF.astype(float).values\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PassengerId)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        id = self.PassengerId[idx]\n",
    "        x = torch.FloatTensor(self.data[idx])\n",
    "        y = torch.FloatTensor(self.Servived[idx])\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        return x, y , id\n",
    "\n",
    "\n",
    "dataSet = MyDataset(DF=featuredDF[:891])\n",
    "testSet = MyDataset(DF=featuredDF[891:])\n",
    "testSet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainSet, valSet = torch.utils.data.random_split(dataSet,(0.7,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2375, 0.1250, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1250, 0.0000, 0.2222,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.2250, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.6500, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "         [0.7875, 0.1250, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        device='mps:0'),\n",
       " tensor([[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         ...,\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.]], device='mps:0'),\n",
       " tensor([505, 292, 420, 614, 696, 845,  60, 119, 434, 224,  85, 821, 177, 565,\n",
       "         633, 860, 676, 121, 277, 375, 169, 587, 295,  82, 466, 808, 105, 847,\n",
       "         740, 478, 820,  40, 304, 616, 610, 575,   6, 269, 572, 834, 192, 310,\n",
       "         227, 533, 110, 663, 206, 380, 623, 583, 278,  91, 660, 149, 130, 332,\n",
       "         548, 394,  57, 464, 529, 858, 768, 151, 416, 539, 163, 212, 128, 772,\n",
       "         405, 468, 241,  56, 388, 339, 773, 596, 789, 775, 496, 418, 579, 176,\n",
       "         804, 407, 584, 256,  45,  66, 795, 712, 140, 608, 495, 629, 116, 248,\n",
       "         399, 430,  76, 288,  30, 790, 107,  63, 570, 178, 440, 357, 564, 580,\n",
       "         350, 488, 870, 662, 424,  99, 274,   7, 147, 313, 152, 735,  79, 492,\n",
       "         123, 137, 650, 126, 314,  27, 387, 801, 254, 188, 319, 165, 429, 545,\n",
       "         125, 289, 783, 831, 853, 270, 593, 767, 510, 648, 230,  55,  53, 611,\n",
       "          92, 852, 626,  87, 364, 734, 368, 784, 627, 309, 349, 621, 567, 234,\n",
       "         625, 441, 741, 202,  49, 619, 179, 763, 713, 497, 104, 341, 345, 872,\n",
       "         643, 117, 433, 698, 507, 844, 685, 412, 449, 501, 415, 573, 158, 680,\n",
       "         571, 353,  41, 267, 555,  21, 612, 814, 543, 535, 249, 365, 785, 331,\n",
       "         691, 465, 883, 425, 552, 711, 127, 782, 258, 875, 302, 527, 638,  78,\n",
       "         812,  11, 786, 318, 459, 589,  44,  22, 446, 757, 448, 503, 196, 340,\n",
       "         463,   4, 540, 245, 841, 435, 523, 800, 607, 531, 139, 701, 378, 674,\n",
       "         282, 859, 461, 518, 885, 211, 702, 792,  54, 628, 889, 723,  23, 617,\n",
       "          48, 284, 793, 283, 528, 708, 787, 506,  50,  12, 235, 709, 226, 744,\n",
       "         290, 807, 761, 329, 887,  72, 850, 322, 494, 794, 129, 243, 298,   9,\n",
       "         755, 280, 100, 253, 228, 167, 777, 856,  93,  33,  95, 598, 221, 372,\n",
       "         244, 138, 450, 716, 675, 730, 133, 327,  62, 630, 753,  83, 876, 171,\n",
       "         321, 209, 220, 805, 656, 538, 728, 504, 828, 260, 516,  77, 816, 682,\n",
       "          94,  37, 476, 483,  97, 600, 732, 166, 822, 131, 279, 522, 354, 373,\n",
       "         469, 848, 576, 370, 115, 817, 477, 268, 156, 672, 597,   5, 214, 582,\n",
       "           3, 553, 362, 216,  98, 122, 486, 891, 265, 840, 186, 549, 827, 547,\n",
       "         306,  26, 843, 437, 511, 765, 409,  52, 884, 606, 113, 390, 725, 307,\n",
       "         346, 411, 315, 832,  39, 566, 285, 197, 673, 574, 439, 489, 833, 266,\n",
       "         120,  59, 759, 421, 644, 493, 750, 470, 846, 164, 320, 524, 736,  18,\n",
       "         482, 726, 780, 551, 259, 520, 824, 102, 799, 830, 422, 480, 391,  67,\n",
       "         842, 867,  89,  38, 631, 426, 637, 453, 351, 544, 717,  35, 438, 303,\n",
       "         106, 802, 639, 185, 797, 678,  61, 737, 168, 150, 275, 343, 193, 747,\n",
       "         542, 160, 710, 561,  24,  74, 687, 103, 393, 810, 342, 148, 356, 751,\n",
       "         173, 481, 595, 769, 557,  88, 454, 647, 447, 641, 135, 871, 646, 878,\n",
       "         222, 175,  58, 383, 541, 414, 101, 347, 605, 218, 809, 381, 442, 379,\n",
       "         766, 890, 514, 886, 246,  43, 203, 273, 679, 758, 855,  42, 563, 869,\n",
       "         738, 291, 336, 581,  10, 754,  25, 653, 622, 337, 344, 517, 204,  64,\n",
       "         851, 403, 385, 866, 562, 392, 879, 154, 132, 655, 301, 297, 569, 410,\n",
       "         183, 428, 707,  17, 684, 624, 406, 554, 640, 724, 210, 881, 683, 223,\n",
       "         661,  16,  84, 281, 681, 299, 143, 815, 229, 369, 474,  80, 366, 829,\n",
       "         649, 475, 778, 325, 731, 363, 408, 417, 700,   1, 727, 532, 199, 718,\n",
       "         530, 560, 201, 233, 157, 746, 515, 500,  73, 427, 742, 359,  15, 796,\n",
       "         806, 257, 719, 272, 745, 239, 338, 603, 335, 198, 231, 443, 305, 558,\n",
       "         413, 559, 144, 323, 296, 835, 715, 276])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(trainSet,batch_size=1024,sampler=torch.utils.data.RandomSampler(trainSet))\n",
    "valLoader= torch.utils.data.DataLoader(valSet,batch_size=1024,sampler=torch.utils.data.RandomSampler(valSet))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=1024)\n",
    "next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(40, 40),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(40, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] cnt: 0 - valLoss: 0.6249459385871887 - trainLoss: 0.668856680393219\n",
      "[1] cnt: 0 - valLoss: 0.5876710414886475 - trainLoss: 0.6302855014801025\n",
      "[1] cnt: 0 - valLoss: 0.5468934178352356 - trainLoss: 0.5979230999946594\n",
      "[1] cnt: 0 - valLoss: 0.5067541003227234 - trainLoss: 0.562787652015686\n",
      "[1] cnt: 0 - valLoss: 0.47277796268463135 - trainLoss: 0.5287901759147644\n",
      "[1] cnt: 0 - valLoss: 0.4490484893321991 - trainLoss: 0.5007523894309998\n",
      "[1] cnt: 0 - valLoss: 0.4338691532611847 - trainLoss: 0.4814339578151703\n",
      "[1] cnt: 0 - valLoss: 0.425301194190979 - trainLoss: 0.4693109691143036\n",
      "[1] cnt: 0 - valLoss: 0.4191795587539673 - trainLoss: 0.4616797864437103\n",
      "[1] cnt: 0 - valLoss: 0.4155835807323456 - trainLoss: 0.45642295479774475\n",
      "[1] cnt: 0 - valLoss: 0.41208410263061523 - trainLoss: 0.4523555636405945\n",
      "[1] cnt: 0 - valLoss: 0.4099859595298767 - trainLoss: 0.4490709602832794\n",
      "[1] cnt: 0 - valLoss: 0.40719810128211975 - trainLoss: 0.44624799489974976\n",
      "[1] cnt: 0 - valLoss: 0.40575021505355835 - trainLoss: 0.44374603033065796\n",
      "[1] cnt: 0 - valLoss: 0.4031239449977875 - trainLoss: 0.4414995610713959\n",
      "[1] cnt: 0 - valLoss: 0.40211936831474304 - trainLoss: 0.43943044543266296\n",
      "[1] cnt: 0 - valLoss: 0.3994346559047699 - trainLoss: 0.4374745786190033\n",
      "[1] cnt: 0 - valLoss: 0.3990749418735504 - trainLoss: 0.4356643557548523\n",
      "[1] cnt: 0 - valLoss: 0.39614295959472656 - trainLoss: 0.4339767396450043\n",
      "[1] cnt: 1 - valLoss: 0.3968220353126526 - trainLoss: 0.432447224855423\n",
      "[1] cnt: 0 - valLoss: 0.3932904303073883 - trainLoss: 0.431039959192276\n",
      "[1] cnt: 1 - valLoss: 0.3956378996372223 - trainLoss: 0.429817259311676\n",
      "[1] cnt: 0 - valLoss: 0.3911696970462799 - trainLoss: 0.42882245779037476\n",
      "[1] cnt: 1 - valLoss: 0.39654654264450073 - trainLoss: 0.4282367527484894\n",
      "[1] cnt: 0 - valLoss: 0.39052101969718933 - trainLoss: 0.42810043692588806\n",
      "[1] cnt: 1 - valLoss: 0.398938924074173 - trainLoss: 0.4288194179534912\n",
      "[1] cnt: 2 - valLoss: 0.3917664885520935 - trainLoss: 0.42903998494148254\n",
      "[1] cnt: 3 - valLoss: 0.40246620774269104 - trainLoss: 0.43111345171928406\n",
      "[1] cnt: 4 - valLoss: 0.3934861123561859 - trainLoss: 0.4314424991607666\n",
      "[1] cnt: 5 - valLoss: 0.4063822031021118 - trainLoss: 0.43371161818504333\n",
      "[1] cnt: 6 - valLoss: 0.3952656686306 - trainLoss: 0.43431463837623596\n",
      "[1] cnt: 7 - valLoss: 0.40841904282569885 - trainLoss: 0.4360888600349426\n",
      "[1] cnt: 8 - valLoss: 0.39687007665634155 - trainLoss: 0.4358706474304199\n",
      "[1] cnt: 9 - valLoss: 0.40860772132873535 - trainLoss: 0.4382173717021942\n",
      "[1] cnt: 10 - valLoss: 0.3936702013015747 - trainLoss: 0.4357432723045349\n",
      "[1] cnt: 11 - valLoss: 0.40478983521461487 - trainLoss: 0.4339529573917389\n",
      "[1] cnt: 12 - valLoss: 0.39082616567611694 - trainLoss: 0.43146124482154846\n",
      "[1] cnt: 13 - valLoss: 0.401176780462265 - trainLoss: 0.4296942353248596\n",
      "[1] cnt: 0 - valLoss: 0.3885495662689209 - trainLoss: 0.4273768663406372\n",
      "[1] cnt: 1 - valLoss: 0.39877811074256897 - trainLoss: 0.42610418796539307\n",
      "[1] cnt: 0 - valLoss: 0.3868335783481598 - trainLoss: 0.4243431091308594\n",
      "[1] cnt: 1 - valLoss: 0.3967725336551666 - trainLoss: 0.42327165603637695\n",
      "[1] cnt: 0 - valLoss: 0.385498046875 - trainLoss: 0.42166754603385925\n",
      "[1] cnt: 1 - valLoss: 0.39495134353637695 - trainLoss: 0.4209105670452118\n",
      "[1] cnt: 0 - valLoss: 0.38448935747146606 - trainLoss: 0.4191713035106659\n",
      "[1] cnt: 1 - valLoss: 0.39406856894493103 - trainLoss: 0.418856680393219\n",
      "[1] cnt: 0 - valLoss: 0.3840409815311432 - trainLoss: 0.41758599877357483\n",
      "[1] cnt: 1 - valLoss: 0.39409321546554565 - trainLoss: 0.4177899956703186\n",
      "[1] cnt: 0 - valLoss: 0.38387903571128845 - trainLoss: 0.41681596636772156\n",
      "[1] cnt: 1 - valLoss: 0.3940731883049011 - trainLoss: 0.41711509227752686\n",
      "[1] cnt: 2 - valLoss: 0.38389113545417786 - trainLoss: 0.4160913825035095\n",
      "[1] cnt: 3 - valLoss: 0.3946159780025482 - trainLoss: 0.4166710674762726\n",
      "[1] cnt: 4 - valLoss: 0.38394612073898315 - trainLoss: 0.4158124625682831\n",
      "[1] cnt: 5 - valLoss: 0.39553961157798767 - trainLoss: 0.416456013917923\n",
      "[1] cnt: 6 - valLoss: 0.3850123882293701 - trainLoss: 0.4159889817237854\n",
      "[1] cnt: 7 - valLoss: 0.3973165452480316 - trainLoss: 0.41788041591644287\n",
      "[1] cnt: 8 - valLoss: 0.3854113519191742 - trainLoss: 0.4171711802482605\n",
      "[1] cnt: 9 - valLoss: 0.39819785952568054 - trainLoss: 0.4183667004108429\n",
      "[1] cnt: 10 - valLoss: 0.38547447323799133 - trainLoss: 0.417457640171051\n",
      "[1] cnt: 11 - valLoss: 0.398541122674942 - trainLoss: 0.4184892475605011\n",
      "[1] cnt: 12 - valLoss: 0.38526996970176697 - trainLoss: 0.41720083355903625\n",
      "[1] cnt: 13 - valLoss: 0.3982662558555603 - trainLoss: 0.418040931224823\n",
      "[1] cnt: 14 - valLoss: 0.3844400942325592 - trainLoss: 0.4163961112499237\n",
      "[1] cnt: 15 - valLoss: 0.3974791467189789 - trainLoss: 0.41674500703811646\n",
      "[1] cnt: 0 - valLoss: 0.3838026821613312 - trainLoss: 0.4150921702384949\n",
      "[1] cnt: 1 - valLoss: 0.39686450362205505 - trainLoss: 0.41561341285705566\n",
      "[1] cnt: 0 - valLoss: 0.38303089141845703 - trainLoss: 0.41396498680114746\n",
      "[1] cnt: 1 - valLoss: 0.39571595191955566 - trainLoss: 0.41430097818374634\n",
      "[1] cnt: 0 - valLoss: 0.38203996419906616 - trainLoss: 0.4123627245426178\n",
      "[1] cnt: 1 - valLoss: 0.39491501450538635 - trainLoss: 0.41272246837615967\n",
      "[1] cnt: 0 - valLoss: 0.38168296217918396 - trainLoss: 0.41091352701187134\n",
      "[1] cnt: 1 - valLoss: 0.3945499360561371 - trainLoss: 0.41177934408187866\n",
      "[1] cnt: 0 - valLoss: 0.38117510080337524 - trainLoss: 0.4101298749446869\n",
      "[1] cnt: 1 - valLoss: 0.39401113986968994 - trainLoss: 0.4106808602809906\n",
      "[1] cnt: 0 - valLoss: 0.3808327317237854 - trainLoss: 0.4089970290660858\n",
      "[1] cnt: 1 - valLoss: 0.3952835500240326 - trainLoss: 0.4097099006175995\n",
      "[1] cnt: 2 - valLoss: 0.38199445605278015 - trainLoss: 0.40942177176475525\n",
      "[1] cnt: 3 - valLoss: 0.39756855368614197 - trainLoss: 0.411204069852829\n",
      "[1] cnt: 4 - valLoss: 0.38310495018959045 - trainLoss: 0.41085365414619446\n",
      "[1] cnt: 5 - valLoss: 0.3999112844467163 - trainLoss: 0.4125899374485016\n",
      "[1] cnt: 6 - valLoss: 0.3840008080005646 - trainLoss: 0.412263423204422\n",
      "[1] cnt: 7 - valLoss: 0.40040475130081177 - trainLoss: 0.41389229893684387\n",
      "[1] cnt: 8 - valLoss: 0.38356122374534607 - trainLoss: 0.4121227264404297\n",
      "[1] cnt: 9 - valLoss: 0.39886438846588135 - trainLoss: 0.41265779733657837\n",
      "[1] cnt: 10 - valLoss: 0.38237065076828003 - trainLoss: 0.4102744162082672\n",
      "[1] cnt: 11 - valLoss: 0.39728277921676636 - trainLoss: 0.4103964865207672\n",
      "[1] cnt: 12 - valLoss: 0.3812884986400604 - trainLoss: 0.4083292484283447\n",
      "[1] cnt: 13 - valLoss: 0.3953777551651001 - trainLoss: 0.4081709086894989\n",
      "[1] cnt: 0 - valLoss: 0.380216121673584 - trainLoss: 0.406211793422699\n",
      "[1] cnt: 1 - valLoss: 0.39419466257095337 - trainLoss: 0.40606147050857544\n",
      "[1] cnt: 0 - valLoss: 0.3796099126338959 - trainLoss: 0.4045882523059845\n",
      "[1] cnt: 1 - valLoss: 0.39321622252464294 - trainLoss: 0.4045737683773041\n",
      "[1] cnt: 0 - valLoss: 0.3789810538291931 - trainLoss: 0.40329045057296753\n",
      "[1] cnt: 1 - valLoss: 0.392531156539917 - trainLoss: 0.4032617509365082\n",
      "[1] cnt: 0 - valLoss: 0.37860387563705444 - trainLoss: 0.4021126329898834\n",
      "[1] cnt: 1 - valLoss: 0.3921589255332947 - trainLoss: 0.40230727195739746\n",
      "[1] cnt: 0 - valLoss: 0.3783537447452545 - trainLoss: 0.4012584686279297\n",
      "[1] cnt: 1 - valLoss: 0.39188989996910095 - trainLoss: 0.40141353011131287\n",
      "[1] cnt: 2 - valLoss: 0.3783697485923767 - trainLoss: 0.40053078532218933\n",
      "[1] cnt: 3 - valLoss: 0.3923879563808441 - trainLoss: 0.4010169804096222\n",
      "[1] cnt: 4 - valLoss: 0.3785921037197113 - trainLoss: 0.400411993265152\n",
      "[1] cnt: 5 - valLoss: 0.39317041635513306 - trainLoss: 0.40114012360572815\n",
      "[1] cnt: 6 - valLoss: 0.3789827823638916 - trainLoss: 0.4005725085735321\n",
      "[1] cnt: 7 - valLoss: 0.3948029577732086 - trainLoss: 0.40162962675094604\n",
      "[1] cnt: 8 - valLoss: 0.3797902762889862 - trainLoss: 0.4015181064605713\n",
      "[1] cnt: 9 - valLoss: 0.3963017165660858 - trainLoss: 0.4028915464878082\n",
      "[1] cnt: 10 - valLoss: 0.3805207312107086 - trainLoss: 0.40228959918022156\n",
      "[1] cnt: 11 - valLoss: 0.3966294825077057 - trainLoss: 0.4038330018520355\n",
      "[1] cnt: 12 - valLoss: 0.3801613748073578 - trainLoss: 0.402330607175827\n",
      "[1] cnt: 13 - valLoss: 0.3965042531490326 - trainLoss: 0.4028041660785675\n",
      "[1] cnt: 14 - valLoss: 0.3802070617675781 - trainLoss: 0.40153685212135315\n",
      "[1] cnt: 15 - valLoss: 0.3961564004421234 - trainLoss: 0.4026761054992676\n",
      "[1] cnt: 16 - valLoss: 0.3797000050544739 - trainLoss: 0.4010874032974243\n",
      "[1] cnt: 17 - valLoss: 0.3956482708454132 - trainLoss: 0.401333749294281\n",
      "[1] cnt: 18 - valLoss: 0.37957504391670227 - trainLoss: 0.3999183475971222\n",
      "[1] cnt: 19 - valLoss: 0.3947957754135132 - trainLoss: 0.4008624851703644\n",
      "[1] cnt: 20 - valLoss: 0.37854626774787903 - trainLoss: 0.3990500867366791\n",
      "[1] cnt: 21 - valLoss: 0.39329010248184204 - trainLoss: 0.39863210916519165\n",
      "[1] cnt: 0 - valLoss: 0.37803468108177185 - trainLoss: 0.3970872461795807\n",
      "[1] cnt: 1 - valLoss: 0.3931967616081238 - trainLoss: 0.3972301483154297\n",
      "[1] cnt: 2 - valLoss: 0.3780829608440399 - trainLoss: 0.3963441252708435\n",
      "[1] cnt: 3 - valLoss: 0.39331045746803284 - trainLoss: 0.39695605635643005\n",
      "[1] cnt: 4 - valLoss: 0.37806615233421326 - trainLoss: 0.396037220954895\n",
      "[1] cnt: 5 - valLoss: 0.3936486542224884 - trainLoss: 0.3966223895549774\n",
      "[1] cnt: 6 - valLoss: 0.3783522844314575 - trainLoss: 0.39582130312919617\n",
      "[1] cnt: 7 - valLoss: 0.39492833614349365 - trainLoss: 0.39688658714294434\n",
      "[1] cnt: 8 - valLoss: 0.3788057565689087 - trainLoss: 0.39637571573257446\n",
      "[1] cnt: 9 - valLoss: 0.39604654908180237 - trainLoss: 0.39749836921691895\n",
      "[1] cnt: 10 - valLoss: 0.37925514578819275 - trainLoss: 0.39694029092788696\n",
      "[1] cnt: 11 - valLoss: 0.3973526954650879 - trainLoss: 0.3983871638774872\n",
      "[1] cnt: 12 - valLoss: 0.3797183036804199 - trainLoss: 0.3975723385810852\n",
      "[1] cnt: 13 - valLoss: 0.398442804813385 - trainLoss: 0.39903196692466736\n",
      "[1] cnt: 14 - valLoss: 0.38006603717803955 - trainLoss: 0.3982369303703308\n",
      "[1] cnt: 15 - valLoss: 0.3982064127922058 - trainLoss: 0.39954909682273865\n",
      "[1] cnt: 16 - valLoss: 0.3795316815376282 - trainLoss: 0.3977489173412323\n",
      "[1] cnt: 17 - valLoss: 0.3975411355495453 - trainLoss: 0.3978259861469269\n",
      "[1] cnt: 18 - valLoss: 0.37948572635650635 - trainLoss: 0.3966245949268341\n",
      "[1] cnt: 19 - valLoss: 0.3970052897930145 - trainLoss: 0.3975161910057068\n",
      "[1] cnt: 20 - valLoss: 0.37911316752433777 - trainLoss: 0.39600932598114014\n",
      "[1] cnt: 21 - valLoss: 0.39646947383880615 - trainLoss: 0.3963119387626648\n",
      "[1] cnt: 22 - valLoss: 0.3788791000843048 - trainLoss: 0.395148903131485\n",
      "[1] cnt: 23 - valLoss: 0.3961358666419983 - trainLoss: 0.3954980969429016\n",
      "[1] cnt: 24 - valLoss: 0.3787190318107605 - trainLoss: 0.3945217430591583\n",
      "[1] cnt: 25 - valLoss: 0.39483892917633057 - trainLoss: 0.39453163743019104\n",
      "[1] cnt: 26 - valLoss: 0.37811964750289917 - trainLoss: 0.3930927813053131\n",
      "[1] cnt: 27 - valLoss: 0.39329805970191956 - trainLoss: 0.39269062876701355\n",
      "[1] cnt: 0 - valLoss: 0.377461701631546 - trainLoss: 0.39131051301956177\n",
      "[1] cnt: 1 - valLoss: 0.3918311297893524 - trainLoss: 0.3911016285419464\n",
      "[1] cnt: 0 - valLoss: 0.3771117627620697 - trainLoss: 0.3897983133792877\n",
      "[1] cnt: 1 - valLoss: 0.39105236530303955 - trainLoss: 0.3895827829837799\n",
      "[1] cnt: 0 - valLoss: 0.3767499029636383 - trainLoss: 0.3886551856994629\n",
      "[1] cnt: 1 - valLoss: 0.39074113965034485 - trainLoss: 0.38872766494750977\n",
      "[1] cnt: 2 - valLoss: 0.3769168257713318 - trainLoss: 0.38803794980049133\n",
      "[1] cnt: 3 - valLoss: 0.39175376296043396 - trainLoss: 0.38861966133117676\n",
      "[1] cnt: 4 - valLoss: 0.3772624433040619 - trainLoss: 0.38833051919937134\n",
      "[1] cnt: 5 - valLoss: 0.39306098222732544 - trainLoss: 0.38917699456214905\n",
      "[1] cnt: 6 - valLoss: 0.37776413559913635 - trainLoss: 0.38891610503196716\n",
      "[1] cnt: 7 - valLoss: 0.3952528238296509 - trainLoss: 0.39042389392852783\n",
      "[1] cnt: 8 - valLoss: 0.3787109851837158 - trainLoss: 0.39029908180236816\n",
      "[1] cnt: 9 - valLoss: 0.3981805443763733 - trainLoss: 0.39202287793159485\n",
      "[1] cnt: 10 - valLoss: 0.37983861565589905 - trainLoss: 0.3921528458595276\n",
      "[1] cnt: 11 - valLoss: 0.40052610635757446 - trainLoss: 0.3948863446712494\n",
      "[1] cnt: 12 - valLoss: 0.3810099959373474 - trainLoss: 0.39400115609169006\n",
      "[1] cnt: 13 - valLoss: 0.40226230025291443 - trainLoss: 0.3965044617652893\n",
      "[1] cnt: 14 - valLoss: 0.38168612122535706 - trainLoss: 0.39503753185272217\n",
      "[1] cnt: 15 - valLoss: 0.40325161814689636 - trainLoss: 0.3977789580821991\n",
      "[1] cnt: 16 - valLoss: 0.3817884922027588 - trainLoss: 0.3956547975540161\n",
      "[1] cnt: 17 - valLoss: 0.40224793553352356 - trainLoss: 0.3978055715560913\n",
      "[1] cnt: 18 - valLoss: 0.38064077496528625 - trainLoss: 0.3943362534046173\n",
      "[1] cnt: 19 - valLoss: 0.3998258709907532 - trainLoss: 0.3945028483867645\n",
      "[1] cnt: 20 - valLoss: 0.37964433431625366 - trainLoss: 0.39189833402633667\n",
      "[1] cnt: 21 - valLoss: 0.39747321605682373 - trainLoss: 0.3924955725669861\n",
      "[1] cnt: 22 - valLoss: 0.37874603271484375 - trainLoss: 0.3896697759628296\n",
      "[1] cnt: 23 - valLoss: 0.39550432562828064 - trainLoss: 0.3894340395927429\n",
      "[1] cnt: 24 - valLoss: 0.37822380661964417 - trainLoss: 0.387676864862442\n",
      "[1] cnt: 25 - valLoss: 0.3940807580947876 - trainLoss: 0.3879019320011139\n",
      "[1] cnt: 26 - valLoss: 0.3779100179672241 - trainLoss: 0.3861770033836365\n",
      "[1] cnt: 27 - valLoss: 0.3928947448730469 - trainLoss: 0.3863891661167145\n",
      "[1] cnt: 28 - valLoss: 0.37742912769317627 - trainLoss: 0.38495129346847534\n",
      "[1] cnt: 29 - valLoss: 0.391782283782959 - trainLoss: 0.38496384024620056\n",
      "[1] cnt: 30 - valLoss: 0.37747809290885925 - trainLoss: 0.38374873995780945\n",
      "[1] cnt: 31 - valLoss: 0.39134085178375244 - trainLoss: 0.3839159905910492\n",
      "[1] cnt: 32 - valLoss: 0.37734106183052063 - trainLoss: 0.38290002942085266\n",
      "[1] cnt: 33 - valLoss: 0.39137428998947144 - trainLoss: 0.38320961594581604\n",
      "[1] cnt: 34 - valLoss: 0.37756794691085815 - trainLoss: 0.38243135809898376\n",
      "[1] cnt: 35 - valLoss: 0.39269107580184937 - trainLoss: 0.3829914331436157\n",
      "[1] cnt: 36 - valLoss: 0.3779992461204529 - trainLoss: 0.3828903138637543\n",
      "[1] cnt: 37 - valLoss: 0.3951931595802307 - trainLoss: 0.38447150588035583\n",
      "[1] cnt: 38 - valLoss: 0.37892991304397583 - trainLoss: 0.38444754481315613\n",
      "[1] cnt: 39 - valLoss: 0.3987483084201813 - trainLoss: 0.3863649070262909\n",
      "[1] cnt: 40 - valLoss: 0.38044077157974243 - trainLoss: 0.3867061734199524\n",
      "[1] cnt: 41 - valLoss: 0.40435829758644104 - trainLoss: 0.390491247177124\n",
      "[1] cnt: 42 - valLoss: 0.3830082416534424 - trainLoss: 0.3909943997859955\n",
      "[1] cnt: 43 - valLoss: 0.4078124761581421 - trainLoss: 0.395753413438797\n",
      "[1] cnt: 44 - valLoss: 0.38330304622650146 - trainLoss: 0.39369940757751465\n",
      "[1] cnt: 45 - valLoss: 0.4068394601345062 - trainLoss: 0.3971334397792816\n",
      "[1] cnt: 46 - valLoss: 0.38210538029670715 - trainLoss: 0.3928391635417938\n",
      "[1] cnt: 47 - valLoss: 0.40397027134895325 - trainLoss: 0.39399561285972595\n",
      "[1] cnt: 48 - valLoss: 0.3808130919933319 - trainLoss: 0.39009180665016174\n",
      "[1] cnt: 49 - valLoss: 0.4015209674835205 - trainLoss: 0.39025217294692993\n",
      "[1] cnt: 50 - valLoss: 0.3803568184375763 - trainLoss: 0.38756683468818665\n",
      "[1] cnt: 51 - valLoss: 0.400360643863678 - trainLoss: 0.3885854482650757\n",
      "[1] cnt: 52 - valLoss: 0.3799281418323517 - trainLoss: 0.3862694501876831\n",
      "[1] cnt: 53 - valLoss: 0.3986930847167969 - trainLoss: 0.38663724064826965\n",
      "[1] cnt: 54 - valLoss: 0.37930914759635925 - trainLoss: 0.3846249580383301\n",
      "[1] cnt: 55 - valLoss: 0.39710110425949097 - trainLoss: 0.38490504026412964\n",
      "[1] cnt: 56 - valLoss: 0.37913620471954346 - trainLoss: 0.38309159874916077\n",
      "[1] cnt: 57 - valLoss: 0.3959798812866211 - trainLoss: 0.38296258449554443\n",
      "[1] cnt: 58 - valLoss: 0.3789558410644531 - trainLoss: 0.38173767924308777\n",
      "[1] cnt: 59 - valLoss: 0.39550694823265076 - trainLoss: 0.38234296441078186\n",
      "[1] cnt: 60 - valLoss: 0.3792716860771179 - trainLoss: 0.38106244802474976\n",
      "[1] cnt: 61 - valLoss: 0.394949734210968 - trainLoss: 0.3811406195163727\n",
      "[1] cnt: 62 - valLoss: 0.3790515065193176 - trainLoss: 0.3800943195819855\n",
      "[1] cnt: 63 - valLoss: 0.3948259651660919 - trainLoss: 0.38084089756011963\n",
      "[1] cnt: 64 - valLoss: 0.3795083165168762 - trainLoss: 0.3797963857650757\n",
      "[1] cnt: 65 - valLoss: 0.39612263441085815 - trainLoss: 0.38045015931129456\n",
      "[1] cnt: 66 - valLoss: 0.3796580135822296 - trainLoss: 0.38006845116615295\n",
      "[1] cnt: 67 - valLoss: 0.397874653339386 - trainLoss: 0.38139578700065613\n",
      "[1] cnt: 68 - valLoss: 0.380461186170578 - trainLoss: 0.3811202943325043\n",
      "[1] cnt: 69 - valLoss: 0.4006219506263733 - trainLoss: 0.38290077447891235\n",
      "[1] cnt: 70 - valLoss: 0.3814150393009186 - trainLoss: 0.38279709219932556\n",
      "[1] cnt: 71 - valLoss: 0.4030296504497528 - trainLoss: 0.3855087459087372\n",
      "[1] cnt: 72 - valLoss: 0.38210269808769226 - trainLoss: 0.38424497842788696\n",
      "[1] cnt: 73 - valLoss: 0.4051584005355835 - trainLoss: 0.38680747151374817\n",
      "[1] cnt: 74 - valLoss: 0.3829314112663269 - trainLoss: 0.38575026392936707\n",
      "[1] cnt: 75 - valLoss: 0.40628767013549805 - trainLoss: 0.3886406719684601\n",
      "[1] cnt: 76 - valLoss: 0.3830816447734833 - trainLoss: 0.38630765676498413\n",
      "[1] cnt: 77 - valLoss: 0.40593579411506653 - trainLoss: 0.3883714973926544\n",
      "[1] cnt: 78 - valLoss: 0.38284730911254883 - trainLoss: 0.385749489068985\n",
      "[1] cnt: 79 - valLoss: 0.4041336476802826 - trainLoss: 0.3870960474014282\n",
      "[1] cnt: 80 - valLoss: 0.3822910785675049 - trainLoss: 0.3841581344604492\n",
      "[1] cnt: 81 - valLoss: 0.40214094519615173 - trainLoss: 0.3845994770526886\n",
      "[1] cnt: 82 - valLoss: 0.3816707730293274 - trainLoss: 0.38196861743927\n",
      "[1] cnt: 83 - valLoss: 0.40033507347106934 - trainLoss: 0.3822498023509979\n",
      "[1] cnt: 84 - valLoss: 0.38147684931755066 - trainLoss: 0.38036438822746277\n",
      "[1] cnt: 85 - valLoss: 0.3987771272659302 - trainLoss: 0.38032811880111694\n",
      "[1] cnt: 86 - valLoss: 0.38149958848953247 - trainLoss: 0.37867918610572815\n",
      "[1] cnt: 87 - valLoss: 0.39799025654792786 - trainLoss: 0.3790474832057953\n",
      "[1] cnt: 88 - valLoss: 0.38127464056015015 - trainLoss: 0.3777906000614166\n",
      "[1] cnt: 89 - valLoss: 0.39786458015441895 - trainLoss: 0.37814146280288696\n",
      "[1] cnt: 90 - valLoss: 0.3817739486694336 - trainLoss: 0.3773624300956726\n",
      "[1] cnt: 91 - valLoss: 0.3981409966945648 - trainLoss: 0.377738893032074\n",
      "[1] cnt: 92 - valLoss: 0.38187146186828613 - trainLoss: 0.37698182463645935\n",
      "[1] cnt: 93 - valLoss: 0.4003922939300537 - trainLoss: 0.3783654570579529\n",
      "[1] cnt: 94 - valLoss: 0.38287708163261414 - trainLoss: 0.3782150149345398\n",
      "[1] cnt: 95 - valLoss: 0.40338975191116333 - trainLoss: 0.3799634277820587\n",
      "[1] cnt: 96 - valLoss: 0.38349372148513794 - trainLoss: 0.3800276219844818\n",
      "[1] cnt: 97 - valLoss: 0.4063025116920471 - trainLoss: 0.3828077018260956\n",
      "[1] cnt: 98 - valLoss: 0.3844892978668213 - trainLoss: 0.38215529918670654\n",
      "[1] cnt: 99 - valLoss: 0.4079795479774475 - trainLoss: 0.38480669260025024\n",
      "[1] cnt: 100 - valLoss: 0.38506418466567993 - trainLoss: 0.38315239548683167\n",
      "[0.1] cnt: 0 - valLoss: 0.38285091519355774 - trainLoss: 0.3862703740596771\n",
      "[0.1] cnt: 0 - valLoss: 0.38183268904685974 - trainLoss: 0.38060659170150757\n",
      "[0.1] cnt: 0 - valLoss: 0.3814832270145416 - trainLoss: 0.37680870294570923\n",
      "[0.1] cnt: 0 - valLoss: 0.38146302103996277 - trainLoss: 0.37429311871528625\n",
      "[0.1] cnt: 1 - valLoss: 0.3816675543785095 - trainLoss: 0.3726241886615753\n",
      "[0.1] cnt: 2 - valLoss: 0.3819635510444641 - trainLoss: 0.371508926153183\n",
      "[0.1] cnt: 3 - valLoss: 0.38231298327445984 - trainLoss: 0.3707839250564575\n",
      "[0.1] cnt: 4 - valLoss: 0.38265344500541687 - trainLoss: 0.3703019618988037\n",
      "[0.1] cnt: 5 - valLoss: 0.38296812772750854 - trainLoss: 0.36996787786483765\n",
      "[0.1] cnt: 6 - valLoss: 0.38325661420822144 - trainLoss: 0.36974653601646423\n",
      "[0.1] cnt: 7 - valLoss: 0.38352298736572266 - trainLoss: 0.3695910573005676\n",
      "[0.1] cnt: 8 - valLoss: 0.38377293944358826 - trainLoss: 0.36947867274284363\n",
      "[0.1] cnt: 9 - valLoss: 0.38398855924606323 - trainLoss: 0.369392067193985\n",
      "[0.1] cnt: 10 - valLoss: 0.3841872811317444 - trainLoss: 0.3693241775035858\n",
      "[0.1] cnt: 11 - valLoss: 0.38437938690185547 - trainLoss: 0.36926427483558655\n",
      "[0.1] cnt: 12 - valLoss: 0.38454827666282654 - trainLoss: 0.3692087233066559\n",
      "[0.1] cnt: 13 - valLoss: 0.38469305634498596 - trainLoss: 0.3691604435443878\n",
      "[0.1] cnt: 14 - valLoss: 0.3848228454589844 - trainLoss: 0.36911875009536743\n",
      "[0.1] cnt: 15 - valLoss: 0.3849443793296814 - trainLoss: 0.36907947063446045\n",
      "[0.1] cnt: 16 - valLoss: 0.3850403428077698 - trainLoss: 0.36904028058052063\n",
      "[0.1] cnt: 17 - valLoss: 0.38512808084487915 - trainLoss: 0.3690039813518524\n",
      "[0.1] cnt: 18 - valLoss: 0.3852001428604126 - trainLoss: 0.36896973848342896\n",
      "[0.1] cnt: 19 - valLoss: 0.3852631449699402 - trainLoss: 0.36893829703330994\n",
      "[0.1] cnt: 20 - valLoss: 0.38531941175460815 - trainLoss: 0.3689076006412506\n",
      "[0.1] cnt: 21 - valLoss: 0.3853706121444702 - trainLoss: 0.3688752353191376\n",
      "[0.1] cnt: 22 - valLoss: 0.38541465997695923 - trainLoss: 0.3688450753688812\n",
      "[0.1] cnt: 23 - valLoss: 0.3854682147502899 - trainLoss: 0.3688150942325592\n",
      "[0.1] cnt: 24 - valLoss: 0.3855133056640625 - trainLoss: 0.36878445744514465\n",
      "[0.1] cnt: 25 - valLoss: 0.38555994629859924 - trainLoss: 0.36875444650650024\n",
      "[0.1] cnt: 26 - valLoss: 0.3856033682823181 - trainLoss: 0.36872440576553345\n",
      "[0.1] cnt: 27 - valLoss: 0.3856506943702698 - trainLoss: 0.36869436502456665\n",
      "[0.1] cnt: 28 - valLoss: 0.38569700717926025 - trainLoss: 0.36866295337677\n",
      "[0.1] cnt: 29 - valLoss: 0.38574111461639404 - trainLoss: 0.3686315715312958\n",
      "[0.1] cnt: 30 - valLoss: 0.3857741355895996 - trainLoss: 0.3686026930809021\n",
      "[0.1] cnt: 31 - valLoss: 0.3858093321323395 - trainLoss: 0.3685745298862457\n",
      "[0.1] cnt: 32 - valLoss: 0.385833740234375 - trainLoss: 0.36854881048202515\n",
      "[0.1] cnt: 33 - valLoss: 0.38586318492889404 - trainLoss: 0.368524432182312\n",
      "[0.1] cnt: 34 - valLoss: 0.3858826458454132 - trainLoss: 0.3685004711151123\n",
      "[0.1] cnt: 35 - valLoss: 0.38592490553855896 - trainLoss: 0.3684767186641693\n",
      "[0.1] cnt: 36 - valLoss: 0.3859533965587616 - trainLoss: 0.36845093965530396\n",
      "[0.1] cnt: 37 - valLoss: 0.38598111271858215 - trainLoss: 0.3684252202510834\n",
      "[0.1] cnt: 38 - valLoss: 0.38600650429725647 - trainLoss: 0.3683994710445404\n",
      "[0.1] cnt: 39 - valLoss: 0.38603198528289795 - trainLoss: 0.3683747351169586\n",
      "[0.1] cnt: 40 - valLoss: 0.3860570788383484 - trainLoss: 0.3683505952358246\n",
      "[0.1] cnt: 41 - valLoss: 0.3860914409160614 - trainLoss: 0.3683255910873413\n",
      "[0.1] cnt: 42 - valLoss: 0.3861294984817505 - trainLoss: 0.36829835176467896\n",
      "[0.1] cnt: 43 - valLoss: 0.3861546814441681 - trainLoss: 0.368271142244339\n",
      "[0.1] cnt: 44 - valLoss: 0.38619333505630493 - trainLoss: 0.3682442307472229\n",
      "[0.1] cnt: 45 - valLoss: 0.38621851801872253 - trainLoss: 0.3682178556919098\n",
      "[0.1] cnt: 46 - valLoss: 0.3862317204475403 - trainLoss: 0.3681929111480713\n",
      "[0.1] cnt: 47 - valLoss: 0.38626226782798767 - trainLoss: 0.3681693971157074\n",
      "[0.1] cnt: 48 - valLoss: 0.38627126812934875 - trainLoss: 0.36814573407173157\n",
      "[0.1] cnt: 49 - valLoss: 0.3862929940223694 - trainLoss: 0.3681219518184662\n",
      "[0.1] cnt: 50 - valLoss: 0.3863212764263153 - trainLoss: 0.36809849739074707\n",
      "[0.1] cnt: 51 - valLoss: 0.38634923100471497 - trainLoss: 0.3680744767189026\n",
      "[0.1] cnt: 52 - valLoss: 0.38637620210647583 - trainLoss: 0.36805105209350586\n",
      "[0.1] cnt: 53 - valLoss: 0.3863927721977234 - trainLoss: 0.3680278956890106\n",
      "[0.1] cnt: 54 - valLoss: 0.3864000737667084 - trainLoss: 0.3680065870285034\n",
      "[0.1] cnt: 55 - valLoss: 0.3864212930202484 - trainLoss: 0.3679853081703186\n",
      "[0.1] cnt: 56 - valLoss: 0.38644132018089294 - trainLoss: 0.3679637312889099\n",
      "[0.1] cnt: 57 - valLoss: 0.38646620512008667 - trainLoss: 0.367941290140152\n",
      "[0.1] cnt: 58 - valLoss: 0.38646775484085083 - trainLoss: 0.36791905760765076\n",
      "[0.1] cnt: 59 - valLoss: 0.3864816427230835 - trainLoss: 0.36789730191230774\n",
      "[0.1] cnt: 60 - valLoss: 0.38649335503578186 - trainLoss: 0.3678751289844513\n",
      "[0.1] cnt: 61 - valLoss: 0.38650429248809814 - trainLoss: 0.36785367131233215\n",
      "[0.1] cnt: 62 - valLoss: 0.38651737570762634 - trainLoss: 0.36783134937286377\n",
      "[0.1] cnt: 63 - valLoss: 0.3865422308444977 - trainLoss: 0.36780983209609985\n",
      "[0.1] cnt: 64 - valLoss: 0.3865346610546112 - trainLoss: 0.36778852343559265\n",
      "[0.1] cnt: 65 - valLoss: 0.3865501880645752 - trainLoss: 0.36776694655418396\n",
      "[0.1] cnt: 66 - valLoss: 0.38656800985336304 - trainLoss: 0.36774590611457825\n",
      "[0.1] cnt: 67 - valLoss: 0.38656747341156006 - trainLoss: 0.3677244484424591\n",
      "[0.1] cnt: 68 - valLoss: 0.386580228805542 - trainLoss: 0.36770308017730713\n",
      "[0.1] cnt: 69 - valLoss: 0.38657549023628235 - trainLoss: 0.3676830530166626\n",
      "[0.1] cnt: 70 - valLoss: 0.38658350706100464 - trainLoss: 0.36766311526298523\n",
      "[0.1] cnt: 71 - valLoss: 0.38660427927970886 - trainLoss: 0.3676431477069855\n",
      "[0.1] cnt: 72 - valLoss: 0.38660743832588196 - trainLoss: 0.3676232397556305\n",
      "[0.1] cnt: 73 - valLoss: 0.3866269588470459 - trainLoss: 0.3676030933856964\n",
      "[0.1] cnt: 74 - valLoss: 0.3866296112537384 - trainLoss: 0.36758336424827576\n",
      "[0.1] cnt: 75 - valLoss: 0.38663995265960693 - trainLoss: 0.36756351590156555\n",
      "[0.1] cnt: 76 - valLoss: 0.3866533935070038 - trainLoss: 0.36754336953163147\n",
      "[0.1] cnt: 77 - valLoss: 0.3866587281227112 - trainLoss: 0.367524117231369\n",
      "[0.1] cnt: 78 - valLoss: 0.3866533935070038 - trainLoss: 0.3675057291984558\n",
      "[0.1] cnt: 79 - valLoss: 0.3866643011569977 - trainLoss: 0.3674874007701874\n",
      "[0.1] cnt: 80 - valLoss: 0.3866661787033081 - trainLoss: 0.36746945977211\n",
      "[0.1] cnt: 81 - valLoss: 0.38666340708732605 - trainLoss: 0.3674517273902893\n",
      "[0.1] cnt: 82 - valLoss: 0.38667669892311096 - trainLoss: 0.3674345314502716\n",
      "[0.1] cnt: 83 - valLoss: 0.386687695980072 - trainLoss: 0.36741676926612854\n",
      "[0.1] cnt: 84 - valLoss: 0.38666775822639465 - trainLoss: 0.3673994541168213\n",
      "[0.1] cnt: 85 - valLoss: 0.3866947293281555 - trainLoss: 0.36738309264183044\n",
      "[0.1] cnt: 86 - valLoss: 0.3867018222808838 - trainLoss: 0.36736464500427246\n",
      "[0.1] cnt: 87 - valLoss: 0.3867059648036957 - trainLoss: 0.36734724044799805\n",
      "[0.1] cnt: 88 - valLoss: 0.38672325015068054 - trainLoss: 0.3673297166824341\n",
      "[0.1] cnt: 89 - valLoss: 0.3867250084877014 - trainLoss: 0.36731213331222534\n",
      "[0.1] cnt: 90 - valLoss: 0.3867354989051819 - trainLoss: 0.36729446053504944\n",
      "[0.1] cnt: 91 - valLoss: 0.38674914836883545 - trainLoss: 0.36727744340896606\n",
      "[0.1] cnt: 92 - valLoss: 0.3867717981338501 - trainLoss: 0.3672601878643036\n",
      "[0.1] cnt: 93 - valLoss: 0.3867506682872772 - trainLoss: 0.3672436773777008\n",
      "[0.1] cnt: 94 - valLoss: 0.3867810368537903 - trainLoss: 0.3672277629375458\n",
      "[0.1] cnt: 95 - valLoss: 0.3867737352848053 - trainLoss: 0.3672106862068176\n",
      "[0.1] cnt: 96 - valLoss: 0.3867912292480469 - trainLoss: 0.3671947419643402\n",
      "[0.1] cnt: 97 - valLoss: 0.3868062198162079 - trainLoss: 0.3671782910823822\n",
      "[0.1] cnt: 98 - valLoss: 0.3868027627468109 - trainLoss: 0.3671625852584839\n",
      "[0.1] cnt: 99 - valLoss: 0.38681671023368835 - trainLoss: 0.36714616417884827\n",
      "[0.1] cnt: 100 - valLoss: 0.3868197798728943 - trainLoss: 0.3671301007270813\n",
      "[0.010000000000000002] cnt: 0 - valLoss: 0.38682132959365845 - trainLoss: 0.3671145439147949\n",
      "[0.010000000000000002] cnt: 1 - valLoss: 0.3868229389190674 - trainLoss: 0.36711275577545166\n",
      "[0.010000000000000002] cnt: 2 - valLoss: 0.38682451844215393 - trainLoss: 0.3671109974384308\n",
      "[0.010000000000000002] cnt: 3 - valLoss: 0.3868263065814972 - trainLoss: 0.3671092391014099\n",
      "[0.010000000000000002] cnt: 4 - valLoss: 0.386827290058136 - trainLoss: 0.3671075701713562\n",
      "[0.010000000000000002] cnt: 5 - valLoss: 0.38682830333709717 - trainLoss: 0.3671058714389801\n",
      "[0.010000000000000002] cnt: 6 - valLoss: 0.3868296444416046 - trainLoss: 0.3671042025089264\n",
      "[0.010000000000000002] cnt: 7 - valLoss: 0.38683077692985535 - trainLoss: 0.36710256338119507\n",
      "[0.010000000000000002] cnt: 8 - valLoss: 0.386831670999527 - trainLoss: 0.36710089445114136\n",
      "[0.010000000000000002] cnt: 9 - valLoss: 0.38683372735977173 - trainLoss: 0.36709919571876526\n",
      "[0.010000000000000002] cnt: 10 - valLoss: 0.38683420419692993 - trainLoss: 0.36709755659103394\n",
      "[0.010000000000000002] cnt: 11 - valLoss: 0.38683560490608215 - trainLoss: 0.367095947265625\n",
      "[0.010000000000000002] cnt: 12 - valLoss: 0.3868368864059448 - trainLoss: 0.36709433794021606\n",
      "[0.010000000000000002] cnt: 13 - valLoss: 0.38683784008026123 - trainLoss: 0.36709269881248474\n",
      "[0.010000000000000002] cnt: 14 - valLoss: 0.3868388831615448 - trainLoss: 0.3670910596847534\n",
      "[0.010000000000000002] cnt: 15 - valLoss: 0.38684093952178955 - trainLoss: 0.3670894503593445\n",
      "[0.010000000000000002] cnt: 16 - valLoss: 0.38684093952178955 - trainLoss: 0.36708784103393555\n",
      "[0.010000000000000002] cnt: 17 - valLoss: 0.38684356212615967 - trainLoss: 0.36708617210388184\n",
      "[0.010000000000000002] cnt: 18 - valLoss: 0.38684335350990295 - trainLoss: 0.3670846223831177\n",
      "[0.010000000000000002] cnt: 19 - valLoss: 0.3868442475795746 - trainLoss: 0.3670829236507416\n",
      "[0.010000000000000002] cnt: 20 - valLoss: 0.38684478402137756 - trainLoss: 0.3670814037322998\n",
      "[0.010000000000000002] cnt: 21 - valLoss: 0.3868463635444641 - trainLoss: 0.3670797646045685\n",
      "[0.010000000000000002] cnt: 22 - valLoss: 0.38684675097465515 - trainLoss: 0.36707812547683716\n",
      "[0.010000000000000002] cnt: 23 - valLoss: 0.38684728741645813 - trainLoss: 0.367076575756073\n",
      "[0.010000000000000002] cnt: 24 - valLoss: 0.38684847950935364 - trainLoss: 0.3670749068260193\n",
      "[0.010000000000000002] cnt: 25 - valLoss: 0.38684865832328796 - trainLoss: 0.36707326769828796\n",
      "[0.010000000000000002] cnt: 26 - valLoss: 0.38685011863708496 - trainLoss: 0.3670716881752014\n",
      "[0.010000000000000002] cnt: 27 - valLoss: 0.3868514597415924 - trainLoss: 0.3670700192451477\n",
      "[0.010000000000000002] cnt: 28 - valLoss: 0.3868531584739685 - trainLoss: 0.36706843972206116\n",
      "[0.010000000000000002] cnt: 29 - valLoss: 0.38685351610183716 - trainLoss: 0.36706680059432983\n",
      "[0.010000000000000002] cnt: 30 - valLoss: 0.38685476779937744 - trainLoss: 0.3670652210712433\n",
      "[0.010000000000000002] cnt: 31 - valLoss: 0.3868563771247864 - trainLoss: 0.36706358194351196\n",
      "[0.010000000000000002] cnt: 32 - valLoss: 0.38685622811317444 - trainLoss: 0.367061972618103\n",
      "[0.010000000000000002] cnt: 33 - valLoss: 0.3868572413921356 - trainLoss: 0.3670603334903717\n",
      "[0.010000000000000002] cnt: 34 - valLoss: 0.3868587911128998 - trainLoss: 0.3670586943626404\n",
      "[0.010000000000000002] cnt: 35 - valLoss: 0.38685768842697144 - trainLoss: 0.36705711483955383\n",
      "[0.010000000000000002] cnt: 36 - valLoss: 0.3868592083454132 - trainLoss: 0.3670555651187897\n",
      "[0.010000000000000002] cnt: 37 - valLoss: 0.38686078786849976 - trainLoss: 0.36705389618873596\n",
      "[0.010000000000000002] cnt: 38 - valLoss: 0.38686129450798035 - trainLoss: 0.3670523762702942\n",
      "[0.010000000000000002] cnt: 39 - valLoss: 0.3868620693683624 - trainLoss: 0.36705073714256287\n",
      "[0.010000000000000002] cnt: 40 - valLoss: 0.3868629038333893 - trainLoss: 0.3670491576194763\n",
      "[0.010000000000000002] cnt: 41 - valLoss: 0.38686466217041016 - trainLoss: 0.367047518491745\n",
      "[0.010000000000000002] cnt: 42 - valLoss: 0.38686421513557434 - trainLoss: 0.36704587936401367\n",
      "[0.010000000000000002] cnt: 43 - valLoss: 0.38686588406562805 - trainLoss: 0.3670443594455719\n",
      "[0.010000000000000002] cnt: 44 - valLoss: 0.3868671655654907 - trainLoss: 0.3670427203178406\n",
      "[0.010000000000000002] cnt: 45 - valLoss: 0.3868664503097534 - trainLoss: 0.36704114079475403\n",
      "[0.010000000000000002] cnt: 46 - valLoss: 0.3868682086467743 - trainLoss: 0.36703959107398987\n",
      "[0.010000000000000002] cnt: 47 - valLoss: 0.38686853647232056 - trainLoss: 0.36703795194625854\n",
      "[0.010000000000000002] cnt: 48 - valLoss: 0.3868705928325653 - trainLoss: 0.3670364320278168\n",
      "[0.010000000000000002] cnt: 49 - valLoss: 0.38687077164649963 - trainLoss: 0.36703479290008545\n",
      "[0.010000000000000002] cnt: 50 - valLoss: 0.38687214255332947 - trainLoss: 0.3670332431793213\n",
      "[0.010000000000000002] cnt: 51 - valLoss: 0.3868730664253235 - trainLoss: 0.3670315742492676\n",
      "[0.010000000000000002] cnt: 52 - valLoss: 0.3868747353553772 - trainLoss: 0.3670300543308258\n",
      "[0.010000000000000002] cnt: 53 - valLoss: 0.3868737518787384 - trainLoss: 0.36702844500541687\n",
      "[0.010000000000000002] cnt: 54 - valLoss: 0.3868749439716339 - trainLoss: 0.36702683568000793\n",
      "[0.010000000000000002] cnt: 55 - valLoss: 0.38687631487846375 - trainLoss: 0.3670252859592438\n",
      "[0.010000000000000002] cnt: 56 - valLoss: 0.38687726855278015 - trainLoss: 0.3670237064361572\n",
      "[0.010000000000000002] cnt: 57 - valLoss: 0.3868773281574249 - trainLoss: 0.3670220971107483\n",
      "[0.010000000000000002] cnt: 58 - valLoss: 0.38687950372695923 - trainLoss: 0.36702051758766174\n",
      "[0.010000000000000002] cnt: 59 - valLoss: 0.38688036799430847 - trainLoss: 0.3670189082622528\n",
      "[0.010000000000000002] cnt: 60 - valLoss: 0.38688069581985474 - trainLoss: 0.3670174181461334\n",
      "[0.010000000000000002] cnt: 61 - valLoss: 0.38688161969184875 - trainLoss: 0.3670157790184021\n",
      "[0.010000000000000002] cnt: 62 - valLoss: 0.3868839144706726 - trainLoss: 0.36701419949531555\n",
      "[0.010000000000000002] cnt: 63 - valLoss: 0.3868820369243622 - trainLoss: 0.367012619972229\n",
      "[0.010000000000000002] cnt: 64 - valLoss: 0.38688454031944275 - trainLoss: 0.36701104044914246\n",
      "[0.010000000000000002] cnt: 65 - valLoss: 0.3868853449821472 - trainLoss: 0.36700937151908875\n",
      "[0.010000000000000002] cnt: 66 - valLoss: 0.38688623905181885 - trainLoss: 0.36700788140296936\n",
      "[0.010000000000000002] cnt: 67 - valLoss: 0.38688719272613525 - trainLoss: 0.36700624227523804\n",
      "[0.010000000000000002] cnt: 68 - valLoss: 0.3868884742259979 - trainLoss: 0.3670046925544739\n",
      "[0.010000000000000002] cnt: 69 - valLoss: 0.386888325214386 - trainLoss: 0.36700311303138733\n",
      "[0.010000000000000002] cnt: 70 - valLoss: 0.3868904709815979 - trainLoss: 0.3670015335083008\n",
      "[0.010000000000000002] cnt: 71 - valLoss: 0.386891633272171 - trainLoss: 0.36699995398521423\n",
      "[0.010000000000000002] cnt: 72 - valLoss: 0.38689157366752625 - trainLoss: 0.3669983446598053\n",
      "[0.010000000000000002] cnt: 73 - valLoss: 0.38689306378364563 - trainLoss: 0.3669968247413635\n",
      "[0.010000000000000002] cnt: 74 - valLoss: 0.3868941366672516 - trainLoss: 0.3669951856136322\n",
      "[0.010000000000000002] cnt: 75 - valLoss: 0.38689374923706055 - trainLoss: 0.36699360609054565\n",
      "[0.010000000000000002] cnt: 76 - valLoss: 0.3868955075740814 - trainLoss: 0.3669920265674591\n",
      "[0.010000000000000002] cnt: 77 - valLoss: 0.3868962824344635 - trainLoss: 0.36699041724205017\n",
      "[0.010000000000000002] cnt: 78 - valLoss: 0.38689813017845154 - trainLoss: 0.3669888377189636\n",
      "[0.010000000000000002] cnt: 79 - valLoss: 0.3868980407714844 - trainLoss: 0.36698728799819946\n",
      "[0.010000000000000002] cnt: 80 - valLoss: 0.38689950108528137 - trainLoss: 0.3669857382774353\n",
      "[0.010000000000000002] cnt: 81 - valLoss: 0.38690072298049927 - trainLoss: 0.366984099149704\n",
      "[0.010000000000000002] cnt: 82 - valLoss: 0.3869001269340515 - trainLoss: 0.3669825494289398\n",
      "[0.010000000000000002] cnt: 83 - valLoss: 0.3869020342826843 - trainLoss: 0.36698099970817566\n",
      "[0.010000000000000002] cnt: 84 - valLoss: 0.386902779340744 - trainLoss: 0.36697936058044434\n",
      "[0.010000000000000002] cnt: 85 - valLoss: 0.3869049847126007 - trainLoss: 0.36697784066200256\n",
      "[0.010000000000000002] cnt: 86 - valLoss: 0.38690468668937683 - trainLoss: 0.36697623133659363\n",
      "[0.010000000000000002] cnt: 87 - valLoss: 0.38690653443336487 - trainLoss: 0.3669746220111847\n",
      "[0.010000000000000002] cnt: 88 - valLoss: 0.38690629601478577 - trainLoss: 0.36697304248809814\n",
      "[0.010000000000000002] cnt: 89 - valLoss: 0.3869074881076813 - trainLoss: 0.36697152256965637\n",
      "[0.010000000000000002] cnt: 90 - valLoss: 0.386909157037735 - trainLoss: 0.36696991324424744\n",
      "[0.010000000000000002] cnt: 91 - valLoss: 0.38690903782844543 - trainLoss: 0.3669682741165161\n",
      "[0.010000000000000002] cnt: 92 - valLoss: 0.3869115114212036 - trainLoss: 0.3669668138027191\n",
      "[0.010000000000000002] cnt: 93 - valLoss: 0.3869108557701111 - trainLoss: 0.36696523427963257\n",
      "[0.010000000000000002] cnt: 94 - valLoss: 0.386913001537323 - trainLoss: 0.366963654756546\n",
      "[0.010000000000000002] cnt: 95 - valLoss: 0.38691237568855286 - trainLoss: 0.3669619858264923\n",
      "[0.010000000000000002] cnt: 96 - valLoss: 0.3869146704673767 - trainLoss: 0.3669605255126953\n",
      "[0.010000000000000002] cnt: 97 - valLoss: 0.38691529631614685 - trainLoss: 0.3669588565826416\n",
      "[0.010000000000000002] cnt: 98 - valLoss: 0.38691574335098267 - trainLoss: 0.36695733666419983\n",
      "[0.010000000000000002] cnt: 99 - valLoss: 0.3869174122810364 - trainLoss: 0.3669557273387909\n",
      "[0.010000000000000002] cnt: 100 - valLoss: 0.38691842555999756 - trainLoss: 0.36695417761802673\n",
      "[0.0010000000000000002] cnt: 0 - valLoss: 0.3869183659553528 - trainLoss: 0.36695265769958496\n",
      "[0.0010000000000000002] cnt: 0 - valLoss: 0.386918306350708 - trainLoss: 0.36695244908332825\n",
      "[0.0010000000000000002] cnt: 1 - valLoss: 0.3869183659553528 - trainLoss: 0.3669522702693939\n",
      "[0.0010000000000000002] cnt: 2 - valLoss: 0.38691842555999756 - trainLoss: 0.3669520616531372\n",
      "[0.0010000000000000002] cnt: 3 - valLoss: 0.3869185447692871 - trainLoss: 0.36695191264152527\n",
      "[0.0010000000000000002] cnt: 4 - valLoss: 0.3869185447692871 - trainLoss: 0.36695173382759094\n",
      "[0.0010000000000000002] cnt: 5 - valLoss: 0.38691872358322144 - trainLoss: 0.3669516146183014\n",
      "[0.0010000000000000002] cnt: 6 - valLoss: 0.386918842792511 - trainLoss: 0.36695146560668945\n",
      "[0.0010000000000000002] cnt: 7 - valLoss: 0.386918842792511 - trainLoss: 0.36695125699043274\n",
      "[0.0010000000000000002] cnt: 8 - valLoss: 0.38691893219947815 - trainLoss: 0.3669511079788208\n",
      "[0.0010000000000000002] cnt: 9 - valLoss: 0.3869190514087677 - trainLoss: 0.3669509291648865\n",
      "[0.0010000000000000002] cnt: 10 - valLoss: 0.3869190216064453 - trainLoss: 0.36695078015327454\n",
      "[0.0010000000000000002] cnt: 11 - valLoss: 0.386919230222702 - trainLoss: 0.3669506311416626\n",
      "[0.0010000000000000002] cnt: 12 - valLoss: 0.3869193196296692 - trainLoss: 0.36695045232772827\n",
      "[0.0010000000000000002] cnt: 13 - valLoss: 0.3869193196296692 - trainLoss: 0.36695030331611633\n",
      "[0.0010000000000000002] cnt: 14 - valLoss: 0.38691940903663635 - trainLoss: 0.3669501841068268\n",
      "[0.0010000000000000002] cnt: 15 - valLoss: 0.3869195282459259 - trainLoss: 0.36695000529289246\n",
      "[0.0010000000000000002] cnt: 16 - valLoss: 0.38691967725753784 - trainLoss: 0.36694982647895813\n",
      "[0.0010000000000000002] cnt: 17 - valLoss: 0.3869197964668274 - trainLoss: 0.3669496774673462\n",
      "[0.0010000000000000002] cnt: 18 - valLoss: 0.3869198262691498 - trainLoss: 0.36694952845573425\n",
      "[0.0010000000000000002] cnt: 19 - valLoss: 0.38691994547843933 - trainLoss: 0.3669493794441223\n",
      "[0.0010000000000000002] cnt: 20 - valLoss: 0.3869202136993408 - trainLoss: 0.366949200630188\n",
      "[0.0010000000000000002] cnt: 21 - valLoss: 0.3869200646877289 - trainLoss: 0.36694905161857605\n",
      "[0.0010000000000000002] cnt: 22 - valLoss: 0.3869202435016632 - trainLoss: 0.3669488728046417\n",
      "[0.0010000000000000002] cnt: 23 - valLoss: 0.3869202435016632 - trainLoss: 0.3669487535953522\n",
      "[0.0010000000000000002] cnt: 24 - valLoss: 0.3869204521179199 - trainLoss: 0.36694857478141785\n",
      "[0.0010000000000000002] cnt: 25 - valLoss: 0.3869204521179199 - trainLoss: 0.3669484257698059\n",
      "[0.0010000000000000002] cnt: 26 - valLoss: 0.38692060112953186 - trainLoss: 0.3669482469558716\n",
      "[0.0010000000000000002] cnt: 27 - valLoss: 0.3869207501411438 - trainLoss: 0.36694809794425964\n",
      "[0.0010000000000000002] cnt: 28 - valLoss: 0.3869207203388214 - trainLoss: 0.3669479489326477\n",
      "[0.0010000000000000002] cnt: 29 - valLoss: 0.38692089915275574 - trainLoss: 0.36694779992103577\n",
      "[0.0010000000000000002] cnt: 30 - valLoss: 0.3869210481643677 - trainLoss: 0.36694762110710144\n",
      "[0.0010000000000000002] cnt: 31 - valLoss: 0.38692089915275574 - trainLoss: 0.3669474720954895\n",
      "[0.0010000000000000002] cnt: 32 - valLoss: 0.3869209885597229 - trainLoss: 0.36694735288619995\n",
      "[0.0010000000000000002] cnt: 33 - valLoss: 0.38692110776901245 - trainLoss: 0.36694714426994324\n",
      "[0.0010000000000000002] cnt: 34 - valLoss: 0.386921226978302 - trainLoss: 0.3669469654560089\n",
      "[0.0010000000000000002] cnt: 35 - valLoss: 0.38692137598991394 - trainLoss: 0.36694684624671936\n",
      "[0.0010000000000000002] cnt: 36 - valLoss: 0.38692137598991394 - trainLoss: 0.3669466972351074\n",
      "[0.0010000000000000002] cnt: 37 - valLoss: 0.38692161440849304 - trainLoss: 0.3669465482234955\n",
      "[0.0010000000000000002] cnt: 38 - valLoss: 0.38692161440849304 - trainLoss: 0.36694636940956116\n",
      "[0.0010000000000000002] cnt: 39 - valLoss: 0.3869217336177826 - trainLoss: 0.3669462203979492\n",
      "[0.0010000000000000002] cnt: 40 - valLoss: 0.38692185282707214 - trainLoss: 0.3669460415840149\n",
      "[0.0010000000000000002] cnt: 41 - valLoss: 0.38692185282707214 - trainLoss: 0.36694589257240295\n",
      "[0.0010000000000000002] cnt: 42 - valLoss: 0.38692203164100647 - trainLoss: 0.366945743560791\n",
      "[0.0010000000000000002] cnt: 43 - valLoss: 0.38692212104797363 - trainLoss: 0.3669455945491791\n",
      "[0.0010000000000000002] cnt: 44 - valLoss: 0.38692209124565125 - trainLoss: 0.36694544553756714\n",
      "[0.0010000000000000002] cnt: 45 - valLoss: 0.38692232966423035 - trainLoss: 0.3669452667236328\n",
      "[0.0010000000000000002] cnt: 46 - valLoss: 0.38692227005958557 - trainLoss: 0.3669451177120209\n",
      "[0.0010000000000000002] cnt: 47 - valLoss: 0.3869224488735199 - trainLoss: 0.36694496870040894\n",
      "[0.0010000000000000002] cnt: 48 - valLoss: 0.3869224786758423 - trainLoss: 0.3669447898864746\n",
      "[0.0010000000000000002] cnt: 49 - valLoss: 0.3869224786758423 - trainLoss: 0.36694464087486267\n",
      "[0.0010000000000000002] cnt: 50 - valLoss: 0.3869226276874542 - trainLoss: 0.36694449186325073\n",
      "[0.0010000000000000002] cnt: 51 - valLoss: 0.38692277669906616 - trainLoss: 0.3669443428516388\n",
      "[0.0010000000000000002] cnt: 52 - valLoss: 0.3869228959083557 - trainLoss: 0.36694419384002686\n",
      "[0.0010000000000000002] cnt: 53 - valLoss: 0.3869229853153229 - trainLoss: 0.36694398522377014\n",
      "[0.0010000000000000002] cnt: 54 - valLoss: 0.38692304491996765 - trainLoss: 0.3669438362121582\n",
      "[0.0010000000000000002] cnt: 55 - valLoss: 0.386923223733902 - trainLoss: 0.36694368720054626\n",
      "[0.0010000000000000002] cnt: 56 - valLoss: 0.386923223733902 - trainLoss: 0.3669435381889343\n",
      "[0.0010000000000000002] cnt: 57 - valLoss: 0.3869234025478363 - trainLoss: 0.366943359375\n",
      "[0.0010000000000000002] cnt: 58 - valLoss: 0.38692358136177063 - trainLoss: 0.36694321036338806\n",
      "[0.0010000000000000002] cnt: 59 - valLoss: 0.38692352175712585 - trainLoss: 0.3669430613517761\n",
      "[0.0010000000000000002] cnt: 60 - valLoss: 0.386923611164093 - trainLoss: 0.3669428825378418\n",
      "[0.0010000000000000002] cnt: 61 - valLoss: 0.3869237005710602 - trainLoss: 0.36694273352622986\n",
      "[0.0010000000000000002] cnt: 62 - valLoss: 0.38692378997802734 - trainLoss: 0.3669425845146179\n",
      "[0.0010000000000000002] cnt: 63 - valLoss: 0.3869238495826721 - trainLoss: 0.36694246530532837\n",
      "[0.0010000000000000002] cnt: 64 - valLoss: 0.38692405819892883 - trainLoss: 0.36694228649139404\n",
      "[0.0010000000000000002] cnt: 65 - valLoss: 0.38692402839660645 - trainLoss: 0.3669421374797821\n",
      "[0.0010000000000000002] cnt: 66 - valLoss: 0.38692420721054077 - trainLoss: 0.36694198846817017\n",
      "[0.0010000000000000002] cnt: 67 - valLoss: 0.3869243860244751 - trainLoss: 0.36694177985191345\n",
      "[0.0010000000000000002] cnt: 68 - valLoss: 0.38692426681518555 - trainLoss: 0.3669416606426239\n",
      "[0.0010000000000000002] cnt: 69 - valLoss: 0.38692447543144226 - trainLoss: 0.36694151163101196\n",
      "[0.0010000000000000002] cnt: 70 - valLoss: 0.3869245648384094 - trainLoss: 0.36694133281707764\n",
      "[0.0010000000000000002] cnt: 71 - valLoss: 0.38692471385002136 - trainLoss: 0.3669411540031433\n",
      "[0.0010000000000000002] cnt: 72 - valLoss: 0.38692471385002136 - trainLoss: 0.36694103479385376\n",
      "[0.0010000000000000002] cnt: 73 - valLoss: 0.3869248330593109 - trainLoss: 0.36694085597991943\n",
      "[0.0010000000000000002] cnt: 74 - valLoss: 0.3869251012802124 - trainLoss: 0.3669407069683075\n",
      "[0.0010000000000000002] cnt: 75 - valLoss: 0.38692495226860046 - trainLoss: 0.36694055795669556\n",
      "[0.0010000000000000002] cnt: 76 - valLoss: 0.38692522048950195 - trainLoss: 0.366940438747406\n",
      "[0.0010000000000000002] cnt: 77 - valLoss: 0.3869251310825348 - trainLoss: 0.3669402599334717\n",
      "[0.0010000000000000002] cnt: 78 - valLoss: 0.38692545890808105 - trainLoss: 0.36694005131721497\n",
      "[0.0010000000000000002] cnt: 79 - valLoss: 0.38692525029182434 - trainLoss: 0.3669399321079254\n",
      "[0.0010000000000000002] cnt: 80 - valLoss: 0.3869255781173706 - trainLoss: 0.3669397830963135\n",
      "[0.0010000000000000002] cnt: 81 - valLoss: 0.38692569732666016 - trainLoss: 0.36693960428237915\n",
      "[0.0010000000000000002] cnt: 82 - valLoss: 0.3869257867336273 - trainLoss: 0.3669394850730896\n",
      "[0.0010000000000000002] cnt: 83 - valLoss: 0.3869258165359497 - trainLoss: 0.3669393062591553\n",
      "[0.0010000000000000002] cnt: 84 - valLoss: 0.38692596554756165 - trainLoss: 0.36693912744522095\n",
      "[0.0010000000000000002] cnt: 85 - valLoss: 0.38692590594291687 - trainLoss: 0.366938978433609\n",
      "[0.0010000000000000002] cnt: 86 - valLoss: 0.38692614436149597 - trainLoss: 0.3669387996196747\n",
      "[0.0010000000000000002] cnt: 87 - valLoss: 0.3869262635707855 - trainLoss: 0.36693865060806274\n",
      "[0.0010000000000000002] cnt: 88 - valLoss: 0.3869263827800751 - trainLoss: 0.3669385313987732\n",
      "[0.0010000000000000002] cnt: 89 - valLoss: 0.38692644238471985 - trainLoss: 0.36693838238716125\n",
      "[0.0010000000000000002] cnt: 90 - valLoss: 0.38692647218704224 - trainLoss: 0.36693820357322693\n",
      "[0.0010000000000000002] cnt: 91 - valLoss: 0.386926531791687 - trainLoss: 0.3669380247592926\n",
      "[0.0010000000000000002] cnt: 92 - valLoss: 0.3869267404079437 - trainLoss: 0.36693790555000305\n",
      "[0.0010000000000000002] cnt: 93 - valLoss: 0.3869268298149109 - trainLoss: 0.3669377267360687\n",
      "[0.0010000000000000002] cnt: 94 - valLoss: 0.3869268298149109 - trainLoss: 0.3669375777244568\n",
      "[0.0010000000000000002] cnt: 95 - valLoss: 0.38692694902420044 - trainLoss: 0.36693739891052246\n",
      "[0.0010000000000000002] cnt: 96 - valLoss: 0.3869270384311676 - trainLoss: 0.3669372797012329\n",
      "[0.0010000000000000002] cnt: 97 - valLoss: 0.38692721724510193 - trainLoss: 0.3669371008872986\n",
      "[0.0010000000000000002] cnt: 98 - valLoss: 0.38692715764045715 - trainLoss: 0.36693692207336426\n",
      "[0.0010000000000000002] cnt: 99 - valLoss: 0.3869274854660034 - trainLoss: 0.36693674325942993\n",
      "[0.0010000000000000002] cnt: 100 - valLoss: 0.3869275748729706 - trainLoss: 0.3669366240501404\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.36693647503852844\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.36693647503852844\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.36693644523620605\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.36693641543388367\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.36693641543388367\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.3669363856315613\n",
      "[0.00010000000000000003] cnt: 1 - valLoss: 0.38692760467529297 - trainLoss: 0.3669363558292389\n",
      "[0.00010000000000000003] cnt: 0 - valLoss: 0.3869275748729706 - trainLoss: 0.3669363558292389\n",
      "[0.00010000000000000003] cnt: 1 - valLoss: 0.38692763447761536 - trainLoss: 0.3669363558292389\n",
      "[0.00010000000000000003] cnt: 2 - valLoss: 0.38692763447761536 - trainLoss: 0.3669363260269165\n",
      "[0.00010000000000000003] cnt: 3 - valLoss: 0.38692763447761536 - trainLoss: 0.3669363260269165\n",
      "[0.00010000000000000003] cnt: 4 - valLoss: 0.38692763447761536 - trainLoss: 0.3669362962245941\n",
      "[0.00010000000000000003] cnt: 5 - valLoss: 0.38692766427993774 - trainLoss: 0.36693626642227173\n",
      "[0.00010000000000000003] cnt: 6 - valLoss: 0.38692766427993774 - trainLoss: 0.36693626642227173\n",
      "[0.00010000000000000003] cnt: 7 - valLoss: 0.38692766427993774 - trainLoss: 0.36693620681762695\n",
      "[0.00010000000000000003] cnt: 8 - valLoss: 0.38692766427993774 - trainLoss: 0.36693620681762695\n",
      "[0.00010000000000000003] cnt: 9 - valLoss: 0.38692766427993774 - trainLoss: 0.36693620681762695\n",
      "[0.00010000000000000003] cnt: 10 - valLoss: 0.38692769408226013 - trainLoss: 0.36693617701530457\n",
      "[0.00010000000000000003] cnt: 11 - valLoss: 0.38692769408226013 - trainLoss: 0.36693617701530457\n",
      "[0.00010000000000000003] cnt: 12 - valLoss: 0.3869277238845825 - trainLoss: 0.3669361472129822\n",
      "[0.00010000000000000003] cnt: 13 - valLoss: 0.38692769408226013 - trainLoss: 0.3669361472129822\n",
      "[0.00010000000000000003] cnt: 14 - valLoss: 0.38692769408226013 - trainLoss: 0.3669361174106598\n",
      "[0.00010000000000000003] cnt: 15 - valLoss: 0.3869277536869049 - trainLoss: 0.3669360876083374\n",
      "[0.00010000000000000003] cnt: 16 - valLoss: 0.3869277536869049 - trainLoss: 0.3669361174106598\n",
      "[0.00010000000000000003] cnt: 17 - valLoss: 0.3869277238845825 - trainLoss: 0.3669360876083374\n",
      "[0.00010000000000000003] cnt: 18 - valLoss: 0.3869277536869049 - trainLoss: 0.366936057806015\n",
      "[0.00010000000000000003] cnt: 19 - valLoss: 0.3869277536869049 - trainLoss: 0.366936057806015\n",
      "[0.00010000000000000003] cnt: 20 - valLoss: 0.3869277536869049 - trainLoss: 0.3669360280036926\n",
      "[0.00010000000000000003] cnt: 21 - valLoss: 0.3869278132915497 - trainLoss: 0.36693599820137024\n",
      "[0.00010000000000000003] cnt: 22 - valLoss: 0.3869277834892273 - trainLoss: 0.3669360280036926\n",
      "[0.00010000000000000003] cnt: 23 - valLoss: 0.3869278132915497 - trainLoss: 0.36693596839904785\n",
      "[0.00010000000000000003] cnt: 24 - valLoss: 0.3869278132915497 - trainLoss: 0.36693596839904785\n",
      "[0.00010000000000000003] cnt: 25 - valLoss: 0.3869278132915497 - trainLoss: 0.36693596839904785\n",
      "[0.00010000000000000003] cnt: 26 - valLoss: 0.3869278132915497 - trainLoss: 0.36693593859672546\n",
      "[0.00010000000000000003] cnt: 27 - valLoss: 0.3869278132915497 - trainLoss: 0.36693593859672546\n",
      "[0.00010000000000000003] cnt: 28 - valLoss: 0.38692784309387207 - trainLoss: 0.3669359087944031\n",
      "[0.00010000000000000003] cnt: 29 - valLoss: 0.38692784309387207 - trainLoss: 0.3669358789920807\n",
      "[0.00010000000000000003] cnt: 30 - valLoss: 0.38692787289619446 - trainLoss: 0.3669358491897583\n",
      "[0.00010000000000000003] cnt: 31 - valLoss: 0.38692784309387207 - trainLoss: 0.3669358789920807\n",
      "[0.00010000000000000003] cnt: 32 - valLoss: 0.38692787289619446 - trainLoss: 0.3669358193874359\n",
      "[0.00010000000000000003] cnt: 33 - valLoss: 0.38692787289619446 - trainLoss: 0.3669358193874359\n",
      "[0.00010000000000000003] cnt: 34 - valLoss: 0.38692787289619446 - trainLoss: 0.3669358193874359\n",
      "[0.00010000000000000003] cnt: 35 - valLoss: 0.38692787289619446 - trainLoss: 0.3669357895851135\n",
      "[0.00010000000000000003] cnt: 36 - valLoss: 0.38692787289619446 - trainLoss: 0.3669357895851135\n",
      "[0.00010000000000000003] cnt: 37 - valLoss: 0.38692790269851685 - trainLoss: 0.3669357895851135\n",
      "[0.00010000000000000003] cnt: 38 - valLoss: 0.38692790269851685 - trainLoss: 0.36693575978279114\n",
      "[0.00010000000000000003] cnt: 39 - valLoss: 0.38692790269851685 - trainLoss: 0.36693572998046875\n",
      "[0.00010000000000000003] cnt: 40 - valLoss: 0.38692790269851685 - trainLoss: 0.36693572998046875\n",
      "[0.00010000000000000003] cnt: 41 - valLoss: 0.38692790269851685 - trainLoss: 0.36693570017814636\n",
      "[0.00010000000000000003] cnt: 42 - valLoss: 0.3869279623031616 - trainLoss: 0.36693570017814636\n",
      "[0.00010000000000000003] cnt: 43 - valLoss: 0.38692793250083923 - trainLoss: 0.366935670375824\n",
      "[0.00010000000000000003] cnt: 44 - valLoss: 0.3869279623031616 - trainLoss: 0.366935670375824\n",
      "[0.00010000000000000003] cnt: 45 - valLoss: 0.3869279623031616 - trainLoss: 0.3669356405735016\n",
      "[0.00010000000000000003] cnt: 46 - valLoss: 0.3869279623031616 - trainLoss: 0.3669356107711792\n",
      "[0.00010000000000000003] cnt: 47 - valLoss: 0.3869279623031616 - trainLoss: 0.3669355809688568\n",
      "[0.00010000000000000003] cnt: 48 - valLoss: 0.3869279623031616 - trainLoss: 0.3669355809688568\n",
      "[0.00010000000000000003] cnt: 49 - valLoss: 0.3869280219078064 - trainLoss: 0.3669355809688568\n",
      "[0.00010000000000000003] cnt: 50 - valLoss: 0.3869279623031616 - trainLoss: 0.3669355511665344\n",
      "[0.00010000000000000003] cnt: 51 - valLoss: 0.3869280219078064 - trainLoss: 0.36693552136421204\n",
      "[0.00010000000000000003] cnt: 52 - valLoss: 0.3869280219078064 - trainLoss: 0.36693552136421204\n",
      "[0.00010000000000000003] cnt: 53 - valLoss: 0.3869280219078064 - trainLoss: 0.36693549156188965\n",
      "[0.00010000000000000003] cnt: 54 - valLoss: 0.3869280219078064 - trainLoss: 0.36693552136421204\n",
      "[0.00010000000000000003] cnt: 55 - valLoss: 0.3869280219078064 - trainLoss: 0.36693549156188965\n",
      "[0.00010000000000000003] cnt: 56 - valLoss: 0.3869280517101288 - trainLoss: 0.36693546175956726\n",
      "[0.00010000000000000003] cnt: 57 - valLoss: 0.3869280517101288 - trainLoss: 0.3669354319572449\n",
      "[0.00010000000000000003] cnt: 58 - valLoss: 0.38692808151245117 - trainLoss: 0.3669354021549225\n",
      "[0.00010000000000000003] cnt: 59 - valLoss: 0.38692808151245117 - trainLoss: 0.3669354021549225\n",
      "[0.00010000000000000003] cnt: 60 - valLoss: 0.38692808151245117 - trainLoss: 0.3669354021549225\n",
      "[0.00010000000000000003] cnt: 61 - valLoss: 0.38692808151245117 - trainLoss: 0.3669354021549225\n",
      "[0.00010000000000000003] cnt: 62 - valLoss: 0.38692811131477356 - trainLoss: 0.3669353723526001\n",
      "[0.00010000000000000003] cnt: 63 - valLoss: 0.38692814111709595 - trainLoss: 0.3669353425502777\n",
      "[0.00010000000000000003] cnt: 64 - valLoss: 0.38692811131477356 - trainLoss: 0.3669353425502777\n",
      "[0.00010000000000000003] cnt: 65 - valLoss: 0.38692814111709595 - trainLoss: 0.3669353425502777\n",
      "[0.00010000000000000003] cnt: 66 - valLoss: 0.38692811131477356 - trainLoss: 0.3669353127479553\n",
      "[0.00010000000000000003] cnt: 67 - valLoss: 0.38692814111709595 - trainLoss: 0.36693528294563293\n",
      "[0.00010000000000000003] cnt: 68 - valLoss: 0.38692814111709595 - trainLoss: 0.36693525314331055\n",
      "[0.00010000000000000003] cnt: 69 - valLoss: 0.38692817091941833 - trainLoss: 0.36693528294563293\n",
      "[0.00010000000000000003] cnt: 70 - valLoss: 0.3869282007217407 - trainLoss: 0.36693525314331055\n",
      "[0.00010000000000000003] cnt: 71 - valLoss: 0.38692817091941833 - trainLoss: 0.36693525314331055\n",
      "[0.00010000000000000003] cnt: 72 - valLoss: 0.3869282007217407 - trainLoss: 0.36693522334098816\n",
      "[0.00010000000000000003] cnt: 73 - valLoss: 0.3869282007217407 - trainLoss: 0.36693519353866577\n",
      "[0.00010000000000000003] cnt: 74 - valLoss: 0.3869282305240631 - trainLoss: 0.3669351637363434\n",
      "[0.00010000000000000003] cnt: 75 - valLoss: 0.3869282305240631 - trainLoss: 0.36693519353866577\n",
      "[0.00010000000000000003] cnt: 76 - valLoss: 0.3869282305240631 - trainLoss: 0.3669351637363434\n",
      "[0.00010000000000000003] cnt: 77 - valLoss: 0.3869282603263855 - trainLoss: 0.366935133934021\n",
      "[0.00010000000000000003] cnt: 78 - valLoss: 0.3869282603263855 - trainLoss: 0.3669351041316986\n",
      "[0.00010000000000000003] cnt: 79 - valLoss: 0.3869282603263855 - trainLoss: 0.3669351041316986\n",
      "[0.00010000000000000003] cnt: 80 - valLoss: 0.3869282603263855 - trainLoss: 0.3669351041316986\n",
      "[0.00010000000000000003] cnt: 81 - valLoss: 0.3869282901287079 - trainLoss: 0.36693504452705383\n",
      "[0.00010000000000000003] cnt: 82 - valLoss: 0.3869282901287079 - trainLoss: 0.36693504452705383\n",
      "[0.00010000000000000003] cnt: 83 - valLoss: 0.3869282901287079 - trainLoss: 0.36693504452705383\n",
      "[0.00010000000000000003] cnt: 84 - valLoss: 0.3869282901287079 - trainLoss: 0.36693501472473145\n",
      "[0.00010000000000000003] cnt: 85 - valLoss: 0.3869283199310303 - trainLoss: 0.36693501472473145\n",
      "[0.00010000000000000003] cnt: 86 - valLoss: 0.3869283199310303 - trainLoss: 0.36693498492240906\n",
      "[0.00010000000000000003] cnt: 87 - valLoss: 0.3869283199310303 - trainLoss: 0.36693498492240906\n",
      "[0.00010000000000000003] cnt: 88 - valLoss: 0.3869283199310303 - trainLoss: 0.36693498492240906\n",
      "[0.00010000000000000003] cnt: 89 - valLoss: 0.38692837953567505 - trainLoss: 0.36693495512008667\n",
      "[0.00010000000000000003] cnt: 90 - valLoss: 0.38692837953567505 - trainLoss: 0.36693495512008667\n",
      "[0.00010000000000000003] cnt: 91 - valLoss: 0.38692837953567505 - trainLoss: 0.3669349253177643\n",
      "[0.00010000000000000003] cnt: 92 - valLoss: 0.38692837953567505 - trainLoss: 0.3669348955154419\n",
      "[0.00010000000000000003] cnt: 93 - valLoss: 0.38692840933799744 - trainLoss: 0.3669348359107971\n",
      "[0.00010000000000000003] cnt: 94 - valLoss: 0.38692837953567505 - trainLoss: 0.3669348657131195\n",
      "[0.00010000000000000003] cnt: 95 - valLoss: 0.38692837953567505 - trainLoss: 0.3669348359107971\n",
      "[0.00010000000000000003] cnt: 96 - valLoss: 0.38692837953567505 - trainLoss: 0.3669348359107971\n",
      "[0.00010000000000000003] cnt: 97 - valLoss: 0.38692840933799744 - trainLoss: 0.3669348359107971\n",
      "[0.00010000000000000003] cnt: 98 - valLoss: 0.3869284391403198 - trainLoss: 0.36693480610847473\n",
      "[0.00010000000000000003] cnt: 99 - valLoss: 0.3869284391403198 - trainLoss: 0.36693480610847473\n",
      "[0.00010000000000000003] cnt: 100 - valLoss: 0.3869284689426422 - trainLoss: 0.36693477630615234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1653ce730>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1,0.1)\n",
    "\n",
    "trainLoss=[]\n",
    "valLoss=[]\n",
    "\n",
    "for phase in range(5):\n",
    "    bestModel = model\n",
    "    bestLoss = float('inf')\n",
    "    cnt=0\n",
    "\n",
    "\n",
    "    while(cnt<100):\n",
    "        model.train()\n",
    "        trainL=0\n",
    "        for X, y,_ in trainLoader:\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            trainL+=loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        valL=0\n",
    "        with torch.no_grad():\n",
    "            for X ,y,_ in valLoader:\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                # for i in loss:\n",
    "                valL+= loss.item()\n",
    "\n",
    "\n",
    "        trainLoss.append(trainL)\n",
    "        valLoss.append(valL)\n",
    "\n",
    "        if bestLoss<valLoss[-1]:\n",
    "            cnt+=1\n",
    "        else:\n",
    "            cnt = 0\n",
    "            bestLoss = valLoss[-1]\n",
    "            bestModel = model\n",
    "    \n",
    "        print(f'{scheduler.get_last_lr()} cnt: {cnt} - valLoss: {valLoss[-1]} - trainLoss: {trainLoss[-1]}')\n",
    "\n",
    "    model = bestModel\n",
    "    scheduler.step()\n",
    "\n",
    "plt.plot(trainLoss,label='trainLoss')\n",
    "plt.plot(valLoss,label='valLoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8539325842696629"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracyAI(dataloader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, _  in dataloader:\n",
    "            pred = model(X)\n",
    "\n",
    "            for idx, i in enumerate(pred):\n",
    "                if torch.argmax(i)== torch.argmax(y[idx]):\n",
    "                    correct +=1 \n",
    "                total+=1\n",
    "    return correct / total\n",
    "\n",
    "accuracyAI(valLoader, bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     passengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for X ,_ , id in dataloader:\n",
    "            pred = model(X)\n",
    "            for idx, i in enumerate(pred):\n",
    "                result.append([id[idx],torch.argmax(i).item()])\n",
    "    return result\n",
    "\n",
    "result = testAI(testLoader, bestModel)\n",
    "result = pd.DataFrame(result)\n",
    "result = result.astype(int)\n",
    "result.columns=['passengerId','Survived']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
